{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 130, "column": 0}, "map": {"version":3,"sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/src/services/crawlerService.ts"],"sourcesContent":["import FirecrawlApp from '@mendable/firecrawl-js';\nimport dotenv from 'dotenv';\n\ndotenv.config();\n\n/**\n * Service to handle crawling of websites using Firecrawl API.\n */\nexport class CrawlerService {\n  private app: FirecrawlApp;\n\n  constructor() {\n    const apiKey = process.env.FIRECRAWL_API_KEY;\n    if (!apiKey) {\n      throw new Error(\"FIRECRAWL_API_KEY is not defined in environment variables.\");\n    }\n    this.app = new FirecrawlApp({ apiKey });\n  }\n\n  /**\n   * Crawls a URL and returns the content in Markdown format.\n   * Can be configured to crawl subpages or just a single page.\n   * \n   * @param url The URL to crawl\n   * @param limit Max number of pages to crawl (default 10)\n   */\n  async crawlWebsite(url: string, limit: number = 10): Promise<any> {\n    try {\n      console.log(`Starting crawl for: ${url} with limit: ${limit}`);\n      \n      const crawlResponse = await this.app.crawl(url, {\n        limit: limit,\n        scrapeOptions: {\n          formats: ['markdown'],\n        },\n      });\n\n      if (crawlResponse.status !== 'completed') {\n        throw new Error(`Failed to crawl URL. Status: ${crawlResponse.status}`);\n      }\n\n      console.log(`Successfully crawled ${crawlResponse.data.length} pages.`);\n      return crawlResponse.data;\n    } catch (error) {\n      console.error(\"Error in crawlWebsite:\", error);\n      throw error;\n    }\n  }\n\n  /**\n   * Scrapes a single URL to get markdown content specifically.\n   * Useful for quick lookups or precise page reading.\n   */\n  async scrapePage(url: string): Promise<string> {\n    try {\n      const scrapeResponse = await this.app.scrape(url, {\n        formats: ['markdown'],\n      });\n\n      // I V2 kastar scrape ofta fel direkt eller returnerar ett objekt.\n      // Kontrollera om markdown finns.\n      if (!scrapeResponse.markdown) {\n        throw new Error(`Failed to scrape URL or no markdown found.`);\n      }\n\n      return scrapeResponse.markdown || \"\";\n    } catch (error) {\n      console.error(\"Error in scrapePage:\", error);\n      throw error;\n    }\n  }\n}\n"],"names":[],"mappings":";;;;AAAA;AACA;;;AAEA,kJAAM,CAAC,MAAM;AAKN,MAAM;IACH,IAAkB;IAE1B,aAAc;QACZ,MAAM,SAAS,QAAQ,GAAG,CAAC,iBAAiB;QAC5C,IAAI,CAAC,QAAQ;YACX,MAAM,IAAI,MAAM;QAClB;QACA,IAAI,CAAC,GAAG,GAAG,IAAI,2KAAY,CAAC;YAAE;QAAO;IACvC;IAEA;;;;;;GAMC,GACD,MAAM,aAAa,GAAW,EAAE,QAAgB,EAAE,EAAgB;QAChE,IAAI;YACF,QAAQ,GAAG,CAAC,CAAC,oBAAoB,EAAE,IAAI,aAAa,EAAE,OAAO;YAE7D,MAAM,gBAAgB,MAAM,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,KAAK;gBAC9C,OAAO;gBACP,eAAe;oBACb,SAAS;wBAAC;qBAAW;gBACvB;YACF;YAEA,IAAI,cAAc,MAAM,KAAK,aAAa;gBACxC,MAAM,IAAI,MAAM,CAAC,6BAA6B,EAAE,cAAc,MAAM,EAAE;YACxE;YAEA,QAAQ,GAAG,CAAC,CAAC,qBAAqB,EAAE,cAAc,IAAI,CAAC,MAAM,CAAC,OAAO,CAAC;YACtE,OAAO,cAAc,IAAI;QAC3B,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,0BAA0B;YACxC,MAAM;QACR;IACF;IAEA;;;GAGC,GACD,MAAM,WAAW,GAAW,EAAmB;QAC7C,IAAI;YACF,MAAM,iBAAiB,MAAM,IAAI,CAAC,GAAG,CAAC,MAAM,CAAC,KAAK;gBAChD,SAAS;oBAAC;iBAAW;YACvB;YAEA,kEAAkE;YAClE,iCAAiC;YACjC,IAAI,CAAC,eAAe,QAAQ,EAAE;gBAC5B,MAAM,IAAI,MAAM,CAAC,0CAA0C,CAAC;YAC9D;YAEA,OAAO,eAAe,QAAQ,IAAI;QACpC,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,wBAAwB;YACtC,MAAM;QACR;IACF;AACF"}},
    {"offset": {"line": 221, "column": 0}, "map": {"version":3,"sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/src/services/ragService.ts"],"sourcesContent":["import { createClient, SupabaseClient } from '@supabase/supabase-js';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { RecursiveCharacterTextSplitter } from '@langchain/textsplitters'; // Uppdaterad import\nimport dotenv from 'dotenv';\n\ndotenv.config();\n\nexport interface SearchResult {\n  content: string;\n  metadata: any;\n  similarity: number;\n}\n\nexport class RagService {\n  private supabase: SupabaseClient;\n  private embeddings: OpenAIEmbeddings;\n  private splitter: RecursiveCharacterTextSplitter;\n\n  constructor() {\n    const sbUrl = process.env.SUPABASE_URL;\n    const sbKey = process.env.SUPABASE_SERVICE_ROLE_KEY; // OBS: Använd Service Role Key för att skriva till DB på backend\n\n    if (!sbUrl || !sbKey) {\n      throw new Error(\"Supabase URL or Service Role Key missing in environment variables.\");\n    }\n\n    this.supabase = createClient(sbUrl, sbKey);\n    \n    // Initiera OpenAI Embeddings (text-embedding-3-small)\n    this.embeddings = new OpenAIEmbeddings({\n      modelName: \"text-embedding-3-small\",\n      // API key hämtas automatiskt från process.env.OPENAI_API_KEY\n    });\n\n    // Konfigurera text splitter\n    this.splitter = new RecursiveCharacterTextSplitter({\n      chunkSize: 1000,\n      chunkOverlap: 200,\n      separators: [\"\\n## \", \"\\n# \", \"\\n\\n\", \"\\n\", \" \", \"\"], // Prioritera rubriker vid split\n    });\n  }\n\n  /**\n   * Bearbetar, splittar och lagrar crawlad data.\n   * @param documents Lista med objekt från Firecrawl (måste innehålla markdown och metadata)\n   * @param companyId Unikt ID för företaget/sessionen\n   */\n  async processAndStoreDocuments(documents: any[], companyId: string): Promise<void> {\n    console.log(`Processing ${documents.length} documents for company: ${companyId}`);\n\n    for (const doc of documents) {\n      if (!doc.markdown) continue;\n\n      // 1. Splitta texten i chunks\n      // Vi skickar med metadata så den följer med varje chunk\n      const splitDocs = await this.splitter.createDocuments(\n        [doc.markdown], \n        [{ source: doc.metadata?.sourceURL || 'unknown', ...doc.metadata }]\n      );\n\n      console.log(`Split document into ${splitDocs.length} chunks.`);\n\n      // 2. Skapa embeddings för alla chunks\n      const texts = splitDocs.map((d: { pageContent: string }) => d.pageContent);\n      const vectors = await this.embeddings.embedDocuments(texts);\n\n      // 3. Förbered data för Supabase\n      const rowsToInsert = splitDocs.map((splitDoc: any, index: number) => ({\n        content: splitDoc.pageContent,\n        metadata: splitDoc.metadata,\n        embedding: vectors[index],\n        company_id: companyId\n      }));\n\n      // 4. Spara i Supabase\n      const { error } = await this.supabase\n        .from('documents')\n        .insert(rowsToInsert);\n\n      if (error) {\n        console.error(\"Error inserting into Supabase:\", error);\n        throw new Error(`Failed to store credentials: ${error.message}`);\n      }\n    }\n    console.log(\"All documents processed and stored successfully.\");\n  }\n\n  /**\n   * Söker efter relevant kontext i Supabase.\n   * Matchar mot SQL-funktionen 'match_documents'.\n   */\n  async performSearch(query: string, companyId: string): Promise<SearchResult[]> {\n    try {\n      // 1. Skapa embedding för användarens fråga\n      const queryEmbedding = await this.embeddings.embedQuery(query);\n\n      // 2. Anropa RPC-funktionen i Supabase\n      // Vi sätter match_count till 5 för att få tillräckligt med kontext\n      const { data: result, error } = await this.supabase.rpc('match_documents', {\n        query_embedding: queryEmbedding,\n        match_threshold: 0.5, // Vi filtrerar bort irrelevant brus (< 50% match)\n        match_count: 5,\n        filter_company_id: companyId\n      });\n\n      if (error) {\n        console.error(\"Error searching Supabase:\", error);\n        throw new Error(`Search failed: ${error.message}`);\n      }\n\n      return result as SearchResult[];\n    } catch (error) {\n      console.error(\"Error in performSearch:\", error);\n      throw error;\n    }\n  }\n}\n"],"names":[],"mappings":";;;;AAAA;AACA;AAAA;AACA,kTAA2E,oBAAoB;AAA/F;AACA;;;;;AAEA,kJAAM,CAAC,MAAM;AAQN,MAAM;IACH,SAAyB;IACzB,WAA6B;IAC7B,SAAyC;IAEjD,aAAc;QACZ,MAAM,QAAQ,QAAQ,GAAG,CAAC,YAAY;QACtC,MAAM,QAAQ,QAAQ,GAAG,CAAC,yBAAyB,EAAE,iEAAiE;QAEtH,IAAI,CAAC,SAAS,CAAC,OAAO;YACpB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAI,CAAC,QAAQ,GAAG,IAAA,gMAAY,EAAC,OAAO;QAEpC,sDAAsD;QACtD,IAAI,CAAC,UAAU,GAAG,IAAI,iLAAgB,CAAC;YACrC,WAAW;QAEb;QAEA,4BAA4B;QAC5B,IAAI,CAAC,QAAQ,GAAG,IAAI,yMAA8B,CAAC;YACjD,WAAW;YACX,cAAc;YACd,YAAY;gBAAC;gBAAS;gBAAQ;gBAAQ;gBAAM;gBAAK;aAAG;QACtD;IACF;IAEA;;;;GAIC,GACD,MAAM,yBAAyB,SAAgB,EAAE,SAAiB,EAAiB;QACjF,QAAQ,GAAG,CAAC,CAAC,WAAW,EAAE,UAAU,MAAM,CAAC,wBAAwB,EAAE,WAAW;QAEhF,KAAK,MAAM,OAAO,UAAW;YAC3B,IAAI,CAAC,IAAI,QAAQ,EAAE;YAEnB,6BAA6B;YAC7B,wDAAwD;YACxD,MAAM,YAAY,MAAM,IAAI,CAAC,QAAQ,CAAC,eAAe,CACnD;gBAAC,IAAI,QAAQ;aAAC,EACd;gBAAC;oBAAE,QAAQ,IAAI,QAAQ,EAAE,aAAa;oBAAW,GAAG,IAAI,QAAQ;gBAAC;aAAE;YAGrE,QAAQ,GAAG,CAAC,CAAC,oBAAoB,EAAE,UAAU,MAAM,CAAC,QAAQ,CAAC;YAE7D,sCAAsC;YACtC,MAAM,QAAQ,UAAU,GAAG,CAAC,CAAC,IAA+B,EAAE,WAAW;YACzE,MAAM,UAAU,MAAM,IAAI,CAAC,UAAU,CAAC,cAAc,CAAC;YAErD,gCAAgC;YAChC,MAAM,eAAe,UAAU,GAAG,CAAC,CAAC,UAAe,QAAkB,CAAC;oBACpE,SAAS,SAAS,WAAW;oBAC7B,UAAU,SAAS,QAAQ;oBAC3B,WAAW,OAAO,CAAC,MAAM;oBACzB,YAAY;gBACd,CAAC;YAED,sBAAsB;YACtB,MAAM,EAAE,KAAK,EAAE,GAAG,MAAM,IAAI,CAAC,QAAQ,CAClC,IAAI,CAAC,aACL,MAAM,CAAC;YAEV,IAAI,OAAO;gBACT,QAAQ,KAAK,CAAC,kCAAkC;gBAChD,MAAM,IAAI,MAAM,CAAC,6BAA6B,EAAE,MAAM,OAAO,EAAE;YACjE;QACF;QACA,QAAQ,GAAG,CAAC;IACd;IAEA;;;GAGC,GACD,MAAM,cAAc,KAAa,EAAE,SAAiB,EAA2B;QAC7E,IAAI;YACF,2CAA2C;YAC3C,MAAM,iBAAiB,MAAM,IAAI,CAAC,UAAU,CAAC,UAAU,CAAC;YAExD,sCAAsC;YACtC,mEAAmE;YACnE,MAAM,EAAE,MAAM,MAAM,EAAE,KAAK,EAAE,GAAG,MAAM,IAAI,CAAC,QAAQ,CAAC,GAAG,CAAC,mBAAmB;gBACzE,iBAAiB;gBACjB,iBAAiB;gBACjB,aAAa;gBACb,mBAAmB;YACrB;YAEA,IAAI,OAAO;gBACT,QAAQ,KAAK,CAAC,6BAA6B;gBAC3C,MAAM,IAAI,MAAM,CAAC,eAAe,EAAE,MAAM,OAAO,EAAE;YACnD;YAEA,OAAO;QACT,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,2BAA2B;YACzC,MAAM;QACR;IACF;AACF"}},
    {"offset": {"line": 333, "column": 0}, "map": {"version":3,"sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/src/app/api/ingest/route.ts"],"sourcesContent":["import { NextResponse } from 'next/server';\nimport { CrawlerService } from '@/services/crawlerService';\nimport { RagService } from '@/services/ragService';\n\n// Vi instansierar dessa utanför handlern för att återanvända kopplingar om möjligt (i serverless miljöer skapas de ofta om ändå)\n// EDIT: Flyttar in dem för att garantera att env vars är laddade vid körning\n// const crawlerService = new CrawlerService();\n// const ragService = new RagService();\n\nexport async function POST(req: Request) {\n  try {\n    const crawlerService = new CrawlerService();\n    const ragService = new RagService();\n\n    const body = await req.json();\n    const { url, companyId } = body;\n\n    if (!url || !companyId) {\n      return NextResponse.json(\n        { error: \"URL and companyId are required\" },\n        { status: 400 }\n      );\n    }\n\n    console.log(`Received ingest request for ${url} (Company: ${companyId})`);\n\n    // 1. Crawla sajten (begränsat till 5 sidor för demo)\n    const crawledData = await crawlerService.crawlWebsite(url, 5);\n\n    // 2. Bearbeta och spara i vektordatabas\n    if (crawledData && crawledData.length > 0) {\n      await ragService.processAndStoreDocuments(crawledData, companyId);\n      return NextResponse.json({ \n        success: true, \n        message: \"Ingestion complete\", \n        pagesProcessed: crawledData.length \n      });\n    } else {\n      return NextResponse.json(\n        { error: \"No data found during crawl\" },\n        { status: 500 }\n      );\n    }\n\n  } catch (error: any) {\n    console.error(\"Ingestion error:\", error);\n    return NextResponse.json(\n      { error: error.message || \"Internal Server Error\" },\n      { status: 500 }\n    );\n  }\n}\n"],"names":[],"mappings":";;;;AAAA;AACA;AACA;;;;AAOO,eAAe,KAAK,GAAY;IACrC,IAAI;QACF,MAAM,iBAAiB,IAAI,qJAAc;QACzC,MAAM,aAAa,IAAI,6IAAU;QAEjC,MAAM,OAAO,MAAM,IAAI,IAAI;QAC3B,MAAM,EAAE,GAAG,EAAE,SAAS,EAAE,GAAG;QAE3B,IAAI,CAAC,OAAO,CAAC,WAAW;YACtB,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAiC,GAC1C;gBAAE,QAAQ;YAAI;QAElB;QAEA,QAAQ,GAAG,CAAC,CAAC,4BAA4B,EAAE,IAAI,WAAW,EAAE,UAAU,CAAC,CAAC;QAExE,qDAAqD;QACrD,MAAM,cAAc,MAAM,eAAe,YAAY,CAAC,KAAK;QAE3D,wCAAwC;QACxC,IAAI,eAAe,YAAY,MAAM,GAAG,GAAG;YACzC,MAAM,WAAW,wBAAwB,CAAC,aAAa;YACvD,OAAO,gJAAY,CAAC,IAAI,CAAC;gBACvB,SAAS;gBACT,SAAS;gBACT,gBAAgB,YAAY,MAAM;YACpC;QACF,OAAO;YACL,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAA6B,GACtC;gBAAE,QAAQ;YAAI;QAElB;IAEF,EAAE,OAAO,OAAY;QACnB,QAAQ,KAAK,CAAC,oBAAoB;QAClC,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO,MAAM,OAAO,IAAI;QAAwB,GAClD;YAAE,QAAQ;QAAI;IAElB;AACF"}}]
}