{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 4, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/utils/errors.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/utils/errors.ts"],"sourcesContent":["/* eslint-disable @typescript-eslint/no-explicit-any */\n\n// Duplicate of core\n// TODO: Remove once we stop supporting 0.2.x core versions\nexport type LangChainErrorCodes =\n  | \"INVALID_PROMPT_INPUT\"\n  | \"INVALID_TOOL_RESULTS\"\n  | \"MESSAGE_COERCION_FAILURE\"\n  | \"MODEL_AUTHENTICATION\"\n  | \"MODEL_NOT_FOUND\"\n  | \"MODEL_RATE_LIMIT\"\n  | \"OUTPUT_PARSING_FAILURE\";\n\nexport function addLangChainErrorFields(\n  error: any,\n  lc_error_code: LangChainErrorCodes\n) {\n  (error as any).lc_error_code = lc_error_code;\n  error.message = `${error.message}\\n\\nTroubleshooting URL: https://docs.langchain.com/oss/javascript/langchain/errors/${lc_error_code}/\\n`;\n  return error;\n}\n"],"names":["error: any","lc_error_code: LangChainErrorCodes"],"mappings":";;;;;AAaA,SAAgB,wBACdA,KAAAA,EACAC,aAAAA,EACA;IACC,MAAc,aAAA,GAAgB;IAC/B,MAAM,OAAA,GAAU,GAAG,MAAM,OAAA,CAAQ,oFAAoF,EAAE,cAAc,GAAG,CAAC;IACzI,OAAO;AACR"}},
    {"offset": {"line": 20, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/utils/client.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/utils/client.ts"],"sourcesContent":["import { APIConnectionTimeoutError, APIUserAbortError } from \"openai\";\nimport { addLangChainErrorFields } from \"./errors.js\";\n\nexport function wrapOpenAIClientError(e: unknown) {\n  if (!e || typeof e !== \"object\") {\n    return e;\n  }\n\n  let error;\n  if (\n    e.constructor.name === APIConnectionTimeoutError.name &&\n    \"message\" in e &&\n    typeof e.message === \"string\"\n  ) {\n    error = new Error(e.message);\n    error.name = \"TimeoutError\";\n  } else if (\n    e.constructor.name === APIUserAbortError.name &&\n    \"message\" in e &&\n    typeof e.message === \"string\"\n  ) {\n    error = new Error(e.message);\n    error.name = \"AbortError\";\n  } else if (\n    \"status\" in e &&\n    e.status === 400 &&\n    \"message\" in e &&\n    typeof e.message === \"string\" &&\n    e.message.includes(\"tool_calls\")\n  ) {\n    error = addLangChainErrorFields(e, \"INVALID_TOOL_RESULTS\");\n  } else if (\"status\" in e && e.status === 401) {\n    error = addLangChainErrorFields(e, \"MODEL_AUTHENTICATION\");\n  } else if (\"status\" in e && e.status === 429) {\n    error = addLangChainErrorFields(e, \"MODEL_RATE_LIMIT\");\n  } else if (\"status\" in e && e.status === 404) {\n    error = addLangChainErrorFields(e, \"MODEL_NOT_FOUND\");\n  } else {\n    error = e;\n  }\n  return error;\n}\n"],"names":["e: unknown"],"mappings":";;;;;;;;;;AAGA,SAAgB,sBAAsBA,CAAAA,EAAY;IAChD,IAAI,CAAC,KAAK,OAAO,MAAM,SACrB,CAAA,OAAO;IAGT,IAAI;IACJ,IACE,EAAE,WAAA,CAAY,IAAA,KAAS,uKAAA,CAA0B,IAAA,IACjD,aAAa,KACb,OAAO,EAAE,OAAA,KAAY,UACrB;QACA,QAAQ,IAAI,MAAM,EAAE,OAAA;QACpB,MAAM,IAAA,GAAO;IACd,OAAA,IACC,EAAE,WAAA,CAAY,IAAA,KAAS,+JAAA,CAAkB,IAAA,IACzC,aAAa,KACb,OAAO,EAAE,OAAA,KAAY,UACrB;QACA,QAAQ,IAAI,MAAM,EAAE,OAAA;QACpB,MAAM,IAAA,GAAO;IACd,OAAA,IACC,YAAY,KACZ,EAAE,MAAA,KAAW,OACb,aAAa,KACb,OAAO,EAAE,OAAA,KAAY,YACrB,EAAE,OAAA,CAAQ,QAAA,CAAS,aAAa,EAEhC,YAAQ,6LAAA,EAAwB,GAAG,uBAAuB;aACjD,YAAY,KAAK,EAAE,MAAA,KAAW,KACvC,YAAQ,6LAAA,EAAwB,GAAG,uBAAuB;aACjD,YAAY,KAAK,EAAE,MAAA,KAAW,KACvC,YAAQ,6LAAA,EAAwB,GAAG,mBAAmB;aAC7C,YAAY,KAAK,EAAE,MAAA,KAAW,KACvC,YAAQ,6LAAA,EAAwB,GAAG,kBAAkB;SAErD,QAAQ;IAEV,OAAO;AACR"}},
    {"offset": {"line": 52, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/utils/misc.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/utils/misc.ts"],"sourcesContent":["import type { OpenAI as OpenAIClient } from \"openai\";\nimport {\n  BaseMessage,\n  ChatMessage,\n  ContentBlock,\n  Data,\n} from \"@langchain/core/messages\";\n\nexport const iife = <T>(fn: () => T) => fn();\n\nexport function isReasoningModel(model?: string) {\n  if (!model) return false;\n  if (/^o\\d/.test(model ?? \"\")) return true;\n  if (model.startsWith(\"gpt-5\") && !model.startsWith(\"gpt-5-chat\")) return true;\n  return false;\n}\n\nexport function extractGenericMessageCustomRole(message: ChatMessage) {\n  if (\n    message.role !== \"system\" &&\n    message.role !== \"developer\" &&\n    message.role !== \"assistant\" &&\n    message.role !== \"user\" &&\n    message.role !== \"function\" &&\n    message.role !== \"tool\"\n  ) {\n    console.warn(`Unknown message role: ${message.role}`);\n  }\n\n  return message.role as OpenAIClient.ChatCompletionRole;\n}\n\nexport function getRequiredFilenameFromMetadata(\n  block:\n    | ContentBlock.Multimodal.File\n    | ContentBlock.Multimodal.Video\n    | Data.StandardFileBlock\n): string {\n  const filename = (block.metadata?.filename ??\n    block.metadata?.name ??\n    block.metadata?.title) as string;\n\n  if (!filename) {\n    throw new Error(\n      \"a filename or name or title is needed via meta-data for OpenAI when working with multimodal blocks\"\n    );\n  }\n\n  return filename;\n}\nexport function messageToOpenAIRole(\n  message: BaseMessage\n): OpenAIClient.ChatCompletionRole {\n  const type = message._getType();\n  switch (type) {\n    case \"system\":\n      return \"system\";\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"function\":\n      return \"function\";\n    case \"tool\":\n      return \"tool\";\n    case \"generic\": {\n      if (!ChatMessage.isInstance(message))\n        throw new Error(\"Invalid generic chat message\");\n      return extractGenericMessageCustomRole(message);\n    }\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\n\nexport function _modelPrefersResponsesAPI(model: string): boolean {\n  return model.includes(\"gpt-5.2-pro\");\n}\n"],"names":["iife","fn: () => T","model?: string","message: ChatMessage","block:\n    | ContentBlock.Multimodal.File\n    | ContentBlock.Multimodal.Video\n    | Data.StandardFileBlock","message: BaseMessage","model: string"],"mappings":";;;;;;;;;;;;;;;;AAQA,MAAaA,SAAO,CAAIC,KAAgB,IAAI;AAE5C,SAAgB,iBAAiBC,KAAAA,EAAgB;IAC/C,IAAI,CAAC,MAAO,CAAA,OAAO;IACnB,IAAI,OAAO,IAAA,CAAK,SAAS,GAAG,CAAE,CAAA,OAAO;IACrC,IAAI,MAAM,UAAA,CAAW,QAAQ,IAAI,CAAC,MAAM,UAAA,CAAW,aAAa,CAAE,CAAA,OAAO;IACzE,OAAO;AACR;AAED,SAAgB,gCAAgCC,OAAAA,EAAsB;IACpE,IACE,QAAQ,IAAA,KAAS,YACjB,QAAQ,IAAA,KAAS,eACjB,QAAQ,IAAA,KAAS,eACjB,QAAQ,IAAA,KAAS,UACjB,QAAQ,IAAA,KAAS,cACjB,QAAQ,IAAA,KAAS,QAEjB,QAAQ,IAAA,CAAK,CAAC,sBAAsB,EAAE,QAAQ,IAAA,EAAM,CAAC;IAGvD,OAAO,QAAQ,IAAA;AAChB;AAED,SAAgB,gCACdC,KAAAA,EAIQ;IACR,MAAM,WAAY,MAAM,QAAA,EAAU,YAChC,MAAM,QAAA,EAAU,QAChB,MAAM,QAAA,EAAU;IAElB,IAAI,CAAC,SACH,CAAA,MAAM,IAAI,MACR;IAIJ,OAAO;AACR;AACD,SAAgB,oBACdC,OAAAA,EACiC;IACjC,MAAM,OAAO,QAAQ,QAAA,EAAU;IAC/B,OAAQ,MAAR;QACE,KAAK,SACH;YAAA,OAAO;QACT,KAAK,KACH;YAAA,OAAO;QACT,KAAK,QACH;YAAA,OAAO;QACT,KAAK,WACH;YAAA,OAAO;QACT,KAAK,OACH;YAAA,OAAO;QACT,KAAK;YACH,IAAI,CAAC,gLAAA,CAAY,UAAA,CAAW,QAAQ,CAClC,CAAA,MAAM,IAAI,MAAM;YAClB,OAAO,gCAAgC,QAAQ;QAEjD,QACE;YAAA,MAAM,IAAI,MAAM,CAAC,sBAAsB,EAAE,MAAM;IAClD;AACF;AAED,SAAgB,0BAA0BC,KAAAA,EAAwB;IAChE,OAAO,MAAM,QAAA,CAAS,cAAc;AACrC"}},
    {"offset": {"line": 113, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/utils/azure.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/utils/azure.ts"],"sourcesContent":["import { getEnv } from \"@langchain/core/utils/env\";\nimport { iife } from \"./misc.js\";\n\nexport interface OpenAIEndpointConfig {\n  azureOpenAIApiDeploymentName?: string;\n  azureOpenAIApiInstanceName?: string;\n  azureOpenAIApiKey?: string;\n  azureADTokenProvider?: () => Promise<string>;\n  azureOpenAIBasePath?: string;\n  baseURL?: string | null;\n  azureOpenAIEndpoint?: string;\n}\n\n/**\n * This function generates an endpoint URL for (Azure) OpenAI\n * based on the configuration parameters provided.\n *\n * @param {OpenAIEndpointConfig} config - The configuration object for the (Azure) endpoint.\n *\n * @property {string} config.azureOpenAIApiDeploymentName - The deployment name of Azure OpenAI.\n * @property {string} config.azureOpenAIApiInstanceName - The instance name of Azure OpenAI, e.g. `example-resource`.\n * @property {string} config.azureOpenAIApiKey - The API Key for Azure OpenAI.\n * @property {string} config.azureOpenAIBasePath - The base path for Azure OpenAI, e.g. `https://example-resource.azure.openai.com/openai/deployments/`.\n * @property {string} config.baseURL - Some other custom base path URL.\n * @property {string} config.azureOpenAIEndpoint - The endpoint for the Azure OpenAI instance, e.g. `https://example-resource.azure.openai.com/`.\n *\n * The function operates as follows:\n * - If both `azureOpenAIBasePath` and `azureOpenAIApiDeploymentName` (plus `azureOpenAIApiKey`) are provided, it returns an URL combining these two parameters (`${azureOpenAIBasePath}/${azureOpenAIApiDeploymentName}`).\n * - If both `azureOpenAIEndpoint` and `azureOpenAIApiDeploymentName` (plus `azureOpenAIApiKey`) are provided, it returns an URL combining these two parameters (`${azureOpenAIEndpoint}/openai/deployments/${azureOpenAIApiDeploymentName}`).\n * - If `azureOpenAIApiKey` is provided, it checks for `azureOpenAIApiInstanceName` and `azureOpenAIApiDeploymentName` and throws an error if any of these is missing. If both are provided, it generates an URL incorporating these parameters.\n * - If none of the above conditions are met, return any custom `baseURL`.\n * - The function returns the generated URL as a string, or undefined if no custom paths are specified.\n *\n * @throws Will throw an error if the necessary parameters for generating the URL are missing.\n *\n * @returns {string | undefined} The generated (Azure) OpenAI endpoint URL.\n */\nexport function getEndpoint(config: OpenAIEndpointConfig) {\n  const {\n    azureOpenAIApiDeploymentName,\n    azureOpenAIApiInstanceName,\n    azureOpenAIApiKey,\n    azureOpenAIBasePath,\n    baseURL,\n    azureADTokenProvider,\n    azureOpenAIEndpoint,\n  } = config;\n\n  if (\n    (azureOpenAIApiKey || azureADTokenProvider) &&\n    azureOpenAIBasePath &&\n    azureOpenAIApiDeploymentName\n  ) {\n    return `${azureOpenAIBasePath}/${azureOpenAIApiDeploymentName}`;\n  }\n  if (\n    (azureOpenAIApiKey || azureADTokenProvider) &&\n    azureOpenAIEndpoint &&\n    azureOpenAIApiDeploymentName\n  ) {\n    return `${azureOpenAIEndpoint}/openai/deployments/${azureOpenAIApiDeploymentName}`;\n  }\n\n  if (azureOpenAIApiKey || azureADTokenProvider) {\n    if (!azureOpenAIApiInstanceName) {\n      throw new Error(\n        \"azureOpenAIApiInstanceName is required when using azureOpenAIApiKey\"\n      );\n    }\n    if (!azureOpenAIApiDeploymentName) {\n      throw new Error(\n        \"azureOpenAIApiDeploymentName is a required parameter when using azureOpenAIApiKey\"\n      );\n    }\n    return `https://${azureOpenAIApiInstanceName}.openai.azure.com/openai/deployments/${azureOpenAIApiDeploymentName}`;\n  }\n\n  return baseURL;\n}\n\ntype HeaderValue = string | undefined | null;\nexport type HeadersLike =\n  | Headers\n  | readonly HeaderValue[][]\n  | Record<string, HeaderValue | readonly HeaderValue[]>\n  | undefined\n  | null\n  // NullableHeaders\n  | { values: Headers; [key: string]: unknown };\n\nexport function isHeaders(headers: unknown): headers is Headers {\n  return (\n    typeof Headers !== \"undefined\" &&\n    headers !== null &&\n    typeof headers === \"object\" &&\n    Object.prototype.toString.call(headers) === \"[object Headers]\"\n  );\n}\n\n/**\n * Normalizes various header formats into a consistent Record format.\n *\n * This function accepts headers in multiple formats and converts them to a\n * Record<string, HeaderValue | readonly HeaderValue[]> for consistent handling.\n *\n * @param headers - The headers to normalize. Can be:\n *   - A Headers instance\n *   - An array of [key, value] pairs\n *   - A plain object with string keys\n *   - A NullableHeaders-like object with a 'values' property containing Headers\n *   - null or undefined\n * @returns A normalized Record containing the header key-value pairs\n *\n * @example\n * ```ts\n * // With Headers instance\n * const headers1 = new Headers([['content-type', 'application/json']]);\n * const normalized1 = normalizeHeaders(headers1);\n *\n * // With plain object\n * const headers2 = { 'content-type': 'application/json' };\n * const normalized2 = normalizeHeaders(headers2);\n *\n * // With array of pairs\n * const headers3 = [['content-type', 'application/json']];\n * const normalized3 = normalizeHeaders(headers3);\n * ```\n */\nexport function normalizeHeaders(\n  headers: HeadersLike\n): Record<string, HeaderValue | readonly HeaderValue[]> {\n  const output = iife(() => {\n    // If headers is a Headers instance\n    if (isHeaders(headers)) {\n      return headers;\n    }\n    // If headers is an array of [key, value] pairs\n    else if (Array.isArray(headers)) {\n      return new Headers(headers);\n    }\n    // If headers is a NullableHeaders-like object (has 'values' property that is a Headers)\n    else if (\n      typeof headers === \"object\" &&\n      headers !== null &&\n      \"values\" in headers &&\n      isHeaders(headers.values)\n    ) {\n      return headers.values;\n    }\n    // If headers is a plain object\n    else if (typeof headers === \"object\" && headers !== null) {\n      const entries: [string, string][] = Object.entries(headers)\n        .filter(([, v]) => typeof v === \"string\")\n        .map(([k, v]) => [k, v as string]);\n      return new Headers(entries);\n    }\n    return new Headers();\n  });\n\n  return Object.fromEntries(output.entries());\n}\n\nexport function getFormattedEnv() {\n  let env = getEnv();\n  if (env === \"node\" || env === \"deno\") {\n    env = `(${env}/${process.version}; ${process.platform}; ${process.arch})`;\n  }\n  return env;\n}\n\n// Note: ideally version would be imported from package.json, but there's\n// currently no good way to do that for all supported environments (Node, Deno, Browser).\nexport function getHeadersWithUserAgent(\n  headers: HeadersLike,\n  isAzure = false,\n  version = \"1.0.0\"\n): Record<string, string> {\n  const normalizedHeaders = normalizeHeaders(headers);\n  const env = getFormattedEnv();\n  const library = `langchainjs${isAzure ? \"-azure\" : \"\"}-openai`;\n  return {\n    ...normalizedHeaders,\n    \"User-Agent\": normalizedHeaders[\"User-Agent\"]\n      ? `${library}/${version} (${env})${normalizedHeaders[\"User-Agent\"]}`\n      : `${library}/${version} (${env})`,\n  };\n}\n"],"names":["config: OpenAIEndpointConfig","headers: unknown","headers: HeadersLike","entries: [string, string][]"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAqCA,SAAgB,YAAYA,MAAAA,EAA8B;IACxD,MAAM,EACJ,4BAAA,EACA,0BAAA,EACA,iBAAA,EACA,mBAAA,EACA,OAAA,EACA,oBAAA,EACA,mBAAA,EACD,GAAG;IAEJ,IAAA,CACG,qBAAqB,oBAAA,KACtB,uBACA,6BAEA,CAAA,OAAO,GAAG,oBAAoB,CAAC,EAAE,8BAA8B;IAEjE,IAAA,CACG,qBAAqB,oBAAA,KACtB,uBACA,6BAEA,CAAA,OAAO,GAAG,oBAAoB,oBAAoB,EAAE,8BAA8B;IAGpF,IAAI,qBAAqB,sBAAsB;QAC7C,IAAI,CAAC,2BACH,CAAA,MAAM,IAAI,MACR;QAGJ,IAAI,CAAC,6BACH,CAAA,MAAM,IAAI,MACR;QAGJ,OAAO,CAAC,QAAQ,EAAE,2BAA2B,qCAAqC,EAAE,8BAA8B;IACnH;IAED,OAAO;AACR;AAYD,SAAgB,UAAUC,OAAAA,EAAsC;IAC9D,OACE,OAAO,YAAY,eACnB,YAAY,QACZ,OAAO,YAAY,YACnB,OAAO,SAAA,CAAU,QAAA,CAAS,IAAA,CAAK,QAAQ,KAAK;AAE/C;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA+BD,SAAgB,iBACdC,OAAAA,EACsD;IACtD,MAAM,aAAS,wKAAA,EAAK,MAAM;QAExB,IAAI,UAAU,QAAQ,CACpB,CAAA,OAAO;iBAGA,MAAM,OAAA,CAAQ,QAAQ,CAC7B,CAAA,OAAO,IAAI,QAAQ;iBAInB,OAAO,YAAY,YACnB,YAAY,QACZ,YAAY,WACZ,UAAU,QAAQ,MAAA,CAAO,CAEzB,CAAA,OAAO,QAAQ,MAAA;iBAGR,OAAO,YAAY,YAAY,YAAY,MAAM;YACxD,MAAMC,UAA8B,OAAO,OAAA,CAAQ,QAAQ,CACxD,MAAA,CAAO,CAAC,GAAG,EAAE,GAAK,OAAO,MAAM,SAAS,CACxC,GAAA,CAAI,CAAC,CAAC,GAAG,EAAE,GAAK;oBAAC;oBAAG,CAAY;iBAAA,CAAC;YACpC,OAAO,IAAI,QAAQ;QACpB;QACD,OAAO,IAAI;IACZ,EAAC;IAEF,OAAO,OAAO,WAAA,CAAY,OAAO,OAAA,EAAS,CAAC;AAC5C;AAED,SAAgB,kBAAkB;IAChC,IAAI,UAAM,uKAAA,EAAQ;IAClB,IAAI,QAAQ,UAAU,QAAQ,QAC5B,MAAM,CAAC,CAAC,EAAE,IAAI,CAAC,EAAE,QAAQ,OAAA,CAAQ,EAAE,EAAE,QAAQ,QAAA,CAAS,EAAE,EAAE,QAAQ,IAAA,CAAK,CAAC,CAAC;IAE3E,OAAO;AACR;AAID,SAAgB,wBACdD,OAAAA,EACA,UAAU,KAAA,EACV,UAAU,OAAA,EACc;IACxB,MAAM,oBAAoB,iBAAiB,QAAQ;IACnD,MAAM,MAAM,iBAAiB;IAC7B,MAAM,UAAU,CAAC,WAAW,EAAE,UAAU,WAAW,GAAG,OAAO,CAAC;IAC9D,OAAO;QACL,GAAG,iBAAA;QACH,cAAc,iBAAA,CAAkB,aAAA,GAC5B,GAAG,QAAQ,CAAC,EAAE,QAAQ,EAAE,EAAE,IAAI,CAAC,EAAE,iBAAA,CAAkB,aAAA,EAAe,GAClE,GAAG,QAAQ,CAAC,EAAE,QAAQ,EAAE,EAAE,IAAI,CAAC,CAAC;IACrC;AACF"}},
    {"offset": {"line": 231, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/utils/tools.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/utils/tools.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\n\nimport { ToolDefinition } from \"@langchain/core/language_models/base\";\nimport { BindToolsInput } from \"@langchain/core/language_models/chat_models\";\nimport {\n  convertToOpenAITool as formatToOpenAITool,\n  isLangChainTool,\n} from \"@langchain/core/utils/function_calling\";\nimport { DynamicTool, StructuredToolInterface } from \"@langchain/core/tools\";\nimport { isInteropZodSchema } from \"@langchain/core/utils/types\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\nimport { ToolCall } from \"@langchain/core/messages/tool\";\n\n/**\n * Formats a tool in either OpenAI format, or LangChain structured tool format\n * into an OpenAI tool format. If the tool is already in OpenAI format, return without\n * any changes. If it is in LangChain structured tool format, convert it to OpenAI tool format\n * using OpenAI's `zodFunction` util, falling back to `convertToOpenAIFunction` if the parameters\n * returned from the `zodFunction` util are not defined.\n *\n * @param {BindToolsInput} tool The tool to convert to an OpenAI tool.\n * @param {Object} [fields] Additional fields to add to the OpenAI tool.\n * @returns {ToolDefinition} The inputted tool in OpenAI tool format.\n */\nexport function _convertToOpenAITool(\n  tool: BindToolsInput,\n  fields?: {\n    /**\n     * If `true`, model output is guaranteed to exactly match the JSON Schema\n     * provided in the function definition.\n     */\n    strict?: boolean;\n  }\n): OpenAIClient.ChatCompletionTool {\n  let toolDef: OpenAIClient.ChatCompletionTool | undefined;\n\n  if (isLangChainTool(tool)) {\n    toolDef = formatToOpenAITool(tool);\n  } else {\n    toolDef = tool as ToolDefinition;\n  }\n\n  if (fields?.strict !== undefined) {\n    toolDef.function.strict = fields.strict;\n  }\n\n  return toolDef;\n}\n\ntype OpenAIFunction = OpenAIClient.Chat.ChatCompletionCreateParams.Function;\n\n// Types representing the OpenAI function definitions. While the OpenAI client library\n// does have types for function definitions, the properties are just Record<string, unknown>,\n// which isn't very useful for type checking this formatting code.\nexport interface FunctionDef extends Omit<OpenAIFunction, \"parameters\"> {\n  name: string;\n  description?: string;\n  parameters: ObjectProp;\n}\n\ninterface ObjectProp {\n  type: \"object\";\n  properties?: {\n    [key: string]: Prop;\n  };\n  required?: string[];\n}\n\ninterface AnyOfProp {\n  anyOf: Prop[];\n}\n\ntype Prop = {\n  description?: string;\n} & (\n  | AnyOfProp\n  | ObjectProp\n  | {\n      type: \"string\";\n      enum?: string[];\n    }\n  | {\n      type: \"number\" | \"integer\";\n      minimum?: number;\n      maximum?: number;\n      enum?: number[];\n    }\n  | { type: \"boolean\" }\n  | { type: \"null\" }\n  | {\n      type: \"array\";\n      items?: Prop;\n    }\n);\n\nfunction isAnyOfProp(prop: Prop): prop is AnyOfProp {\n  return (\n    (prop as AnyOfProp).anyOf !== undefined &&\n    Array.isArray((prop as AnyOfProp).anyOf)\n  );\n}\n\n// When OpenAI use functions in the prompt, they format them as TypeScript definitions rather than OpenAPI JSON schemas.\n// This function converts the JSON schemas into TypeScript definitions.\nexport function formatFunctionDefinitions(functions: FunctionDef[]) {\n  const lines = [\"namespace functions {\", \"\"];\n  for (const f of functions) {\n    if (f.description) {\n      lines.push(`// ${f.description}`);\n    }\n    if (Object.keys(f.parameters.properties ?? {}).length > 0) {\n      lines.push(`type ${f.name} = (_: {`);\n      lines.push(formatObjectProperties(f.parameters, 0));\n      lines.push(\"}) => any;\");\n    } else {\n      lines.push(`type ${f.name} = () => any;`);\n    }\n    lines.push(\"\");\n  }\n  lines.push(\"} // namespace functions\");\n  return lines.join(\"\\n\");\n}\n\n// Format just the properties of an object (not including the surrounding braces)\nfunction formatObjectProperties(obj: ObjectProp, indent: number): string {\n  const lines: string[] = [];\n  for (const [name, param] of Object.entries(obj.properties ?? {})) {\n    if (param.description && indent < 2) {\n      lines.push(`// ${param.description}`);\n    }\n    if (obj.required?.includes(name)) {\n      lines.push(`${name}: ${formatType(param, indent)},`);\n    } else {\n      lines.push(`${name}?: ${formatType(param, indent)},`);\n    }\n  }\n  return lines.map((line) => \" \".repeat(indent) + line).join(\"\\n\");\n}\n\n// Format a single property type\nfunction formatType(param: Prop, indent: number): string {\n  if (isAnyOfProp(param)) {\n    return param.anyOf.map((v) => formatType(v, indent)).join(\" | \");\n  }\n  switch (param.type) {\n    case \"string\":\n      if (param.enum) {\n        return param.enum.map((v) => `\"${v}\"`).join(\" | \");\n      }\n      return \"string\";\n    case \"number\":\n      if (param.enum) {\n        return param.enum.map((v) => `${v}`).join(\" | \");\n      }\n      return \"number\";\n    case \"integer\":\n      if (param.enum) {\n        return param.enum.map((v) => `${v}`).join(\" | \");\n      }\n      return \"number\";\n    case \"boolean\":\n      return \"boolean\";\n    case \"null\":\n      return \"null\";\n    case \"object\":\n      return [\"{\", formatObjectProperties(param, indent + 2), \"}\"].join(\"\\n\");\n    case \"array\":\n      if (param.items) {\n        return `${formatType(param.items, indent)}[]`;\n      }\n      return \"any[]\";\n    default:\n      return \"\";\n  }\n}\n\nexport function formatToOpenAIAssistantTool(\n  tool: StructuredToolInterface\n): ToolDefinition {\n  return {\n    type: \"function\",\n    function: {\n      name: tool.name,\n      description: tool.description,\n      parameters: isInteropZodSchema(tool.schema)\n        ? toJsonSchema(tool.schema)\n        : tool.schema,\n    },\n  };\n}\n\nexport type OpenAIToolChoice =\n  | OpenAIClient.ChatCompletionToolChoiceOption\n  | \"any\"\n  | string;\n\nexport type ResponsesToolChoice = NonNullable<\n  OpenAIClient.Responses.ResponseCreateParams[\"tool_choice\"]\n>;\n\nexport type ChatOpenAIToolType =\n  | BindToolsInput\n  | OpenAIClient.Chat.ChatCompletionTool\n  | ResponsesTool;\n\nexport type ResponsesTool = NonNullable<\n  OpenAIClient.Responses.ResponseCreateParams[\"tools\"]\n>[number];\n\nexport function formatToOpenAIToolChoice(\n  toolChoice?: OpenAIToolChoice\n): OpenAIClient.ChatCompletionToolChoiceOption | undefined {\n  if (!toolChoice) {\n    return undefined;\n  } else if (toolChoice === \"any\" || toolChoice === \"required\") {\n    return \"required\";\n  } else if (toolChoice === \"auto\") {\n    return \"auto\";\n  } else if (toolChoice === \"none\") {\n    return \"none\";\n  } else if (typeof toolChoice === \"string\") {\n    return {\n      type: \"function\",\n      function: {\n        name: toolChoice,\n      },\n    };\n  } else {\n    return toolChoice;\n  }\n}\n\nexport function isBuiltInTool(tool: ChatOpenAIToolType): tool is ResponsesTool {\n  return \"type\" in tool && tool.type !== \"function\";\n}\n\n/**\n * Type for LangChain tools that have a provider-specific tool definition\n * stored in extras.providerToolDefinition.\n */\ntype LangchainToolWithProviderDefinition = StructuredToolInterface & {\n  extras: {\n    providerToolDefinition: ResponsesTool;\n  };\n};\n\n/**\n * Checks if a tool has a provider-specific tool definition in extras.providerToolDefinition.\n * This is used for tools like localShell, shell, computerUse, and applyPatch\n * that need to be sent as built-in tool types to the OpenAI API.\n */\nexport function hasProviderToolDefinition(\n  tool: unknown\n): tool is LangchainToolWithProviderDefinition {\n  return (\n    typeof tool === \"object\" &&\n    tool !== null &&\n    \"extras\" in tool &&\n    typeof (tool as LangchainToolWithProviderDefinition).extras === \"object\" &&\n    (tool as LangchainToolWithProviderDefinition).extras !== null &&\n    \"providerToolDefinition\" in\n      (tool as LangchainToolWithProviderDefinition).extras &&\n    typeof (tool as LangchainToolWithProviderDefinition).extras\n      .providerToolDefinition === \"object\" &&\n    (tool as LangchainToolWithProviderDefinition).extras\n      .providerToolDefinition !== null\n  );\n}\n\nexport function isBuiltInToolChoice(\n  tool_choice: OpenAIToolChoice | ResponsesToolChoice | undefined\n): tool_choice is ResponsesToolChoice {\n  return (\n    tool_choice != null &&\n    typeof tool_choice === \"object\" &&\n    \"type\" in tool_choice &&\n    tool_choice.type !== \"function\"\n  );\n}\n\nexport type CustomToolCall = ToolCall & {\n  call_id: string;\n  isCustomTool: true;\n};\n\ntype LangchainCustomTool = DynamicTool<string> & {\n  metadata: {\n    customTool: OpenAIClient.Responses.CustomTool;\n  };\n};\n\nexport function isCustomTool(tool: unknown): tool is LangchainCustomTool {\n  return (\n    typeof tool === \"object\" &&\n    tool !== null &&\n    \"metadata\" in tool &&\n    typeof tool.metadata === \"object\" &&\n    tool.metadata !== null &&\n    \"customTool\" in tool.metadata &&\n    typeof tool.metadata.customTool === \"object\" &&\n    tool.metadata.customTool !== null\n  );\n}\n\nexport function isOpenAICustomTool(\n  tool: ChatOpenAIToolType\n): tool is OpenAIClient.Chat.ChatCompletionCustomTool {\n  return (\n    \"type\" in tool &&\n    tool.type === \"custom\" &&\n    \"custom\" in tool &&\n    typeof tool.custom === \"object\" &&\n    tool.custom !== null\n  );\n}\n\nexport function parseCustomToolCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>\n): CustomToolCall | undefined {\n  if (rawToolCall.type !== \"custom_tool_call\") {\n    return undefined;\n  }\n  return {\n    ...rawToolCall,\n    type: \"tool_call\",\n    call_id: rawToolCall.id,\n    id: rawToolCall.call_id,\n    name: rawToolCall.name,\n    isCustomTool: true,\n    args: {\n      input: rawToolCall.input,\n    },\n  };\n}\n\nexport type ComputerToolCall = ToolCall & {\n  call_id: string;\n  /**\n   * marker to indicate that the tool call is a computer tool call\n   */\n  isComputerTool: true;\n};\n\n/**\n * Parses a computer_call output item from the OpenAI Responses API\n * into a ToolCall format that can be processed by the ToolNode.\n *\n * @param rawToolCall - The raw computer_call output item from the API\n * @returns A ComputerToolCall object if valid, undefined otherwise\n */\nexport function parseComputerCall(\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  rawToolCall: Record<string, any>\n): ComputerToolCall | undefined {\n  if (rawToolCall.type !== \"computer_call\") {\n    return undefined;\n  }\n  return {\n    ...rawToolCall,\n    type: \"tool_call\",\n    call_id: rawToolCall.id,\n    id: rawToolCall.call_id,\n    name: \"computer_use\",\n    isComputerTool: true,\n    args: {\n      action: rawToolCall.action,\n    },\n  };\n}\n\n/**\n * Checks if a tool call is a computer tool call.\n * @param toolCall - The tool call to check.\n * @returns True if the tool call is a computer tool call, false otherwise.\n */\nexport function isComputerToolCall(\n  toolCall: unknown\n): toolCall is ComputerToolCall {\n  return (\n    typeof toolCall === \"object\" &&\n    toolCall !== null &&\n    \"type\" in toolCall &&\n    toolCall.type === \"tool_call\" &&\n    \"isComputerTool\" in toolCall &&\n    toolCall.isComputerTool === true\n  );\n}\n\nexport function isCustomToolCall(\n  toolCall: unknown\n): toolCall is CustomToolCall {\n  return (\n    typeof toolCall === \"object\" &&\n    toolCall !== null &&\n    \"type\" in toolCall &&\n    toolCall.type === \"tool_call\" &&\n    \"isCustomTool\" in toolCall &&\n    toolCall.isCustomTool === true\n  );\n}\n\nexport function convertCompletionsCustomTool(\n  tool: OpenAIClient.Chat.ChatCompletionCustomTool\n): OpenAIClient.Responses.CustomTool {\n  const getFormat = () => {\n    if (!tool.custom.format) {\n      return undefined;\n    }\n    if (tool.custom.format.type === \"grammar\") {\n      return {\n        type: \"grammar\" as const,\n        definition: tool.custom.format.grammar.definition,\n        syntax: tool.custom.format.grammar.syntax,\n      };\n    }\n    if (tool.custom.format.type === \"text\") {\n      return {\n        type: \"text\" as const,\n      };\n    }\n    return undefined;\n  };\n  return {\n    type: \"custom\",\n    name: tool.custom.name,\n    description: tool.custom.description,\n    format: getFormat(),\n  };\n}\n\nexport function convertResponsesCustomTool(\n  tool: OpenAIClient.Responses.CustomTool\n): OpenAIClient.Chat.ChatCompletionCustomTool {\n  const getFormat = () => {\n    if (!tool.format) {\n      return undefined;\n    }\n    if (tool.format.type === \"grammar\") {\n      return {\n        type: \"grammar\" as const,\n        grammar: {\n          definition: tool.format.definition,\n          syntax: tool.format.syntax,\n        },\n      };\n    }\n    if (tool.format.type === \"text\") {\n      return {\n        type: \"text\" as const,\n      };\n    }\n    return undefined;\n  };\n  return {\n    type: \"custom\",\n    custom: {\n      name: tool.name,\n      description: tool.description,\n      format: getFormat(),\n    },\n  };\n}\n"],"names":["tool: BindToolsInput","fields?: {\n    /**\n     * If `true`, model output is guaranteed to exactly match the JSON Schema\n     * provided in the function definition.\n     */\n    strict?: boolean;\n  }","toolDef: OpenAIClient.ChatCompletionTool | undefined","formatToOpenAITool","prop: Prop","functions: FunctionDef[]","obj: ObjectProp","indent: number","lines: string[]","param: Prop","toolChoice?: OpenAIToolChoice","tool: ChatOpenAIToolType","tool: unknown","tool_choice: OpenAIToolChoice | ResponsesToolChoice | undefined","rawToolCall: Record<string, any>","toolCall: unknown","tool: OpenAIClient.Chat.ChatCompletionCustomTool","tool: OpenAIClient.Responses.CustomTool"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAwBA,SAAgB,qBACdA,IAAAA,EACAC,MAAAA,EAOiC;IACjC,IAAIC;IAEJ,QAAI,kLAAA,EAAgB,KAAK,EACvB,cAAUC,iNAAAA,EAAmB,KAAK;SAElC,UAAU;IAGZ,IAAI,QAAQ,WAAW,KAAA,GACrB,QAAQ,QAAA,CAAS,MAAA,GAAS,OAAO,MAAA;IAGnC,OAAO;AACR;AAgDD,SAAS,YAAYC,IAAAA,EAA+B;IAClD,OACG,KAAmB,KAAA,KAAU,KAAA,KAC9B,MAAM,OAAA,CAAS,KAAmB,KAAA,CAAM;AAE3C;AAID,SAAgB,0BAA0BC,SAAAA,EAA0B;IAClE,MAAM,QAAQ;QAAC;QAAyB,EAAG;KAAA;IAC3C,KAAK,MAAM,KAAK,UAAW;QACzB,IAAI,EAAE,WAAA,EACJ,MAAM,IAAA,CAAK,CAAC,GAAG,EAAE,EAAE,WAAA,EAAa,CAAC;QAEnC,IAAI,OAAO,IAAA,CAAK,EAAE,UAAA,CAAW,UAAA,IAAc,CAAE,EAAC,CAAC,MAAA,GAAS,GAAG;YACzD,MAAM,IAAA,CAAK,CAAC,KAAK,EAAE,EAAE,IAAA,CAAK,QAAQ,CAAC,CAAC;YACpC,MAAM,IAAA,CAAK,uBAAuB,EAAE,UAAA,EAAY,EAAE,CAAC;YACnD,MAAM,IAAA,CAAK,aAAa;QACzB,OACC,MAAM,IAAA,CAAK,CAAC,KAAK,EAAE,EAAE,IAAA,CAAK,aAAa,CAAC,CAAC;QAE3C,MAAM,IAAA,CAAK,GAAG;IACf;IACD,MAAM,IAAA,CAAK,2BAA2B;IACtC,OAAO,MAAM,IAAA,CAAK,KAAK;AACxB;AAGD,SAAS,uBAAuBC,GAAAA,EAAiBC,MAAAA,EAAwB;IACvE,MAAMC,QAAkB,CAAE,CAAA;IAC1B,KAAK,MAAM,CAAC,MAAM,MAAM,IAAI,OAAO,OAAA,CAAQ,IAAI,UAAA,IAAc,CAAE,EAAC,CAAE;QAChE,IAAI,MAAM,WAAA,IAAe,SAAS,GAChC,MAAM,IAAA,CAAK,CAAC,GAAG,EAAE,MAAM,WAAA,EAAa,CAAC;QAEvC,IAAI,IAAI,QAAA,EAAU,SAAS,KAAK,EAC9B,MAAM,IAAA,CAAK,GAAG,KAAK,EAAE,EAAE,WAAW,OAAO,OAAO,CAAC,CAAC,CAAC,CAAC;aAEpD,MAAM,IAAA,CAAK,GAAG,KAAK,GAAG,EAAE,WAAW,OAAO,OAAO,CAAC,CAAC,CAAC,CAAC;IAExD;IACD,OAAO,MAAM,GAAA,CAAI,CAAC,OAAS,IAAI,MAAA,CAAO,OAAO,GAAG,KAAK,CAAC,IAAA,CAAK,KAAK;AACjE;AAGD,SAAS,WAAWC,KAAAA,EAAaF,MAAAA,EAAwB;IACvD,IAAI,YAAY,MAAM,CACpB,CAAA,OAAO,MAAM,KAAA,CAAM,GAAA,CAAI,CAAC,IAAM,WAAW,GAAG,OAAO,CAAC,CAAC,IAAA,CAAK,MAAM;IAElE,OAAQ,MAAM,IAAA,EAAd;QACE,KAAK;YACH,IAAI,MAAM,IAAA,CACR,CAAA,OAAO,MAAM,IAAA,CAAK,GAAA,CAAI,CAAC,IAAM,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,IAAA,CAAK,MAAM;YAEpD,OAAO;QACT,KAAK;YACH,IAAI,MAAM,IAAA,CACR,CAAA,OAAO,MAAM,IAAA,CAAK,GAAA,CAAI,CAAC,IAAM,GAAG,GAAG,CAAC,CAAC,IAAA,CAAK,MAAM;YAElD,OAAO;QACT,KAAK;YACH,IAAI,MAAM,IAAA,CACR,CAAA,OAAO,MAAM,IAAA,CAAK,GAAA,CAAI,CAAC,IAAM,GAAG,GAAG,CAAC,CAAC,IAAA,CAAK,MAAM;YAElD,OAAO;QACT,KAAK,UACH;YAAA,OAAO;QACT,KAAK,OACH;YAAA,OAAO;QACT,KAAK,SACH;YAAA,OAAO;gBAAC;gBAAK,uBAAuB,OAAO,SAAS,EAAE;gBAAE;aAAI,CAAC,IAAA,CAAK,KAAK;QACzE,KAAK;YACH,IAAI,MAAM,KAAA,CACR,CAAA,OAAO,GAAG,WAAW,MAAM,KAAA,EAAO,OAAO,CAAC,EAAE,CAAC;YAE/C,OAAO;QACT,QACE;YAAA,OAAO;IACV;AACF;AAmCD,SAAgB,yBACdG,UAAAA,EACyD;IACzD,IAAI,CAAC,WACH,CAAA,OAAO,KAAA;aACE,eAAe,SAAS,eAAe,WAChD,CAAA,OAAO;aACE,eAAe,OACxB,CAAA,OAAO;aACE,eAAe,OACxB,CAAA,OAAO;aACE,OAAO,eAAe,SAC/B,CAAA,OAAO;QACL,MAAM;QACN,UAAU;YACR,MAAM;QACP;IACF;SAED,OAAO;AAEV;AAED,SAAgB,cAAcC,IAAAA,EAAiD;IAC7E,OAAO,UAAU,QAAQ,KAAK,IAAA,KAAS;AACxC;;;;;GAiBD,SAAgB,0BACdC,IAAAA,EAC6C;IAC7C,OACE,OAAO,SAAS,YAChB,SAAS,QACT,YAAY,QACZ,OAAQ,KAA6C,MAAA,KAAW,YAC/D,KAA6C,MAAA,KAAW,QACzD,4BACG,KAA6C,MAAA,IAChD,OAAQ,KAA6C,MAAA,CAClD,sBAAA,KAA2B,YAC7B,KAA6C,MAAA,CAC3C,sBAAA,KAA2B;AAEjC;AAED,SAAgB,oBACdC,WAAAA,EACoC;IACpC,OACE,eAAe,QACf,OAAO,gBAAgB,YACvB,UAAU,eACV,YAAY,IAAA,KAAS;AAExB;AAaD,SAAgB,aAAaD,IAAAA,EAA4C;IACvE,OACE,OAAO,SAAS,YAChB,SAAS,QACT,cAAc,QACd,OAAO,KAAK,QAAA,KAAa,YACzB,KAAK,QAAA,KAAa,QAClB,gBAAgB,KAAK,QAAA,IACrB,OAAO,KAAK,QAAA,CAAS,UAAA,KAAe,YACpC,KAAK,QAAA,CAAS,UAAA,KAAe;AAEhC;AAED,SAAgB,mBACdD,IAAAA,EACoD;IACpD,OACE,UAAU,QACV,KAAK,IAAA,KAAS,YACd,YAAY,QACZ,OAAO,KAAK,MAAA,KAAW,YACvB,KAAK,MAAA,KAAW;AAEnB;AAED,SAAgB,oBAEdG,WAAAA,EAC4B;IAC5B,IAAI,YAAY,IAAA,KAAS,mBACvB,CAAA,OAAO,KAAA;IAET,OAAO;QACL,GAAG,WAAA;QACH,MAAM;QACN,SAAS,YAAY,EAAA;QACrB,IAAI,YAAY,OAAA;QAChB,MAAM,YAAY,IAAA;QAClB,cAAc;QACd,MAAM;YACJ,OAAO,YAAY,KAAA;QACpB;IACF;AACF;;;;;;;GAiBD,SAAgB,kBAEdA,WAAAA,EAC8B;IAC9B,IAAI,YAAY,IAAA,KAAS,gBACvB,CAAA,OAAO,KAAA;IAET,OAAO;QACL,GAAG,WAAA;QACH,MAAM;QACN,SAAS,YAAY,EAAA;QACrB,IAAI,YAAY,OAAA;QAChB,MAAM;QACN,gBAAgB;QAChB,MAAM;YACJ,QAAQ,YAAY,MAAA;QACrB;IACF;AACF;;;;;GAOD,SAAgB,mBACdC,QAAAA,EAC8B;IAC9B,OACE,OAAO,aAAa,YACpB,aAAa,QACb,UAAU,YACV,SAAS,IAAA,KAAS,eAClB,oBAAoB,YACpB,SAAS,cAAA,KAAmB;AAE/B;AAED,SAAgB,iBACdA,QAAAA,EAC4B;IAC5B,OACE,OAAO,aAAa,YACpB,aAAa,QACb,UAAU,YACV,SAAS,IAAA,KAAS,eAClB,kBAAkB,YAClB,SAAS,YAAA,KAAiB;AAE7B;AAED,SAAgB,6BACdC,IAAAA,EACmC;IACnC,MAAM,YAAY,MAAM;QACtB,IAAI,CAAC,KAAK,MAAA,CAAO,MAAA,CACf,CAAA,OAAO,KAAA;QAET,IAAI,KAAK,MAAA,CAAO,MAAA,CAAO,IAAA,KAAS,UAC9B,CAAA,OAAO;YACL,MAAM;YACN,YAAY,KAAK,MAAA,CAAO,MAAA,CAAO,OAAA,CAAQ,UAAA;YACvC,QAAQ,KAAK,MAAA,CAAO,MAAA,CAAO,OAAA,CAAQ,MAAA;QACpC;QAEH,IAAI,KAAK,MAAA,CAAO,MAAA,CAAO,IAAA,KAAS,OAC9B,CAAA,OAAO;YACL,MAAM;QACP;QAEH,OAAO,KAAA;IACR;IACD,OAAO;QACL,MAAM;QACN,MAAM,KAAK,MAAA,CAAO,IAAA;QAClB,aAAa,KAAK,MAAA,CAAO,WAAA;QACzB,QAAQ,WAAW;IACpB;AACF;AAED,SAAgB,2BACdC,IAAAA,EAC4C;IAC5C,MAAM,YAAY,MAAM;QACtB,IAAI,CAAC,KAAK,MAAA,CACR,CAAA,OAAO,KAAA;QAET,IAAI,KAAK,MAAA,CAAO,IAAA,KAAS,UACvB,CAAA,OAAO;YACL,MAAM;YACN,SAAS;gBACP,YAAY,KAAK,MAAA,CAAO,UAAA;gBACxB,QAAQ,KAAK,MAAA,CAAO,MAAA;YACrB;QACF;QAEH,IAAI,KAAK,MAAA,CAAO,IAAA,KAAS,OACvB,CAAA,OAAO;YACL,MAAM;QACP;QAEH,OAAO,KAAA;IACR;IACD,OAAO;QACL,MAAM;QACN,QAAQ;YACN,MAAM,KAAK,IAAA;YACX,aAAa,KAAK,WAAA;YAClB,QAAQ,WAAW;QACpB;IACF;AACF"}},
    {"offset": {"line": 470, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/utils/output.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/utils/output.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport {\n  InteropZodType,\n  isZodSchemaV3,\n  isZodSchemaV4,\n} from \"@langchain/core/utils/types\";\nimport { parse as parseV4 } from \"zod/v4/core\";\nimport { ResponseFormatJSONSchema } from \"openai/resources\";\nimport { zodResponseFormat } from \"openai/helpers/zod\";\nimport { ContentBlock, UsageMetadata } from \"@langchain/core/messages\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\n\nconst SUPPORTED_METHODS = [\n  \"jsonSchema\",\n  \"functionCalling\",\n  \"jsonMode\",\n] as const;\ntype SupportedMethod = (typeof SUPPORTED_METHODS)[number];\n\n/**\n * Get the structured output method for a given model. By default, it uses\n * `jsonSchema` if the model supports it, otherwise it uses `functionCalling`.\n *\n * @throws if the method is invalid, e.g. is not a string or invalid method is provided.\n * @param model - The model name.\n * @param config - The structured output method options.\n * @returns The structured output method.\n */\nexport function getStructuredOutputMethod(\n  model: string,\n  method: unknown\n): SupportedMethod {\n  /**\n   * If a method is provided, validate it.\n   */\n  if (\n    typeof method !== \"undefined\" &&\n    !SUPPORTED_METHODS.includes(method as SupportedMethod)\n  ) {\n    throw new Error(\n      `Invalid method: ${method}. Supported methods are: ${SUPPORTED_METHODS.join(\n        \", \"\n      )}`\n    );\n  }\n\n  const hasSupportForJsonSchema =\n    !model.startsWith(\"gpt-3\") &&\n    !model.startsWith(\"gpt-4-\") &&\n    model !== \"gpt-4\";\n\n  /**\n   * If the model supports JSON Schema, use it by default.\n   */\n  if (hasSupportForJsonSchema && !method) {\n    return \"jsonSchema\";\n  }\n\n  if (!hasSupportForJsonSchema && method === \"jsonSchema\") {\n    throw new Error(\n      `JSON Schema is not supported for model \"${model}\". Please use a different method, e.g. \"functionCalling\" or \"jsonMode\".`\n    );\n  }\n\n  /**\n   * If the model does not support JSON Schema, use function calling by default.\n   */\n  return (method as SupportedMethod) ?? \"functionCalling\";\n}\n\n// inlined from openai/lib/parser.ts\nfunction makeParseableResponseFormat<ParsedT>(\n  response_format: ResponseFormatJSONSchema,\n  parser: (content: string) => ParsedT\n) {\n  const obj = { ...response_format };\n\n  Object.defineProperties(obj, {\n    $brand: {\n      value: \"auto-parseable-response-format\",\n      enumerable: false,\n    },\n    $parseRaw: {\n      value: parser,\n      enumerable: false,\n    },\n  });\n\n  return obj;\n}\n\nexport function interopZodResponseFormat(\n  zodSchema: InteropZodType,\n  name: string,\n  props: Omit<ResponseFormatJSONSchema.JSONSchema, \"schema\" | \"strict\" | \"name\">\n) {\n  if (isZodSchemaV3(zodSchema)) {\n    return zodResponseFormat(zodSchema, name, props);\n  }\n  if (isZodSchemaV4(zodSchema)) {\n    return makeParseableResponseFormat(\n      {\n        type: \"json_schema\",\n        json_schema: {\n          ...props,\n          name,\n          strict: true,\n          schema: toJsonSchema(zodSchema, {\n            cycles: \"ref\", // equivalent to nameStrategy: 'duplicate-ref'\n            reused: \"ref\", // equivalent to $refStrategy: 'extract-to-root'\n            override(ctx) {\n              ctx.jsonSchema.title = name; // equivalent to `name` property\n              // TODO: implement `nullableStrategy` patch-fix (zod doesn't support openApi3 json schema target)\n              // TODO: implement `openaiStrictMode` patch-fix (where optional properties without `nullable` are not supported)\n            },\n            /// property equivalents from native `zodResponseFormat` fn\n            // openaiStrictMode: true,\n            // name,\n            // nameStrategy: 'duplicate-ref',\n            // $refStrategy: 'extract-to-root',\n            // nullableStrategy: 'property',\n          }),\n        },\n      },\n      (content) => parseV4(zodSchema, JSON.parse(content))\n    );\n  }\n  throw new Error(\"Unsupported schema response format\");\n}\n\n/**\n * Handle multi modal response content.\n *\n * @param content The content of the message.\n * @param messages The messages of the response.\n * @returns The new content of the message.\n */\nexport function handleMultiModalOutput(\n  content: string,\n  messages: unknown\n): ContentBlock[] | string {\n  /**\n   * Handle OpenRouter image responses\n   * @see https://openrouter.ai/docs/features/multimodal/image-generation#api-usage\n   */\n  if (\n    messages &&\n    typeof messages === \"object\" &&\n    \"images\" in messages &&\n    Array.isArray(messages.images)\n  ) {\n    const images = messages.images\n      .filter((image) => typeof image?.image_url?.url === \"string\")\n      .map(\n        (image) =>\n          ({\n            type: \"image\",\n            url: image.image_url.url as string,\n          } as const)\n      );\n    return [{ type: \"text\", text: content }, ...images];\n  }\n\n  return content;\n}\n\n// TODO: make this a converter\nexport function _convertOpenAIResponsesUsageToLangChainUsage(\n  usage?: OpenAIClient.Responses.ResponseUsage\n): UsageMetadata {\n  const inputTokenDetails = {\n    ...(usage?.input_tokens_details?.cached_tokens != null && {\n      cache_read: usage?.input_tokens_details?.cached_tokens,\n    }),\n  };\n  const outputTokenDetails = {\n    ...(usage?.output_tokens_details?.reasoning_tokens != null && {\n      reasoning: usage?.output_tokens_details?.reasoning_tokens,\n    }),\n  };\n  return {\n    input_tokens: usage?.input_tokens ?? 0,\n    output_tokens: usage?.output_tokens ?? 0,\n    total_tokens: usage?.total_tokens ?? 0,\n    input_token_details: inputTokenDetails,\n    output_token_details: outputTokenDetails,\n  };\n}\n"],"names":["model: string","method: unknown","response_format: ResponseFormatJSONSchema","parser: (content: string) => ParsedT","zodSchema: InteropZodType","name: string","props: Omit<ResponseFormatJSONSchema.JSONSchema, \"schema\" | \"strict\" | \"name\">","parseV4","content: string","messages: unknown"],"mappings":";;;;;;;;;;;;;;;;;;;AAYA,MAAM,oBAAoB;IACxB;IACA;IACA;CACD;;;;;;;;;GAYD,SAAgB,0BACdA,KAAAA,EACAC,MAAAA,EACiB;;;IAIjB,IACE,OAAO,WAAW,eAClB,CAAC,kBAAkB,QAAA,CAAS,OAA0B,CAEtD,CAAA,MAAM,IAAI,MACR,CAAC,gBAAgB,EAAE,OAAO,yBAAyB,EAAE,kBAAkB,IAAA,CACrE,KACD,EAAE;IAIP,MAAM,0BACJ,CAAC,MAAM,UAAA,CAAW,QAAQ,IAC1B,CAAC,MAAM,UAAA,CAAW,SAAS,IAC3B,UAAU;;;IAKZ,IAAI,2BAA2B,CAAC,OAC9B,CAAA,OAAO;IAGT,IAAI,CAAC,2BAA2B,WAAW,aACzC,CAAA,MAAM,IAAI,MACR,CAAC,wCAAwC,EAAE,MAAM,uEAAuE,CAAC;;;IAO7H,OAAQ,UAA8B;AACvC;AAGD,SAAS,4BACPC,eAAAA,EACAC,MAAAA,EACA;IACA,MAAM,MAAM;QAAE,GAAG,eAAA;IAAiB;IAElC,OAAO,gBAAA,CAAiB,KAAK;QAC3B,QAAQ;YACN,OAAO;YACP,YAAY;QACb;QACD,WAAW;YACT,OAAO;YACP,YAAY;QACb;IACF,EAAC;IAEF,OAAO;AACR;AAED,SAAgB,yBACdC,SAAAA,EACAC,IAAAA,EACAC,KAAAA,EACA;IACA,QAAI,uLAAA,EAAc,UAAU,CAC1B,CAAA,WAAO,gKAAA,EAAkB,WAAW,MAAM,MAAM;IAElD,QAAI,uLAAA,EAAc,UAAU,CAC1B,CAAA,OAAO,4BACL;QACE,MAAM;QACN,aAAa;YACX,GAAG,KAAA;YACH;YACA,QAAQ;YACR,YAAQ,qMAAA,EAAa,WAAW;gBAC9B,QAAQ;gBACR,QAAQ;gBACR,UAAS,GAAA,EAAK;oBACZ,IAAI,UAAA,CAAW,KAAA,GAAQ;gBAGxB;YAOF,EAAC;QACH;IACF,GACD,CAAC,cAAYC,qJAAAA,EAAQ,WAAW,KAAK,KAAA,CAAM,QAAQ,CAAC,CACrD;IAEH,MAAM,IAAI,MAAM;AACjB;;;;;;;GASD,SAAgB,uBACdC,OAAAA,EACAC,QAAAA,EACyB;;;;IAKzB,IACE,YACA,OAAO,aAAa,YACpB,YAAY,YACZ,MAAM,OAAA,CAAQ,SAAS,MAAA,CAAO,EAC9B;QACA,MAAM,SAAS,SAAS,MAAA,CACrB,MAAA,CAAO,CAAC,QAAU,OAAO,OAAO,WAAW,QAAQ,SAAS,CAC5D,GAAA,CACC,CAAC,QAAA,CACE;gBACC,MAAM;gBACN,KAAK,MAAM,SAAA,CAAU,GAAA;YACtB,CAAA,EACJ;QACH,OAAO;YAAC;gBAAE,MAAM;gBAAQ,MAAM;YAAS,GAAE;eAAG,MAAO;SAAA;IACpD;IAED,OAAO;AACR"}},
    {"offset": {"line": 581, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/chat_models/profiles.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/chat_models/profiles.ts"],"sourcesContent":["/**\n * This file was automatically generated by an automated script. Do not edit manually.\n */\nimport type { ModelProfile } from \"@langchain/core/language_models/profile\";\nconst PROFILES: Record<string, ModelProfile> = {\n  \"gpt-4.1-nano\": {\n    maxInputTokens: 1047576,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"text-embedding-3-small\": {\n    maxInputTokens: 8191,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 1536,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4\": {\n    maxInputTokens: 8192,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 8192,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o1-pro\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o-2024-05-13\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 4096,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o-2024-08-06\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4.1-mini\": {\n    maxInputTokens: 1047576,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o3-deep-research\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-3.5-turbo\": {\n    maxInputTokens: 16385,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: false,\n    videoInputs: false,\n    maxOutputTokens: 4096,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: false,\n    imageUrlInputs: false,\n    pdfToolMessage: false,\n    imageToolMessage: false,\n    toolChoice: true,\n  },\n  \"text-embedding-3-large\": {\n    maxInputTokens: 8191,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 3072,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4-turbo\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 4096,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o1-preview\": {\n    maxInputTokens: 128000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o3-mini\": {\n    maxInputTokens: 200000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"codex-mini-latest\": {\n    maxInputTokens: 200000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-nano\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-codex\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4.1\": {\n    maxInputTokens: 1047576,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 32768,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o4-mini\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  o1: {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-mini\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o1-mini\": {\n    maxInputTokens: 128000,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 65536,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"text-embedding-ada-002\": {\n    maxInputTokens: 8192,\n    imageInputs: false,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 1536,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o3-pro\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o-2024-11-20\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  o3: {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"o4-mini-deep-research\": {\n    maxInputTokens: 200000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 100000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-chat-latest\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: false,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-4o-mini\": {\n    maxInputTokens: 128000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 16384,\n    reasoningOutput: false,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 128000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n  \"gpt-5-pro\": {\n    maxInputTokens: 400000,\n    imageInputs: true,\n    audioInputs: false,\n    pdfInputs: true,\n    videoInputs: false,\n    maxOutputTokens: 272000,\n    reasoningOutput: true,\n    imageOutputs: false,\n    audioOutputs: false,\n    videoOutputs: false,\n    toolCalling: true,\n    structuredOutput: true,\n    imageUrlInputs: true,\n    pdfToolMessage: true,\n    imageToolMessage: true,\n    toolChoice: true,\n  },\n};\nexport default PROFILES;\n"],"names":["PROFILES: Record<string, ModelProfile>"],"mappings":";;;;;AAIA,MAAMA,WAAyC;IAC7C,gBAAgB;QACd,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,0BAA0B;QACxB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,SAAS;QACP,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,UAAU;QACR,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,qBAAqB;QACnB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,qBAAqB;QACnB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,gBAAgB;QACd,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,oBAAoB;QAClB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,iBAAiB;QACf,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,0BAA0B;QACxB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,eAAe;QACb,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,cAAc;QACZ,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,WAAW;QACT,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,qBAAqB;QACnB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,cAAc;QACZ,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,eAAe;QACb,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,UAAU;QACR,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,WAAW;QACT,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,WAAW;QACT,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,IAAI;QACF,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,cAAc;QACZ,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,WAAW;QACT,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,0BAA0B;QACxB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,UAAU;QACR,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,qBAAqB;QACnB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,IAAI;QACF,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,yBAAyB;QACvB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,qBAAqB;QACnB,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,eAAe;QACb,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,SAAS;QACP,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;IACD,aAAa;QACX,gBAAgB;QAChB,aAAa;QACb,aAAa;QACb,WAAW;QACX,aAAa;QACb,iBAAiB;QACjB,iBAAiB;QACjB,cAAc;QACd,cAAc;QACd,cAAc;QACd,aAAa;QACb,kBAAkB;QAClB,gBAAgB;QAChB,gBAAgB;QAChB,kBAAkB;QAClB,YAAY;IACb;AACF;AACD,IAAA,mBAAe"}},
    {"offset": {"line": 1153, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/chat_models/base.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/chat_models/base.ts"],"sourcesContent":["import OpenAI, { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { type ChatGeneration } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  BaseChatModel,\n  type LangSmithParams,\n  type BaseChatModelParams,\n  BaseChatModelCallOptions,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  isOpenAITool as isOpenAIFunctionTool,\n  type BaseFunctionCallOptions,\n  type BaseLanguageModelInput,\n  type FunctionDefinition,\n  type StructuredOutputMethodOptions,\n} from \"@langchain/core/language_models/base\";\nimport { ModelProfile } from \"@langchain/core/language_models/profile\";\nimport {\n  Runnable,\n  RunnableLambda,\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"@langchain/core/runnables\";\nimport {\n  JsonOutputParser,\n  StructuredOutputParser,\n} from \"@langchain/core/output_parsers\";\nimport { JsonOutputKeyToolsParser } from \"@langchain/core/output_parsers/openai_tools\";\nimport {\n  getSchemaDescription,\n  InteropZodType,\n  isInteropZodSchema,\n} from \"@langchain/core/utils/types\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\nimport {\n  type OpenAICallOptions,\n  type OpenAIChatInput,\n  type OpenAICoreRequestOptions,\n  type ChatOpenAIResponseFormat,\n  ResponseFormatConfiguration,\n  OpenAIVerbosityParam,\n  type OpenAIApiKey,\n  OpenAICacheRetentionParam,\n} from \"../types.js\";\nimport {\n  type OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"../utils/azure.js\";\nimport {\n  type FunctionDef,\n  formatFunctionDefinitions,\n  OpenAIToolChoice,\n  _convertToOpenAITool,\n  ChatOpenAIToolType,\n  convertResponsesCustomTool,\n  isBuiltInTool,\n  isCustomTool,\n  hasProviderToolDefinition,\n  ResponsesToolChoice,\n} from \"../utils/tools.js\";\nimport {\n  getStructuredOutputMethod,\n  interopZodResponseFormat,\n  _convertOpenAIResponsesUsageToLangChainUsage,\n} from \"../utils/output.js\";\nimport { isReasoningModel, messageToOpenAIRole } from \"../utils/misc.js\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\nimport PROFILES from \"./profiles.js\";\n\ninterface OpenAILLMOutput {\n  tokenUsage: {\n    completionTokens?: number;\n    promptTokens?: number;\n    totalTokens?: number;\n  };\n}\n\nexport type { OpenAICallOptions, OpenAIChatInput };\n\nexport interface BaseChatOpenAICallOptions\n  extends BaseChatModelCallOptions,\n    BaseFunctionCallOptions {\n  /**\n   * Additional options to pass to the underlying axios request.\n   */\n  options?: OpenAICoreRequestOptions;\n\n  /**\n   * A list of tools that the model may use to generate responses.\n   * Each tool can be a function, a built-in tool, or a custom tool definition.\n   * If not provided, the model will not use any tools.\n   */\n  tools?: ChatOpenAIToolType[];\n\n  /**\n   * Specifies which tool the model should use to respond.\n   * Can be an {@link OpenAIToolChoice} or a {@link ResponsesToolChoice}.\n   * If not set, the model will decide which tool to use automatically.\n   */\n  // TODO: break OpenAIToolChoice and ResponsesToolChoice into options sub classes\n  tool_choice?: OpenAIToolChoice | ResponsesToolChoice;\n\n  /**\n   * Adds a prompt index to prompts passed to the model to track\n   * what prompt is being used for a given generation.\n   */\n  promptIndex?: number;\n\n  /**\n   * An object specifying the format that the model must output.\n   */\n  response_format?: ChatOpenAIResponseFormat;\n\n  /**\n   * When provided, the completions API will make a best effort to sample\n   * deterministically, such that repeated requests with the same `seed`\n   * and parameters should return the same result.\n   */\n  seed?: number;\n\n  /**\n   * Additional options to pass to streamed completions.\n   * If provided, this takes precedence over \"streamUsage\" set at\n   * initialization time.\n   */\n  stream_options?: OpenAIClient.Chat.ChatCompletionStreamOptions;\n\n  /**\n   * The model may choose to call multiple functions in a single turn. You can\n   * set parallel_tool_calls to false which ensures only one tool is called at most.\n   * [Learn more](https://platform.openai.com/docs/guides/function-calling#parallel-function-calling)\n   */\n  parallel_tool_calls?: boolean;\n\n  /**\n   * If `true`, model output is guaranteed to exactly match the JSON Schema\n   * provided in the tool definition. If `true`, the input schema will also be\n   * validated according to\n   * https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n   *\n   * If `false`, input schema will not be validated and model output will not\n   * be validated.\n   *\n   * If `undefined`, `strict` argument will not be passed to the model.\n   */\n  strict?: boolean;\n\n  /**\n   * Output types that you would like the model to generate for this request. Most\n   * models are capable of generating text, which is the default:\n   *\n   * `[\"text\"]`\n   *\n   * The `gpt-4o-audio-preview` model can also be used to\n   * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n   * this model generate both text and audio responses, you can use:\n   *\n   * `[\"text\", \"audio\"]`\n   */\n  modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n\n  /**\n   * Parameters for audio output. Required when audio output is requested with\n   * `modalities: [\"audio\"]`.\n   * [Learn more](https://platform.openai.com/docs/guides/audio).\n   */\n  audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n\n  /**\n   * Static predicted output content, such as the content of a text file that is being regenerated.\n   * [Learn more](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs).\n   */\n  prediction?: OpenAIClient.ChatCompletionPredictionContent;\n\n  /**\n   * Options for reasoning models.\n   *\n   * Note that some options, like reasoning summaries, are only available when using the responses\n   * API. If these options are set, the responses API will be used to fulfill the request.\n   *\n   * These options will be ignored when not using a reasoning model.\n   */\n  reasoning?: OpenAIClient.Reasoning;\n\n  /**\n   * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\"\n   * Specifies the service tier for prioritization and latency optimization.\n   */\n  service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates. Replaces the `user` field.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  promptCacheKey?: string;\n\n  /**\n   * Used by OpenAI to set cache retention time\n   */\n  promptCacheRetention?: OpenAICacheRetentionParam;\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n}\n\nexport interface BaseChatOpenAIFields\n  extends Partial<OpenAIChatInput>,\n    BaseChatModelParams {\n  /**\n   * Optional configuration options for the OpenAI client.\n   */\n  configuration?: ClientOptions;\n}\n\n/** @internal */\nexport abstract class BaseChatOpenAI<\n    CallOptions extends BaseChatOpenAICallOptions\n  >\n  extends BaseChatModel<CallOptions, AIMessageChunk>\n  implements Partial<OpenAIChatInput>\n{\n  temperature?: number;\n\n  topP?: number;\n\n  frequencyPenalty?: number;\n\n  presencePenalty?: number;\n\n  n?: number;\n\n  logitBias?: Record<string, number>;\n\n  model = \"gpt-3.5-turbo\";\n\n  modelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n\n  stop?: string[];\n\n  stopSequences?: string[];\n\n  user?: string;\n\n  timeout?: number;\n\n  streaming = false;\n\n  streamUsage = true;\n\n  maxTokens?: number;\n\n  logprobs?: boolean;\n\n  topLogprobs?: number;\n\n  apiKey?: OpenAIApiKey;\n\n  organization?: string;\n\n  __includeRawResponse?: boolean;\n\n  /** @internal */\n  client: OpenAIClient;\n\n  /** @internal */\n  clientConfig: ClientOptions;\n\n  /**\n   * Whether the model supports the `strict` argument when passing in tools.\n   * If `undefined` the `strict` argument will not be passed to OpenAI.\n   */\n  supportsStrictToolCalling?: boolean;\n\n  audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n\n  modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n\n  reasoning?: OpenAIClient.Reasoning;\n\n  /**\n   * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable\n   * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your\n   * OpenAI organization or project. This must be configured directly with OpenAI.\n   *\n   * See:\n   * https://platform.openai.com/docs/guides/your-data\n   * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n   *\n   * @default false\n   */\n  zdrEnabled?: boolean | undefined;\n\n  /**\n   * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n   * Specifies the service tier for prioritization and latency optimization.\n   */\n  service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  promptCacheKey: string;\n\n  /**\n   * Used by OpenAI to set cache retention time\n   */\n  promptCacheRetention?: OpenAICacheRetentionParam;\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n\n  protected defaultOptions: CallOptions;\n\n  _llmType() {\n    return \"openai\";\n  }\n\n  static lc_name() {\n    return \"ChatOpenAI\";\n  }\n\n  get callKeys() {\n    return [\n      ...super.callKeys,\n      \"options\",\n      \"function_call\",\n      \"functions\",\n      \"tools\",\n      \"tool_choice\",\n      \"promptIndex\",\n      \"response_format\",\n      \"seed\",\n      \"reasoning\",\n      \"service_tier\",\n    ];\n  }\n\n  lc_serializable = true;\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\",\n    };\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      apiKey: \"openai_api_key\",\n      modelName: \"model\",\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [\n      \"configuration\",\n      \"logprobs\",\n      \"topLogprobs\",\n      \"prefixMessages\",\n      \"supportsStrictToolCalling\",\n      \"modalities\",\n      \"audio\",\n      \"temperature\",\n      \"maxTokens\",\n      \"topP\",\n      \"frequencyPenalty\",\n      \"presencePenalty\",\n      \"n\",\n      \"logitBias\",\n      \"user\",\n      \"streaming\",\n      \"streamUsage\",\n      \"model\",\n      \"modelName\",\n      \"modelKwargs\",\n      \"stop\",\n      \"stopSequences\",\n      \"timeout\",\n      \"apiKey\",\n      \"cache\",\n      \"maxConcurrency\",\n      \"maxRetries\",\n      \"verbose\",\n      \"callbacks\",\n      \"tags\",\n      \"metadata\",\n      \"disableStreaming\",\n      \"zdrEnabled\",\n      \"reasoning\",\n      \"promptCacheKey\",\n      \"promptCacheRetention\",\n      \"verbosity\",\n    ];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = this.invocationParams(options);\n    return {\n      ls_provider: \"openai\",\n      ls_model_name: this.model,\n      ls_model_type: \"chat\",\n      ls_temperature: params.temperature ?? undefined,\n      ls_max_tokens: params.max_tokens ?? undefined,\n      ls_stop: options.stop,\n    };\n  }\n\n  /** @ignore */\n  _identifyingParams(): Omit<\n    OpenAIClient.Chat.ChatCompletionCreateParams,\n    \"messages\"\n  > & {\n    model_name: string;\n  } & ClientOptions {\n    return {\n      model_name: this.model,\n      ...this.invocationParams(),\n      ...this.clientConfig,\n    };\n  }\n\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n\n  constructor(fields?: BaseChatOpenAIFields) {\n    super(fields ?? {});\n\n    const configApiKey =\n      typeof fields?.configuration?.apiKey === \"string\" ||\n      typeof fields?.configuration?.apiKey === \"function\"\n        ? fields?.configuration?.apiKey\n        : undefined;\n    this.apiKey =\n      fields?.apiKey ??\n      configApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.organization =\n      fields?.configuration?.organization ??\n      getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.timeout = fields?.timeout;\n\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.logprobs = fields?.logprobs;\n    this.topLogprobs = fields?.topLogprobs;\n    this.n = fields?.n ?? this.n;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stopSequences ?? fields?.stop;\n    this.stopSequences = this.stop;\n    this.user = fields?.user;\n    this.__includeRawResponse = fields?.__includeRawResponse;\n    this.audio = fields?.audio;\n    this.modalities = fields?.modalities;\n    this.reasoning = fields?.reasoning;\n    this.maxTokens = fields?.maxCompletionTokens ?? fields?.maxTokens;\n    this.promptCacheKey = fields?.promptCacheKey ?? this.promptCacheKey;\n    this.promptCacheRetention =\n      fields?.promptCacheRetention ?? this.promptCacheRetention;\n    this.verbosity = fields?.verbosity ?? this.verbosity;\n\n    this.disableStreaming = fields?.disableStreaming === true;\n    this.streaming = fields?.streaming === true;\n    if (this.disableStreaming) this.streaming = false;\n    // disable streaming in BaseChatModel if explicitly disabled\n    if (fields?.streaming === false) this.disableStreaming = true;\n\n    this.streamUsage = fields?.streamUsage ?? this.streamUsage;\n    if (this.disableStreaming) this.streamUsage = false;\n\n    this.clientConfig = {\n      apiKey: this.apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration,\n    };\n\n    // If `supportsStrictToolCalling` is explicitly set, use that value.\n    // Else leave undefined so it's not passed to OpenAI.\n    if (fields?.supportsStrictToolCalling !== undefined) {\n      this.supportsStrictToolCalling = fields.supportsStrictToolCalling;\n    }\n\n    if (fields?.service_tier !== undefined) {\n      this.service_tier = fields.service_tier;\n    }\n\n    this.zdrEnabled = fields?.zdrEnabled ?? false;\n  }\n\n  /**\n   * Returns backwards compatible reasoning parameters from constructor params and call options\n   * @internal\n   */\n  protected _getReasoningParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): OpenAIClient.Reasoning | undefined {\n    if (!isReasoningModel(this.model)) {\n      return;\n    }\n\n    // apply options in reverse order of importance -- newer options supersede older options\n    let reasoning: OpenAIClient.Reasoning | undefined;\n    if (this.reasoning !== undefined) {\n      reasoning = {\n        ...reasoning,\n        ...this.reasoning,\n      };\n    }\n    if (options?.reasoning !== undefined) {\n      reasoning = {\n        ...reasoning,\n        ...options.reasoning,\n      };\n    }\n\n    return reasoning;\n  }\n\n  /**\n   * Returns an openai compatible response format from a set of options\n   * @internal\n   */\n  protected _getResponseFormat(\n    resFormat?: CallOptions[\"response_format\"]\n  ): ResponseFormatConfiguration | undefined {\n    if (\n      resFormat &&\n      resFormat.type === \"json_schema\" &&\n      resFormat.json_schema.schema &&\n      isInteropZodSchema(resFormat.json_schema.schema)\n    ) {\n      return interopZodResponseFormat(\n        resFormat.json_schema.schema,\n        resFormat.json_schema.name,\n        {\n          description: resFormat.json_schema.description,\n        }\n      );\n    }\n    return resFormat as ResponseFormatConfiguration | undefined;\n  }\n\n  protected _combineCallOptions(\n    additionalOptions?: this[\"ParsedCallOptions\"]\n  ): this[\"ParsedCallOptions\"] {\n    return {\n      ...this.defaultOptions,\n      ...(additionalOptions ?? {}),\n    };\n  }\n\n  /** @internal */\n  _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(params.defaultHeaders);\n\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options,\n    } as OpenAICoreRequestOptions;\n    return requestOptions;\n  }\n\n  // TODO: move to completions class\n  protected _convertChatOpenAIToolToCompletionsTool(\n    tool: ChatOpenAIToolType,\n    fields?: { strict?: boolean }\n  ): OpenAIClient.ChatCompletionTool {\n    if (isCustomTool(tool)) {\n      return convertResponsesCustomTool(tool.metadata.customTool);\n    }\n    if (isOpenAIFunctionTool(tool)) {\n      if (fields?.strict !== undefined) {\n        return {\n          ...tool,\n          function: {\n            ...tool.function,\n            strict: fields.strict,\n          },\n        };\n      }\n\n      return tool;\n    }\n    return _convertToOpenAITool(tool, fields);\n  }\n\n  override bindTools(\n    tools: ChatOpenAIToolType[],\n    kwargs?: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    let strict: boolean | undefined;\n    if (kwargs?.strict !== undefined) {\n      strict = kwargs.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n    return this.withConfig({\n      tools: tools.map((tool) => {\n        // Built-in tools and custom tools pass through as-is\n        if (isBuiltInTool(tool) || isCustomTool(tool)) {\n          return tool;\n        }\n        // Tools with providerToolDefinition (e.g., localShell, shell, computerUse, applyPatch)\n        // should use their provider-specific definition\n        if (hasProviderToolDefinition(tool)) {\n          return tool.extras.providerToolDefinition;\n        }\n        // Regular tools get converted to OpenAI function format\n        return this._convertChatOpenAIToolToCompletionsTool(tool, { strict });\n      }),\n      ...kwargs,\n    } as Partial<CallOptions>);\n  }\n\n  override async stream(input: BaseLanguageModelInput, options?: CallOptions) {\n    return super.stream(\n      input,\n      this._combineCallOptions(options) as CallOptions\n    );\n  }\n\n  override async invoke(input: BaseLanguageModelInput, options?: CallOptions) {\n    return super.invoke(\n      input,\n      this._combineCallOptions(options) as CallOptions\n    );\n  }\n\n  /** @ignore */\n  _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput {\n    return llmOutputs.reduce<{\n      [key in keyof OpenAILLMOutput]: Required<OpenAILLMOutput[key]>;\n    }>(\n      (acc, llmOutput) => {\n        if (llmOutput && llmOutput.tokenUsage) {\n          acc.tokenUsage.completionTokens +=\n            llmOutput.tokenUsage.completionTokens ?? 0;\n          acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n          acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n        }\n        return acc;\n      },\n      {\n        tokenUsage: {\n          completionTokens: 0,\n          promptTokens: 0,\n          totalTokens: 0,\n        },\n      }\n    );\n  }\n\n  async getNumTokensFromMessages(messages: BaseMessage[]) {\n    let totalCount = 0;\n    let tokensPerMessage = 0;\n    let tokensPerName = 0;\n\n    // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    if (this.model === \"gpt-3.5-turbo-0301\") {\n      tokensPerMessage = 4;\n      tokensPerName = -1;\n    } else {\n      tokensPerMessage = 3;\n      tokensPerName = 1;\n    }\n\n    const countPerMessage = await Promise.all(\n      messages.map(async (message) => {\n        const textCount = await this.getNumTokens(message.content);\n        const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n        const nameCount =\n          message.name !== undefined\n            ? tokensPerName + (await this.getNumTokens(message.name))\n            : 0;\n        let count = textCount + tokensPerMessage + roleCount + nameCount;\n\n        // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate\n        const openAIMessage = message;\n        if (openAIMessage._getType() === \"function\") {\n          count -= 2;\n        }\n        if (openAIMessage.additional_kwargs?.function_call) {\n          count += 3;\n        }\n        if (openAIMessage?.additional_kwargs.function_call?.name) {\n          count += await this.getNumTokens(\n            openAIMessage.additional_kwargs.function_call?.name\n          );\n        }\n        if (openAIMessage.additional_kwargs.function_call?.arguments) {\n          try {\n            count += await this.getNumTokens(\n              // Remove newlines and spaces\n              JSON.stringify(\n                JSON.parse(\n                  openAIMessage.additional_kwargs.function_call?.arguments\n                )\n              )\n            );\n          } catch (error) {\n            console.error(\n              \"Error parsing function arguments\",\n              error,\n              JSON.stringify(openAIMessage.additional_kwargs.function_call)\n            );\n            count += await this.getNumTokens(\n              openAIMessage.additional_kwargs.function_call?.arguments\n            );\n          }\n        }\n\n        totalCount += count;\n        return count;\n      })\n    );\n\n    totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n\n    return { totalCount, countPerMessage };\n  }\n\n  /** @internal */\n  protected async _getNumTokensFromGenerations(generations: ChatGeneration[]) {\n    const generationUsages = await Promise.all(\n      generations.map(async (generation) => {\n        if (generation.message.additional_kwargs?.function_call) {\n          return (await this.getNumTokensFromMessages([generation.message]))\n            .countPerMessage[0];\n        } else {\n          return await this.getNumTokens(generation.message.content);\n        }\n      })\n    );\n\n    return generationUsages.reduce((a, b) => a + b, 0);\n  }\n\n  /** @internal */\n  protected async _getEstimatedTokenCountFromPrompt(\n    messages: BaseMessage[],\n    functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[],\n    function_call?:\n      | \"none\"\n      | \"auto\"\n      | OpenAIClient.Chat.ChatCompletionFunctionCallOption\n  ): Promise<number> {\n    // It appears that if functions are present, the first system message is padded with a trailing newline. This\n    // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.\n\n    let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;\n\n    // If there are functions, add the function definitions as they count towards token usage\n    if (functions && function_call !== \"auto\") {\n      const promptDefinitions = formatFunctionDefinitions(\n        functions as unknown as FunctionDef[]\n      );\n      tokens += await this.getNumTokens(promptDefinitions);\n      tokens += 9; // Add nine per completion\n    }\n\n    // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because\n    // functions typically add a system message, but reuse the first one if it's already there. This offsets\n    // the extra 9 tokens added by the function definitions.\n    if (functions && messages.find((m) => m._getType() === \"system\")) {\n      tokens -= 4;\n    }\n\n    // If function_call is 'none', add one token.\n    // If it's a FunctionCall object, add 4 + the number of tokens in the function name.\n    // If it's undefined or 'auto', don't add anything.\n    if (function_call === \"none\") {\n      tokens += 1;\n    } else if (typeof function_call === \"object\") {\n      tokens += (await this.getNumTokens(function_call.name)) + 4;\n    }\n\n    return tokens;\n  }\n\n  /**\n   * Moderate content using OpenAI's Moderation API.\n   *\n   * This method checks whether content violates OpenAI's content policy by\n   * analyzing text for categories such as hate, harassment, self-harm,\n   * sexual content, violence, and more.\n   *\n   * @param input - The text or array of texts to moderate\n   * @param params - Optional parameters for the moderation request\n   * @param params.model - The moderation model to use. Defaults to \"omni-moderation-latest\".\n   * @param params.options - Additional options to pass to the underlying request\n   * @returns A promise that resolves to the moderation response containing results for each input\n   *\n   * @example\n   * ```typescript\n   * const model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n   *\n   * // Moderate a single text\n   * const result = await model.moderateContent(\"This is a test message\");\n   * console.log(result.results[0].flagged); // false\n   * console.log(result.results[0].categories); // { hate: false, harassment: false, ... }\n   *\n   * // Moderate multiple texts\n   * const results = await model.moderateContent([\n   *   \"Hello, how are you?\",\n   *   \"This is inappropriate content\"\n   * ]);\n   * results.results.forEach((result, index) => {\n   *   console.log(`Text ${index + 1} flagged:`, result.flagged);\n   * });\n   *\n   * // Use a specific moderation model\n   * const stableResult = await model.moderateContent(\n   *   \"Test content\",\n   *   { model: \"omni-moderation-latest\" }\n   * );\n   * ```\n   */\n  async moderateContent(\n    input: string | string[],\n    params?: {\n      model?: OpenAI.ModerationModel;\n      options?: OpenAICoreRequestOptions;\n    }\n  ): Promise<OpenAIClient.ModerationCreateResponse> {\n    const clientOptions = this._getClientOptions(params?.options);\n    const moderationModel = params?.model ?? \"omni-moderation-latest\";\n    const moderationRequest: OpenAIClient.ModerationCreateParams = {\n      input,\n      model: moderationModel,\n    };\n\n    return this.caller.call(async () => {\n      try {\n        const response = await this.client.moderations.create(\n          moderationRequest,\n          clientOptions\n        );\n        return response;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /**\n   * Return profiling information for the model.\n   *\n   * Provides information about the model's capabilities and constraints,\n   * including token limits, multimodal support, and advanced features like\n   * tool calling and structured output.\n   *\n   * @returns {ModelProfile} An object describing the model's capabilities and constraints\n   *\n   * @example\n   * ```typescript\n   * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n   * const profile = model.profile;\n   * console.log(profile.maxInputTokens); // 128000\n   * console.log(profile.imageInputs); // true\n   * ```\n   */\n  get profile(): ModelProfile {\n    return PROFILES[this.model] ?? {};\n  }\n\n  /** @internal */\n  protected _getStructuredOutputMethod(\n    config: StructuredOutputMethodOptions<boolean>\n  ) {\n    const ensuredConfig = { ...config };\n    if (\n      !this.model.startsWith(\"gpt-3\") &&\n      !this.model.startsWith(\"gpt-4-\") &&\n      this.model !== \"gpt-4\"\n    ) {\n      if (ensuredConfig?.method === undefined) {\n        return \"jsonSchema\";\n      }\n    } else if (ensuredConfig.method === \"jsonSchema\") {\n      console.warn(\n        `[WARNING]: JSON Schema is not supported for model \"${this.model}\". Falling back to tool calling.`\n      );\n    }\n    return ensuredConfig.method;\n  }\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  /**\n   * Add structured output to the model.\n   *\n   * The OpenAI model family supports the following structured output methods:\n   * - `jsonSchema`: Use the `response_format` field in the response to return a JSON schema. Only supported with the `gpt-4o-mini`,\n   *   `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots and later.\n   * - `functionCalling`: Function calling is useful when you are building an application that bridges the models and functionality\n   *   of your application.\n   * - `jsonMode`: JSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model\n   *   output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify.\n   *   We recommend you use `functionCalling` or `jsonSchema` if it is supported for your use case.\n   *\n   * The default method is `functionCalling`.\n   *\n   * @see https://platform.openai.com/docs/guides/structured-outputs\n   * @param outputSchema - The schema to use for structured output.\n   * @param config - The structured output method options.\n   * @returns The model with structured output.\n   */\n  withStructuredOutput<\n    RunOutput extends Record<string, unknown> = Record<string, unknown>\n  >(\n    outputSchema: InteropZodType<RunOutput> | Record<string, unknown>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ) {\n    let llm: Runnable<BaseLanguageModelInput>;\n    let outputParser: Runnable<AIMessageChunk, RunOutput>;\n\n    const { schema, name, includeRaw } = {\n      ...config,\n      schema: outputSchema,\n    };\n\n    if (config?.strict !== undefined && config.method === \"jsonMode\") {\n      throw new Error(\n        \"Argument `strict` is only supported for `method` = 'function_calling'\"\n      );\n    }\n\n    const method = getStructuredOutputMethod(this.model, config?.method);\n\n    if (method === \"jsonMode\") {\n      if (isInteropZodSchema(schema)) {\n        outputParser = StructuredOutputParser.fromZodSchema(schema);\n      } else {\n        outputParser = new JsonOutputParser<RunOutput>();\n      }\n      const asJsonSchema = toJsonSchema(schema);\n      llm = this.withConfig({\n        outputVersion: \"v0\",\n        response_format: { type: \"json_object\" },\n        ls_structured_output_format: {\n          kwargs: { method: \"json_mode\" },\n          schema: { title: name ?? \"extract\", ...asJsonSchema },\n        },\n      } as Partial<CallOptions>);\n    } else if (method === \"jsonSchema\") {\n      const openaiJsonSchemaParams = {\n        name: name ?? \"extract\",\n        description: getSchemaDescription(schema),\n        schema,\n        strict: config?.strict,\n      };\n      const asJsonSchema = toJsonSchema(openaiJsonSchemaParams.schema);\n      llm = this.withConfig({\n        outputVersion: \"v0\",\n        response_format: {\n          type: \"json_schema\",\n          json_schema: openaiJsonSchemaParams,\n        },\n        ls_structured_output_format: {\n          kwargs: { method: \"json_schema\" },\n          schema: {\n            title: openaiJsonSchemaParams.name,\n            description: openaiJsonSchemaParams.description,\n            ...asJsonSchema,\n          },\n        },\n      } as Partial<CallOptions>);\n      if (isInteropZodSchema(schema)) {\n        const altParser = StructuredOutputParser.fromZodSchema(schema);\n        outputParser = RunnableLambda.from<AIMessageChunk, RunOutput>(\n          (aiMessage: AIMessageChunk) => {\n            if (\"parsed\" in aiMessage.additional_kwargs) {\n              return aiMessage.additional_kwargs.parsed as RunOutput;\n            }\n            return altParser;\n          }\n        );\n      } else {\n        outputParser = new JsonOutputParser<RunOutput>();\n      }\n    } else {\n      let functionName = name ?? \"extract\";\n      // Is function calling\n      if (isInteropZodSchema(schema)) {\n        const asJsonSchema = toJsonSchema(schema);\n        llm = this.withConfig({\n          outputVersion: \"v0\",\n          tools: [\n            {\n              type: \"function\" as const,\n              function: {\n                name: functionName,\n                description: asJsonSchema.description,\n                parameters: asJsonSchema,\n              },\n            },\n          ],\n          tool_choice: {\n            type: \"function\" as const,\n            function: {\n              name: functionName,\n            },\n          },\n          ls_structured_output_format: {\n            kwargs: { method: \"function_calling\" },\n            schema: { title: functionName, ...asJsonSchema },\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser({\n          returnSingle: true,\n          keyName: functionName,\n          zodSchema: schema,\n        });\n      } else {\n        let openAIFunctionDefinition: FunctionDefinition;\n        if (\n          typeof schema.name === \"string\" &&\n          typeof schema.parameters === \"object\" &&\n          schema.parameters != null\n        ) {\n          openAIFunctionDefinition = schema as unknown as FunctionDefinition;\n          functionName = schema.name;\n        } else {\n          functionName = (schema.title as string) ?? functionName;\n          openAIFunctionDefinition = {\n            name: functionName,\n            description: (schema.description as string) ?? \"\",\n            parameters: schema,\n          };\n        }\n        const asJsonSchema = toJsonSchema(schema);\n        llm = this.withConfig({\n          outputVersion: \"v0\",\n          tools: [\n            {\n              type: \"function\" as const,\n              function: openAIFunctionDefinition,\n            },\n          ],\n          tool_choice: {\n            type: \"function\" as const,\n            function: {\n              name: functionName,\n            },\n          },\n          ls_structured_output_format: {\n            kwargs: { method: \"function_calling\" },\n            schema: { title: functionName, ...asJsonSchema },\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser<RunOutput>({\n          returnSingle: true,\n          keyName: functionName,\n        });\n      }\n    }\n\n    if (!includeRaw) {\n      return llm.pipe(outputParser) as Runnable<\n        BaseLanguageModelInput,\n        RunOutput\n      >;\n    }\n\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input: any, config) => outputParser.invoke(input.raw, config),\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null,\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone],\n    });\n    return RunnableSequence.from<\n      BaseLanguageModelInput,\n      { raw: BaseMessage; parsed: RunOutput }\n    >([{ raw: llm }, parsedWithFallback]);\n  }\n}\n"],"names":["options: this[\"ParsedCallOptions\"]","fields?: BaseChatOpenAIFields","options?: this[\"ParsedCallOptions\"]","reasoning: OpenAIClient.Reasoning | undefined","resFormat?: CallOptions[\"response_format\"]","additionalOptions?: this[\"ParsedCallOptions\"]","options: OpenAICoreRequestOptions | undefined","openAIEndpointConfig: OpenAIEndpointConfig","OpenAIClient","tool: ChatOpenAIToolType","fields?: { strict?: boolean }","isOpenAIFunctionTool","tools: ChatOpenAIToolType[]","kwargs?: Partial<CallOptions>","strict: boolean | undefined","input: BaseLanguageModelInput","options?: CallOptions","messages: BaseMessage[]","generations: ChatGeneration[]","functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[]","function_call?:\n      | \"none\"\n      | \"auto\"\n      | OpenAIClient.Chat.ChatCompletionFunctionCallOption","input: string | string[]","params?: {\n      model?: OpenAI.ModerationModel;\n      options?: OpenAICoreRequestOptions;\n    }","moderationRequest: OpenAIClient.ModerationCreateParams","PROFILES","config: StructuredOutputMethodOptions<boolean>","outputSchema: InteropZodType<RunOutput> | Record<string, unknown>","config?: StructuredOutputMethodOptions<boolean>","llm: Runnable<BaseLanguageModelInput>","outputParser: Runnable<AIMessageChunk, RunOutput>","aiMessage: AIMessageChunk","openAIFunctionDefinition: FunctionDefinition","input: any","config"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;iBA4NA,IAAsB,iBAAtB,cAGU,gMAAA,CAEV;IACE,YAAA;IAEA,KAAA;IAEA,iBAAA;IAEA,gBAAA;IAEA,EAAA;IAEA,UAAA;IAEA,QAAQ,gBAAA;IAER,YAAA;IAEA,KAAA;IAEA,cAAA;IAEA,KAAA;IAEA,QAAA;IAEA,YAAY,MAAA;IAEZ,cAAc,KAAA;IAEd,UAAA;IAEA,SAAA;IAEA,YAAA;IAEA,OAAA;IAEA,aAAA;IAEA,qBAAA;qBAGA,OAAA;qBAGA,aAAA;;;;IAMA,0BAAA;IAEA,MAAA;IAEA,WAAA;IAEA,UAAA;;;;;;;;;;;IAaA,WAAA;;;;IAMA,aAAA;;;;;IAOA,eAAA;;;IAKA,qBAAA;;;IAKA,UAAA;IAEU,eAAA;IAEV,WAAW;QACT,OAAO;IACR;IAED,OAAO,UAAU;QACf,OAAO;IACR;IAED,IAAI,WAAW;QACb,OAAO;eACF,KAAA,CAAM;YACT;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;SACD;IACF;IAED,kBAAkB,KAAA;IAElB,IAAI,aAAoD;QACtD,OAAO;YACL,QAAQ;YACR,cAAc;QACf;IACF;IAED,IAAI,aAAqC;QACvC,OAAO;YACL,QAAQ;YACR,WAAW;QACZ;IACF;IAED,IAAI,uBAAiC;QACnC,OAAO;YACL;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;SACD;IACF;IAED,YAAYA,OAAAA,EAAqD;QAC/D,MAAM,SAAS,IAAA,CAAK,gBAAA,CAAiB,QAAQ;QAC7C,OAAO;YACL,aAAa;YACb,eAAe,IAAA,CAAK,KAAA;YACpB,eAAe;YACf,gBAAgB,OAAO,WAAA,IAAe,KAAA;YACtC,eAAe,OAAO,UAAA,IAAc,KAAA;YACpC,SAAS,QAAQ,IAAA;QAClB;IACF;mBAGD,qBAKkB;QAChB,OAAO;YACL,YAAY,IAAA,CAAK,KAAA;YACjB,GAAG,IAAA,CAAK,gBAAA,EAAkB;YAC1B,GAAG,IAAA,CAAK,YAAA;QACT;IACF;;;IAKD,oBAAoB;QAClB,OAAO,IAAA,CAAK,kBAAA,EAAoB;IACjC;IAED,YAAYC,MAAAA,CAA+B;QACzC,KAAA,CAAM,UAAU,CAAE,EAAC;QAEnB,MAAM,eACJ,OAAO,QAAQ,eAAe,WAAW,YACzC,OAAO,QAAQ,eAAe,WAAW,aACrC,QAAQ,eAAe,SACvB,KAAA;QACN,IAAA,CAAK,MAAA,GACH,QAAQ,UACR,oBACA,uLAAA,EAAuB,iBAAiB;QAC1C,IAAA,CAAK,YAAA,GACH,QAAQ,eAAe,oBACvB,uLAAA,EAAuB,sBAAsB;QAE/C,IAAA,CAAK,KAAA,GAAQ,QAAQ,SAAS,QAAQ,aAAa,IAAA,CAAK,KAAA;QACxD,IAAA,CAAK,WAAA,GAAc,QAAQ,eAAe,CAAE;QAC5C,IAAA,CAAK,OAAA,GAAU,QAAQ;QAEvB,IAAA,CAAK,WAAA,GAAc,QAAQ,eAAe,IAAA,CAAK,WAAA;QAC/C,IAAA,CAAK,IAAA,GAAO,QAAQ,QAAQ,IAAA,CAAK,IAAA;QACjC,IAAA,CAAK,gBAAA,GAAmB,QAAQ,oBAAoB,IAAA,CAAK,gBAAA;QACzD,IAAA,CAAK,eAAA,GAAkB,QAAQ,mBAAmB,IAAA,CAAK,eAAA;QACvD,IAAA,CAAK,QAAA,GAAW,QAAQ;QACxB,IAAA,CAAK,WAAA,GAAc,QAAQ;QAC3B,IAAA,CAAK,CAAA,GAAI,QAAQ,KAAK,IAAA,CAAK,CAAA;QAC3B,IAAA,CAAK,SAAA,GAAY,QAAQ;QACzB,IAAA,CAAK,IAAA,GAAO,QAAQ,iBAAiB,QAAQ;QAC7C,IAAA,CAAK,aAAA,GAAgB,IAAA,CAAK,IAAA;QAC1B,IAAA,CAAK,IAAA,GAAO,QAAQ;QACpB,IAAA,CAAK,oBAAA,GAAuB,QAAQ;QACpC,IAAA,CAAK,KAAA,GAAQ,QAAQ;QACrB,IAAA,CAAK,UAAA,GAAa,QAAQ;QAC1B,IAAA,CAAK,SAAA,GAAY,QAAQ;QACzB,IAAA,CAAK,SAAA,GAAY,QAAQ,uBAAuB,QAAQ;QACxD,IAAA,CAAK,cAAA,GAAiB,QAAQ,kBAAkB,IAAA,CAAK,cAAA;QACrD,IAAA,CAAK,oBAAA,GACH,QAAQ,wBAAwB,IAAA,CAAK,oBAAA;QACvC,IAAA,CAAK,SAAA,GAAY,QAAQ,aAAa,IAAA,CAAK,SAAA;QAE3C,IAAA,CAAK,gBAAA,GAAmB,QAAQ,qBAAqB;QACrD,IAAA,CAAK,SAAA,GAAY,QAAQ,cAAc;QACvC,IAAI,IAAA,CAAK,gBAAA,EAAkB,IAAA,CAAK,SAAA,GAAY;QAE5C,IAAI,QAAQ,cAAc,OAAO,IAAA,CAAK,gBAAA,GAAmB;QAEzD,IAAA,CAAK,WAAA,GAAc,QAAQ,eAAe,IAAA,CAAK,WAAA;QAC/C,IAAI,IAAA,CAAK,gBAAA,EAAkB,IAAA,CAAK,WAAA,GAAc;QAE9C,IAAA,CAAK,YAAA,GAAe;YAClB,QAAQ,IAAA,CAAK,MAAA;YACb,cAAc,IAAA,CAAK,YAAA;YACnB,yBAAyB;YACzB,GAAG,QAAQ,aAAA;QACZ;QAID,IAAI,QAAQ,8BAA8B,KAAA,GACxC,IAAA,CAAK,yBAAA,GAA4B,OAAO,yBAAA;QAG1C,IAAI,QAAQ,iBAAiB,KAAA,GAC3B,IAAA,CAAK,YAAA,GAAe,OAAO,YAAA;QAG7B,IAAA,CAAK,UAAA,GAAa,QAAQ,cAAc;IACzC;;;;IAMS,oBACRC,OAAAA,EACoC;QACpC,IAAI,KAAC,oLAAA,EAAiB,IAAA,CAAK,KAAA,CAAM,CAC/B,CAAA;QAIF,IAAIC;QACJ,IAAI,IAAA,CAAK,SAAA,KAAc,KAAA,GACrB,YAAY;YACV,GAAG,SAAA;YACH,GAAG,IAAA,CAAK,SAAA;QACT;QAEH,IAAI,SAAS,cAAc,KAAA,GACzB,YAAY;YACV,GAAG,SAAA;YACH,GAAG,QAAQ,SAAA;QACZ;QAGH,OAAO;IACR;;;;IAMS,mBACRC,SAAAA,EACyC;QACzC,IACE,aACA,UAAU,IAAA,KAAS,iBACnB,UAAU,WAAA,CAAY,MAAA,QACtB,4LAAA,EAAmB,UAAU,WAAA,CAAY,MAAA,CAAO,CAEhD,CAAA,WAAO,8LAAA,EACL,UAAU,WAAA,CAAY,MAAA,EACtB,UAAU,WAAA,CAAY,IAAA,EACtB;YACE,aAAa,UAAU,WAAA,CAAY,WAAA;QACpC,EACF;QAEH,OAAO;IACR;IAES,oBACRC,iBAAAA,EAC2B;QAC3B,OAAO;YACL,GAAG,IAAA,CAAK,cAAA;YACR,GAAI,qBAAqB,CAAE,CAAA;QAC5B;IACF;qBAGD,kBACEC,OAAAA,EAC0B;QAC1B,IAAI,CAAC,IAAA,CAAK,MAAA,EAAQ;YAChB,MAAMC,uBAA6C;gBACjD,SAAS,IAAA,CAAK,YAAA,CAAa,OAAA;YAC5B;YAED,MAAM,eAAW,gLAAA,EAAY,qBAAqB;YAClD,MAAM,SAAS;gBACb,GAAG,IAAA,CAAK,YAAA;gBACR,SAAS;gBACT,SAAS,IAAA,CAAK,OAAA;gBACd,YAAY;YACb;YACD,IAAI,CAAC,OAAO,OAAA,EACV,OAAO,OAAO,OAAA;YAGhB,OAAO,cAAA,OAAiB,4LAAA,EAAwB,OAAO,cAAA,CAAe;YAEtE,IAAA,CAAK,MAAA,GAAS,IAAIC,6IAAAA,CAAa;QAChC;QACD,MAAM,iBAAiB;YACrB,GAAG,IAAA,CAAK,YAAA;YACR,GAAG,OAAA;QACJ;QACD,OAAO;IACR;IAGS,wCACRC,IAAAA,EACAC,MAAAA,EACiC;QACjC,QAAI,iLAAA,EAAa,KAAK,CACpB,CAAA,WAAO,+LAAA,EAA2B,KAAK,QAAA,CAAS,UAAA,CAAW;QAE7D,QAAIC,wLAAAA,EAAqB,KAAK,EAAE;YAC9B,IAAI,QAAQ,WAAW,KAAA,EACrB,CAAA,OAAO;gBACL,GAAG,IAAA;gBACH,UAAU;oBACR,GAAG,KAAK,QAAA;oBACR,QAAQ,OAAO,MAAA;gBAChB;YACF;YAGH,OAAO;QACR;QACD,WAAO,yLAAA,EAAqB,MAAM,OAAO;IAC1C;IAEQ,UACPC,KAAAA,EACAC,MAAAA,EAC+D;QAC/D,IAAIC;QACJ,IAAI,QAAQ,WAAW,KAAA,GACrB,SAAS,OAAO,MAAA;iBACP,IAAA,CAAK,yBAAA,KAA8B,KAAA,GAC5C,SAAS,IAAA,CAAK,yBAAA;QAEhB,OAAO,IAAA,CAAK,UAAA,CAAW;YACrB,OAAO,MAAM,GAAA,CAAI,CAAC,SAAS;gBAEzB,QAAI,kLAAA,EAAc,KAAK,QAAI,iLAAA,EAAa,KAAK,CAC3C,CAAA,OAAO;gBAIT,QAAI,8LAAA,EAA0B,KAAK,CACjC,CAAA,OAAO,KAAK,MAAA,CAAO,sBAAA;gBAGrB,OAAO,IAAA,CAAK,uCAAA,CAAwC,MAAM;oBAAE;gBAAQ,EAAC;YACtE,EAAC;YACF,GAAG,MAAA;QACJ,EAAyB;IAC3B;IAED,MAAe,OAAOC,KAAAA,EAA+BC,OAAAA,EAAuB;QAC1E,OAAO,KAAA,CAAM,OACX,OACA,IAAA,CAAK,mBAAA,CAAoB,QAAQ,CAClC;IACF;IAED,MAAe,OAAOD,KAAAA,EAA+BC,OAAAA,EAAuB;QAC1E,OAAO,KAAA,CAAM,OACX,OACA,IAAA,CAAK,mBAAA,CAAoB,QAAQ,CAClC;IACF;mBAGD,kBAAkB,GAAG,UAAA,EAAgD;QACnE,OAAO,WAAW,MAAA,CAGhB,CAAC,KAAK,cAAc;YAClB,IAAI,aAAa,UAAU,UAAA,EAAY;gBACrC,IAAI,UAAA,CAAW,gBAAA,IACb,UAAU,UAAA,CAAW,gBAAA,IAAoB;gBAC3C,IAAI,UAAA,CAAW,YAAA,IAAgB,UAAU,UAAA,CAAW,YAAA,IAAgB;gBACpE,IAAI,UAAA,CAAW,WAAA,IAAe,UAAU,UAAA,CAAW,WAAA,IAAe;YACnE;YACD,OAAO;QACR,GACD;YACE,YAAY;gBACV,kBAAkB;gBAClB,cAAc;gBACd,aAAa;YACd;QACF,EACF;IACF;IAED,MAAM,yBAAyBC,QAAAA,EAAyB;QACtD,IAAI,aAAa;QACjB,IAAI,mBAAmB;QACvB,IAAI,gBAAgB;QAGpB,IAAI,IAAA,CAAK,KAAA,KAAU,sBAAsB;YACvC,mBAAmB;YACnB,gBAAgB,CAAA;QACjB,OAAM;YACL,mBAAmB;YACnB,gBAAgB;QACjB;QAED,MAAM,kBAAkB,MAAM,QAAQ,GAAA,CACpC,SAAS,GAAA,CAAI,OAAO,YAAY;YAC9B,MAAM,YAAY,MAAM,IAAA,CAAK,YAAA,CAAa,QAAQ,OAAA,CAAQ;YAC1D,MAAM,YAAY,MAAM,IAAA,CAAK,YAAA,KAAa,uLAAA,EAAoB,QAAQ,CAAC;YACvE,MAAM,YACJ,QAAQ,IAAA,KAAS,KAAA,IACb,gBAAiB,MAAM,IAAA,CAAK,YAAA,CAAa,QAAQ,IAAA,CAAK,GACtD;YACN,IAAI,QAAQ,YAAY,mBAAmB,YAAY;YAGvD,MAAM,gBAAgB;YACtB,IAAI,cAAc,QAAA,EAAU,KAAK,YAC/B,SAAS;YAEX,IAAI,cAAc,iBAAA,EAAmB,eACnC,SAAS;YAEX,IAAI,eAAe,kBAAkB,eAAe,MAClD,SAAS,MAAM,IAAA,CAAK,YAAA,CAClB,cAAc,iBAAA,CAAkB,aAAA,EAAe,KAChD;YAEH,IAAI,cAAc,iBAAA,CAAkB,aAAA,EAAe,UACjD,CAAA,IAAI;gBACF,SAAS,MAAM,IAAA,CAAK,YAAA,CAElB,KAAK,SAAA,CACH,KAAK,KAAA,CACH,cAAc,iBAAA,CAAkB,aAAA,EAAe,UAChD,CACF,CACF;YACF,EAAA,OAAQ,OAAO;gBACd,QAAQ,KAAA,CACN,oCACA,OACA,KAAK,SAAA,CAAU,cAAc,iBAAA,CAAkB,aAAA,CAAc,CAC9D;gBACD,SAAS,MAAM,IAAA,CAAK,YAAA,CAClB,cAAc,iBAAA,CAAkB,aAAA,EAAe,UAChD;YACF;YAGH,cAAc;YACd,OAAO;QACR,EAAC,CACH;QAED,cAAc;QAEd,OAAO;YAAE;YAAY;QAAiB;IACvC;qBAGD,MAAgB,6BAA6BC,WAAAA,EAA+B;QAC1E,MAAM,mBAAmB,MAAM,QAAQ,GAAA,CACrC,YAAY,GAAA,CAAI,OAAO,eAAe;YACpC,IAAI,WAAW,OAAA,CAAQ,iBAAA,EAAmB,cACxC,CAAA,OAAA,CAAQ,MAAM,IAAA,CAAK,wBAAA,CAAyB;gBAAC,WAAW,OAAQ;aAAA,CAAC,EAC9D,eAAA,CAAgB,EAAA;iBAEnB,OAAO,MAAM,IAAA,CAAK,YAAA,CAAa,WAAW,OAAA,CAAQ,OAAA,CAAQ;QAE7D,EAAC,CACH;QAED,OAAO,iBAAiB,MAAA,CAAO,CAAC,GAAG,IAAM,IAAI,GAAG,EAAE;IACnD;qBAGD,MAAgB,kCACdD,QAAAA,EACAE,SAAAA,EACAC,aAAAA,EAIiB;QAIjB,IAAI,SAAA,CAAU,MAAM,IAAA,CAAK,wBAAA,CAAyB,SAAS,EAAE,UAAA;QAG7D,IAAI,aAAa,kBAAkB,QAAQ;YACzC,MAAM,wBAAoB,8LAAA,EACxB,UACD;YACD,UAAU,MAAM,IAAA,CAAK,YAAA,CAAa,kBAAkB;YACpD,UAAU;QACX;QAKD,IAAI,aAAa,SAAS,IAAA,CAAK,CAAC,IAAM,EAAE,QAAA,EAAU,KAAK,SAAS,EAC9D,UAAU;QAMZ,IAAI,kBAAkB,QACpB,UAAU;iBACD,OAAO,kBAAkB,UAClC,UAAW,MAAM,IAAA,CAAK,YAAA,CAAa,cAAc,IAAA,CAAK,GAAI;QAG5D,OAAO;IACR;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;IAwCD,MAAM,gBACJC,KAAAA,EACAC,MAAAA,EAIgD;QAChD,MAAM,gBAAgB,IAAA,CAAK,iBAAA,CAAkB,QAAQ,QAAQ;QAC7D,MAAM,kBAAkB,QAAQ,SAAS;QACzC,MAAMC,oBAAyD;YAC7D;YACA,OAAO;QACR;QAED,OAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,YAAY;YAClC,IAAI;gBACF,MAAM,WAAW,MAAM,IAAA,CAAK,MAAA,CAAO,WAAA,CAAY,MAAA,CAC7C,mBACA,cACD;gBACD,OAAO;YACR,EAAA,OAAQ,GAAG;gBACV,MAAM,YAAQ,2LAAA,EAAsB,EAAE;gBACtC,MAAM;YACP;QACF,EAAC;IACH;;;;;;;;;;;;;;;;;IAmBD,IAAI,UAAwB;QAC1B,OAAOC,qLAAAA,CAAS,IAAA,CAAK,KAAA,CAAA,IAAU,CAAE;IAClC;qBAGS,2BACRC,MAAAA,EACA;QACA,MAAM,gBAAgB;YAAE,GAAG,MAAA;QAAQ;QACnC,IACE,CAAC,IAAA,CAAK,KAAA,CAAM,UAAA,CAAW,QAAQ,IAC/B,CAAC,IAAA,CAAK,KAAA,CAAM,UAAA,CAAW,SAAS,IAChC,IAAA,CAAK,KAAA,KAAU,SAEf;gBAAI,eAAe,WAAW,KAAA,EAC5B,CAAA,OAAO;QACR,OAAA,IACQ,cAAc,MAAA,KAAW,cAClC,QAAQ,IAAA,CACN,CAAC,mDAAmD,EAAE,IAAA,CAAK,KAAA,CAAM,gCAAgC,CAAC,CACnG;QAEH,OAAO,cAAc,MAAA;IACtB;;;;;;;;;;;;;;;;;;;IAwDD,qBAGEC,YAAAA,EACAC,MAAAA,EACA;QACA,IAAIC;QACJ,IAAIC;QAEJ,MAAM,EAAE,MAAA,EAAQ,IAAA,EAAM,UAAA,EAAY,GAAG;YACnC,GAAG,MAAA;YACH,QAAQ;QACT;QAED,IAAI,QAAQ,WAAW,KAAA,KAAa,OAAO,MAAA,KAAW,WACpD,CAAA,MAAM,IAAI,MACR;QAIJ,MAAM,aAAS,+LAAA,EAA0B,IAAA,CAAK,KAAA,EAAO,QAAQ,OAAO;QAEpE,IAAI,WAAW,YAAY;YACzB,QAAI,4LAAA,EAAmB,OAAO,EAC5B,eAAe,uMAAA,CAAuB,aAAA,CAAc,OAAO;iBAE3D,eAAe,IAAI,2LAAA;YAErB,MAAM,mBAAe,qMAAA,EAAa,OAAO;YACzC,MAAM,IAAA,CAAK,UAAA,CAAW;gBACpB,eAAe;gBACf,iBAAiB;oBAAE,MAAM;gBAAe;gBACxC,6BAA6B;oBAC3B,QAAQ;wBAAE,QAAQ;oBAAa;oBAC/B,QAAQ;wBAAE,OAAO,QAAQ;wBAAW,GAAG,YAAA;oBAAc;gBACtD;YACF,EAAyB;QAC3B,OAAA,IAAU,WAAW,cAAc;YAClC,MAAM,yBAAyB;gBAC7B,MAAM,QAAQ;gBACd,iBAAa,8LAAA,EAAqB,OAAO;gBACzC;gBACA,QAAQ,QAAQ;YACjB;YACD,MAAM,mBAAe,qMAAA,EAAa,uBAAuB,MAAA,CAAO;YAChE,MAAM,IAAA,CAAK,UAAA,CAAW;gBACpB,eAAe;gBACf,iBAAiB;oBACf,MAAM;oBACN,aAAa;gBACd;gBACD,6BAA6B;oBAC3B,QAAQ;wBAAE,QAAQ;oBAAe;oBACjC,QAAQ;wBACN,OAAO,uBAAuB,IAAA;wBAC9B,aAAa,uBAAuB,WAAA;wBACpC,GAAG,YAAA;oBACJ;gBACF;YACF,EAAyB;YAC1B,QAAI,4LAAA,EAAmB,OAAO,EAAE;gBAC9B,MAAM,YAAY,uMAAA,CAAuB,aAAA,CAAc,OAAO;gBAC9D,eAAe,oLAAA,CAAe,IAAA,CAC5B,CAACC,cAA8B;oBAC7B,IAAI,YAAY,UAAU,iBAAA,CACxB,CAAA,OAAO,UAAU,iBAAA,CAAkB,MAAA;oBAErC,OAAO;gBACR,EACF;YACF,OACC,eAAe,IAAI,2LAAA;QAEtB,OAAM;YACL,IAAI,eAAe,QAAQ;YAE3B,QAAI,4LAAA,EAAmB,OAAO,EAAE;gBAC9B,MAAM,mBAAe,qMAAA,EAAa,OAAO;gBACzC,MAAM,IAAA,CAAK,UAAA,CAAW;oBACpB,eAAe;oBACf,OAAO;wBACL;4BACE,MAAM;4BACN,UAAU;gCACR,MAAM;gCACN,aAAa,aAAa,WAAA;gCAC1B,YAAY;4BACb;wBACF,CACF;qBAAA;oBACD,aAAa;wBACX,MAAM;wBACN,UAAU;4BACR,MAAM;wBACP;oBACF;oBACD,6BAA6B;wBAC3B,QAAQ;4BAAE,QAAQ;wBAAoB;wBACtC,QAAQ;4BAAE,OAAO;4BAAc,GAAG,YAAA;wBAAc;oBACjD;oBAED,GAAI,QAAQ,WAAW,KAAA,IAAY;wBAAE,QAAQ,OAAO,MAAA;oBAAQ,IAAG,CAAE,CAAA;gBAClE,EAAyB;gBAC1B,eAAe,IAAI,wOAAA,CAAyB;oBAC1C,cAAc;oBACd,SAAS;oBACT,WAAW;gBACZ;YACF,OAAM;gBACL,IAAIC;gBACJ,IACE,OAAO,OAAO,IAAA,KAAS,YACvB,OAAO,OAAO,UAAA,KAAe,YAC7B,OAAO,UAAA,IAAc,MACrB;oBACA,2BAA2B;oBAC3B,eAAe,OAAO,IAAA;gBACvB,OAAM;oBACL,eAAgB,OAAO,KAAA,IAAoB;oBAC3C,2BAA2B;wBACzB,MAAM;wBACN,aAAc,OAAO,WAAA,IAA0B;wBAC/C,YAAY;oBACb;gBACF;gBACD,MAAM,mBAAe,qMAAA,EAAa,OAAO;gBACzC,MAAM,IAAA,CAAK,UAAA,CAAW;oBACpB,eAAe;oBACf,OAAO;wBACL;4BACE,MAAM;4BACN,UAAU;wBACX,CACF;qBAAA;oBACD,aAAa;wBACX,MAAM;wBACN,UAAU;4BACR,MAAM;wBACP;oBACF;oBACD,6BAA6B;wBAC3B,QAAQ;4BAAE,QAAQ;wBAAoB;wBACtC,QAAQ;4BAAE,OAAO;4BAAc,GAAG,YAAA;wBAAc;oBACjD;oBAED,GAAI,QAAQ,WAAW,KAAA,IAAY;wBAAE,QAAQ,OAAO,MAAA;oBAAQ,IAAG,CAAE,CAAA;gBAClE,EAAyB;gBAC1B,eAAe,IAAI,wOAAA,CAAoC;oBACrD,cAAc;oBACd,SAAS;gBACV;YACF;QACF;QAED,IAAI,CAAC,WACH,CAAA,OAAO,IAAI,IAAA,CAAK,aAAa;QAM/B,MAAM,eAAe,gMAAA,CAAoB,MAAA,CAAO;YAE9C,QAAQ,CAACC,OAAYC,WAAW,aAAa,MAAA,CAAO,MAAM,GAAA,EAAKA,SAAO;QACvE,EAAC;QACF,MAAM,aAAa,gMAAA,CAAoB,MAAA,CAAO;YAC5C,QAAQ,IAAM;QACf,EAAC;QACF,MAAM,qBAAqB,aAAa,aAAA,CAAc;YACpD,WAAW;gBAAC,UAAW;aAAA;QACxB,EAAC;QACF,OAAO,sLAAA,CAAiB,IAAA,CAGtB;YAAC;gBAAE,KAAK;YAAK;YAAE,kBAAmB;SAAA,CAAC;IACtC;AACF"}},
    {"offset": {"line": 1826, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/converters/completions.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/converters/completions.ts"],"sourcesContent":["import {\n  AIMessage,\n  AIMessageChunk,\n  BaseMessage,\n  BaseMessageChunk,\n  ChatMessage,\n  ChatMessageChunk,\n  FunctionMessageChunk,\n  HumanMessageChunk,\n  OpenAIToolCall,\n  SystemMessageChunk,\n  ToolCallChunk,\n  ToolMessage,\n  ToolMessageChunk,\n  parseBase64DataUrl,\n  parseMimeType,\n  StandardContentBlockConverter,\n  isDataContentBlock,\n  ContentBlock,\n  iife,\n  convertToProviderContentBlock,\n} from \"@langchain/core/messages\";\nimport {\n  convertLangChainToolCallToOpenAI,\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport { Converter } from \"@langchain/core/utils/format\";\nimport type {\n  ChatCompletionContentPartText,\n  ChatCompletionContentPartImage,\n  ChatCompletionContentPartInputAudio,\n  ChatCompletionContentPart,\n} from \"openai/resources/chat/completions\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { handleMultiModalOutput } from \"../utils/output.js\";\nimport {\n  getRequiredFilenameFromMetadata,\n  isReasoningModel,\n  messageToOpenAIRole,\n} from \"../utils/misc.js\";\n\n/**\n * @deprecated This converter is an internal detail of the OpenAI provider. Do not use it directly. This will be revisited in a future release.\n */\nexport const completionsApiContentBlockConverter: StandardContentBlockConverter<{\n  text: ChatCompletionContentPartText;\n  image: ChatCompletionContentPartImage;\n  audio: ChatCompletionContentPartInputAudio;\n  file: ChatCompletionContentPart.File;\n}> = {\n  providerName: \"ChatOpenAI\",\n\n  fromStandardTextBlock(block): ChatCompletionContentPartText {\n    return { type: \"text\", text: block.text };\n  },\n\n  fromStandardImageBlock(block): ChatCompletionContentPartImage {\n    if (block.source_type === \"url\") {\n      return {\n        type: \"image_url\",\n        image_url: {\n          url: block.url,\n          ...(block.metadata?.detail\n            ? { detail: block.metadata.detail as \"auto\" | \"low\" | \"high\" }\n            : {}),\n        },\n      };\n    }\n\n    if (block.source_type === \"base64\") {\n      const url = `data:${block.mime_type ?? \"\"};base64,${block.data}`;\n      return {\n        type: \"image_url\",\n        image_url: {\n          url,\n          ...(block.metadata?.detail\n            ? { detail: block.metadata.detail as \"auto\" | \"low\" | \"high\" }\n            : {}),\n        },\n      };\n    }\n\n    throw new Error(\n      `Image content blocks with source_type ${block.source_type} are not supported for ChatOpenAI`\n    );\n  },\n\n  fromStandardAudioBlock(block): ChatCompletionContentPartInputAudio {\n    if (block.source_type === \"url\") {\n      const data = parseBase64DataUrl({ dataUrl: block.url });\n      if (!data) {\n        throw new Error(\n          `URL audio blocks with source_type ${block.source_type} must be formatted as a data URL for ChatOpenAI`\n        );\n      }\n\n      const rawMimeType = data.mime_type || block.mime_type || \"\";\n      let mimeType: { type: string; subtype: string };\n\n      try {\n        mimeType = parseMimeType(rawMimeType);\n      } catch {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      if (\n        mimeType.type !== \"audio\" ||\n        (mimeType.subtype !== \"wav\" && mimeType.subtype !== \"mp3\")\n      ) {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      return {\n        type: \"input_audio\",\n        input_audio: {\n          format: mimeType.subtype,\n          data: data.data,\n        },\n      };\n    }\n\n    if (block.source_type === \"base64\") {\n      let mimeType: { type: string; subtype: string };\n\n      try {\n        mimeType = parseMimeType(block.mime_type ?? \"\");\n      } catch {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      if (\n        mimeType.type !== \"audio\" ||\n        (mimeType.subtype !== \"wav\" && mimeType.subtype !== \"mp3\")\n      ) {\n        throw new Error(\n          `Audio blocks with source_type ${block.source_type} must have mime type of audio/wav or audio/mp3`\n        );\n      }\n\n      return {\n        type: \"input_audio\",\n        input_audio: {\n          format: mimeType.subtype,\n          data: block.data,\n        },\n      };\n    }\n\n    throw new Error(\n      `Audio content blocks with source_type ${block.source_type} are not supported for ChatOpenAI`\n    );\n  },\n\n  fromStandardFileBlock(block): ChatCompletionContentPart.File {\n    if (block.source_type === \"url\") {\n      const data = parseBase64DataUrl({ dataUrl: block.url });\n\n      const filename = getRequiredFilenameFromMetadata(block);\n\n      if (!data) {\n        throw new Error(\n          `URL file blocks with source_type ${block.source_type} must be formatted as a data URL for ChatOpenAI`\n        );\n      }\n\n      return {\n        type: \"file\",\n        file: {\n          file_data: block.url, // formatted as base64 data URL\n          ...(block.metadata?.filename || block.metadata?.name\n            ? {\n                filename,\n              }\n            : {}),\n        },\n      };\n    }\n\n    if (block.source_type === \"base64\") {\n      const filename = getRequiredFilenameFromMetadata(block);\n\n      return {\n        type: \"file\",\n        file: {\n          file_data: `data:${block.mime_type ?? \"\"};base64,${block.data}`,\n          ...(block.metadata?.filename ||\n          block.metadata?.name ||\n          block.metadata?.title\n            ? {\n                filename,\n              }\n            : {}),\n        },\n      };\n    }\n\n    if (block.source_type === \"id\") {\n      return {\n        type: \"file\",\n        file: {\n          file_id: block.id,\n        },\n      };\n    }\n\n    throw new Error(\n      `File content blocks with source_type ${block.source_type} are not supported for ChatOpenAI`\n    );\n  },\n};\n\n/**\n * Converts an OpenAI Chat Completions API message to a LangChain BaseMessage.\n *\n * This converter transforms messages from OpenAI's Chat Completions API format into\n * LangChain's internal message representation, handling various message types and\n * preserving metadata, tool calls, and other relevant information.\n *\n * @remarks\n * The converter handles the following message roles:\n * - `assistant`: Converted to {@link AIMessage} with support for tool calls, function calls,\n *   audio content, and multi-modal outputs\n * - Other roles: Converted to generic {@link ChatMessage}\n *\n * For assistant messages, the converter:\n * - Parses and validates tool calls, separating valid and invalid calls\n * - Preserves function call information in additional_kwargs\n * - Includes usage statistics and system fingerprint in response_metadata\n * - Handles multi-modal content (text, images, audio)\n * - Optionally includes the raw API response for debugging\n *\n * @param params - Conversion parameters\n * @param params.message - The OpenAI chat completion message to convert\n * @param params.rawResponse - The complete raw response from OpenAI's API, used to extract\n *   metadata like model name, usage statistics, and system fingerprint\n * @param params.includeRawResponse - If true, includes the raw OpenAI response in the\n *   message's additional_kwargs under the `__raw_response` key. Useful for debugging\n *   or accessing provider-specific fields. Defaults to false.\n *\n * @returns A LangChain BaseMessage instance:\n *   - {@link AIMessage} for assistant messages with tool calls, metadata, and content\n *   - {@link ChatMessage} for all other message types\n *\n * @example\n * ```typescript\n * const baseMessage = convertCompletionsMessageToBaseMessage({\n *   message: {\n *     role: \"assistant\",\n *     content: \"Hello! How can I help you?\",\n *     tool_calls: [\n *       {\n *         id: \"call_123\",\n *         type: \"function\",\n *         function: { name: \"get_weather\", arguments: '{\"location\":\"NYC\"}' }\n *       }\n *     ]\n *   },\n *   rawResponse: completionResponse,\n *   includeRawResponse: true\n * });\n * // Returns an AIMessage with parsed tool calls and metadata\n * ```\n *\n * @throws {Error} If tool call parsing fails, the invalid tool call is captured in\n *   the `invalid_tool_calls` array rather than throwing an error\n *\n */\nexport const convertCompletionsMessageToBaseMessage: Converter<\n  {\n    message: OpenAIClient.Chat.Completions.ChatCompletionMessage;\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletion;\n    includeRawResponse?: boolean;\n  },\n  BaseMessage\n> = ({ message, rawResponse, includeRawResponse }) => {\n  const rawToolCalls: OpenAIToolCall[] | undefined = message.tool_calls as\n    | OpenAIToolCall[]\n    | undefined;\n  switch (message.role) {\n    case \"assistant\": {\n      const toolCalls = [];\n      const invalidToolCalls = [];\n      for (const rawToolCall of rawToolCalls ?? []) {\n        try {\n          toolCalls.push(parseToolCall(rawToolCall, { returnId: true }));\n          // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        } catch (e: any) {\n          invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n        }\n      }\n      const additional_kwargs: Record<string, unknown> = {\n        function_call: message.function_call,\n        tool_calls: rawToolCalls,\n      };\n      if (includeRawResponse !== undefined) {\n        additional_kwargs.__raw_response = rawResponse;\n      }\n      const response_metadata: Record<string, unknown> | undefined = {\n        model_provider: \"openai\",\n        model_name: rawResponse.model,\n        ...(rawResponse.system_fingerprint\n          ? {\n              usage: { ...rawResponse.usage },\n              system_fingerprint: rawResponse.system_fingerprint,\n            }\n          : {}),\n      };\n\n      if (message.audio) {\n        additional_kwargs.audio = message.audio;\n      }\n\n      const content = handleMultiModalOutput(\n        message.content || \"\",\n        rawResponse.choices?.[0]?.message\n      );\n      return new AIMessage({\n        content,\n        tool_calls: toolCalls,\n        invalid_tool_calls: invalidToolCalls,\n        additional_kwargs,\n        response_metadata,\n        id: rawResponse.id,\n      });\n    }\n    default:\n      return new ChatMessage(message.content || \"\", message.role ?? \"unknown\");\n  }\n};\n\n/**\n * Converts an OpenAI Chat Completions API delta (streaming chunk) to a LangChain BaseMessageChunk.\n *\n * This converter is used during streaming responses to transform incremental updates from OpenAI's\n * Chat Completions API into LangChain message chunks. It handles various message types, tool calls,\n * function calls, audio content, and role-specific message chunk creation.\n *\n * @param params - Conversion parameters\n * @param params.delta - The delta object from an OpenAI streaming chunk containing incremental\n *   message updates. May include content, role, tool_calls, function_call, audio, etc.\n * @param params.rawResponse - The complete raw ChatCompletionChunk response from OpenAI,\n *   containing metadata like model info, usage stats, and the delta\n * @param params.includeRawResponse - Optional flag to include the raw OpenAI response in the\n *   message chunk's additional_kwargs. Useful for debugging or accessing provider-specific data\n * @param params.defaultRole - Optional default role to use if the delta doesn't specify one.\n *   Typically used to maintain role consistency across chunks in a streaming response\n *\n * @returns A BaseMessageChunk subclass appropriate for the message role:\n *   - HumanMessageChunk for \"user\" role\n *   - AIMessageChunk for \"assistant\" role (includes tool call chunks)\n *   - SystemMessageChunk for \"system\" or \"developer\" roles\n *   - FunctionMessageChunk for \"function\" role\n *   - ToolMessageChunk for \"tool\" role\n *   - ChatMessageChunk for any other role\n *\n * @example\n * Basic streaming text chunk:\n * ```typescript\n * const chunk = convertCompletionsDeltaToBaseMessageChunk({\n *   delta: { role: \"assistant\", content: \"Hello\" },\n *   rawResponse: { id: \"chatcmpl-123\", model: \"gpt-4\", ... }\n * });\n * // Returns: AIMessageChunk with content \"Hello\"\n * ```\n *\n * @example\n * Streaming chunk with tool call:\n * ```typescript\n * const chunk = convertCompletionsDeltaToBaseMessageChunk({\n *   delta: {\n *     role: \"assistant\",\n *     tool_calls: [{\n *       index: 0,\n *       id: \"call_123\",\n *       function: { name: \"get_weather\", arguments: '{\"location\":' }\n *     }]\n *   },\n *   rawResponse: { id: \"chatcmpl-123\", ... }\n * });\n * // Returns: AIMessageChunk with tool_call_chunks containing partial tool call data\n * ```\n *\n * @remarks\n * - Tool calls are converted to ToolCallChunk objects with incremental data\n * - Audio content includes the chunk index from the raw response\n * - The \"developer\" role is mapped to SystemMessageChunk with a special marker\n * - Response metadata includes model provider info and usage statistics\n * - Function calls and tool calls are stored in additional_kwargs for compatibility\n */\nexport const convertCompletionsDeltaToBaseMessageChunk: Converter<\n  {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta: Record<string, any>;\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk;\n    includeRawResponse?: boolean;\n    defaultRole?: OpenAIClient.Chat.ChatCompletionRole;\n  },\n  BaseMessageChunk\n> = ({ delta, rawResponse, includeRawResponse, defaultRole }) => {\n  const role = delta.role ?? defaultRole;\n  const content = delta.content ?? \"\";\n  let additional_kwargs: Record<string, unknown>;\n  if (delta.function_call) {\n    additional_kwargs = {\n      function_call: delta.function_call,\n    };\n  } else if (delta.tool_calls) {\n    additional_kwargs = {\n      tool_calls: delta.tool_calls,\n    };\n  } else {\n    additional_kwargs = {};\n  }\n  if (includeRawResponse) {\n    additional_kwargs.__raw_response = rawResponse;\n  }\n\n  if (delta.audio) {\n    additional_kwargs.audio = {\n      ...delta.audio,\n      index: rawResponse.choices[0].index,\n    };\n  }\n\n  const response_metadata = {\n    model_provider: \"openai\",\n    usage: { ...rawResponse.usage },\n  };\n  if (role === \"user\") {\n    return new HumanMessageChunk({ content, response_metadata });\n  } else if (role === \"assistant\") {\n    const toolCallChunks: ToolCallChunk[] = [];\n    if (Array.isArray(delta.tool_calls)) {\n      for (const rawToolCall of delta.tool_calls) {\n        toolCallChunks.push({\n          name: rawToolCall.function?.name,\n          args: rawToolCall.function?.arguments,\n          id: rawToolCall.id,\n          index: rawToolCall.index,\n          type: \"tool_call_chunk\",\n        });\n      }\n    }\n    return new AIMessageChunk({\n      content,\n      tool_call_chunks: toolCallChunks,\n      additional_kwargs,\n      id: rawResponse.id,\n      response_metadata,\n    });\n  } else if (role === \"system\") {\n    return new SystemMessageChunk({ content, response_metadata });\n  } else if (role === \"developer\") {\n    return new SystemMessageChunk({\n      content,\n      response_metadata,\n      additional_kwargs: {\n        __openai_role__: \"developer\",\n      },\n    });\n  } else if (role === \"function\") {\n    return new FunctionMessageChunk({\n      content,\n      additional_kwargs,\n      name: delta.name,\n      response_metadata,\n    });\n  } else if (role === \"tool\") {\n    return new ToolMessageChunk({\n      content,\n      additional_kwargs,\n      tool_call_id: delta.tool_call_id,\n      response_metadata,\n    });\n  } else {\n    return new ChatMessageChunk({ content, role, response_metadata });\n  }\n};\n\n/**\n * Converts a standard LangChain content block to an OpenAI Completions API content part.\n *\n * This converter transforms LangChain's standardized content blocks (image, audio, file)\n * into the format expected by OpenAI's Chat Completions API. It handles various content\n * types including images (URL or base64), audio (base64), and files (data or file ID).\n *\n * @param block - The standard content block to convert. Can be an image, audio, or file block.\n *\n * @returns An OpenAI Chat Completions content part object, or undefined if the block\n *   cannot be converted (e.g., missing required data).\n *\n * @example\n * Image with URL:\n * ```typescript\n * const block = { type: \"image\", url: \"https://example.com/image.jpg\" };\n * const part = convertStandardContentBlockToCompletionsContentPart(block);\n * // Returns: { type: \"image_url\", image_url: { url: \"https://example.com/image.jpg\" } }\n * ```\n *\n * @example\n * Image with base64 data:\n * ```typescript\n * const block = { type: \"image\", data: \"iVBORw0KGgo...\", mimeType: \"image/png\" };\n * const part = convertStandardContentBlockToCompletionsContentPart(block);\n * // Returns: { type: \"image_url\", image_url: { url: \"data:image/png;base64,iVBORw0KGgo...\" } }\n * ```\n */\nexport const convertStandardContentBlockToCompletionsContentPart: Converter<\n  ContentBlock.Standard,\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPartImage\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPartInputAudio\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPart.File\n  | undefined\n> = (block) => {\n  if (block.type === \"image\") {\n    if (block.url) {\n      return {\n        type: \"image_url\",\n        image_url: {\n          url: block.url,\n        },\n      };\n    } else if (block.data) {\n      return {\n        type: \"image_url\",\n        image_url: {\n          url: `data:${block.mimeType};base64,${block.data}`,\n        },\n      };\n    }\n  }\n  if (block.type === \"audio\") {\n    if (block.data) {\n      const format = iife(() => {\n        const [, format] = block.mimeType.split(\"/\");\n        if (format === \"wav\" || format === \"mp3\") {\n          return format;\n        }\n        return \"wav\";\n      });\n      return {\n        type: \"input_audio\",\n        input_audio: {\n          data: block.data.toString(),\n          format,\n        },\n      };\n    }\n  }\n  if (block.type === \"file\") {\n    if (block.data) {\n      const filename = getRequiredFilenameFromMetadata(block);\n\n      return {\n        type: \"file\",\n        file: {\n          file_data: `data:${block.mimeType};base64,${block.data}`,\n          filename: filename,\n        },\n      };\n    }\n    if (block.fileId) {\n      return {\n        type: \"file\",\n        file: {\n          file_id: block.fileId,\n        },\n      };\n    }\n  }\n  return undefined;\n};\n\n/**\n * Converts a LangChain BaseMessage with standard content blocks to an OpenAI Chat Completions API message parameter.\n *\n * This converter transforms LangChain's standardized message format (using contentBlocks) into the format\n * expected by OpenAI's Chat Completions API. It handles role mapping, content filtering, and multi-modal\n * content conversion for various message types.\n *\n * @remarks\n * The converter performs the following transformations:\n * - Maps LangChain message roles to OpenAI API roles (user, assistant, system, developer, tool, function)\n * - For reasoning models, automatically converts \"system\" role to \"developer\" role\n * - Filters content blocks based on message role (most roles only include text blocks)\n * - For user messages, converts multi-modal content blocks (images, audio, files) to OpenAI format\n * - Preserves tool call IDs for tool messages and function names for function messages\n *\n * Role-specific behavior:\n * - **developer**: Returns only text content blocks (used for reasoning models)\n * - **system**: Returns only text content blocks\n * - **assistant**: Returns only text content blocks\n * - **tool**: Returns only text content blocks with tool_call_id preserved\n * - **function**: Returns text content blocks joined as a single string with function name\n * - **user** (default): Returns multi-modal content including text, images, audio, and files\n *\n * @param params - Conversion parameters\n * @param params.message - The LangChain BaseMessage to convert. Must have contentBlocks property\n *   containing an array of standard content blocks (text, image, audio, file, etc.)\n * @param params.model - Optional model name. Used to determine if special role mapping is needed\n *   (e.g., \"system\" -> \"developer\" for reasoning models like o1)\n *\n * @returns An OpenAI ChatCompletionMessageParam object formatted for the Chat Completions API.\n *   The structure varies by role:\n *   - Developer/System/Assistant: `{ role, content: TextBlock[] }`\n *   - Tool: `{ role: \"tool\", tool_call_id, content: TextBlock[] }`\n *   - Function: `{ role: \"function\", name, content: string }`\n *   - User: `{ role: \"user\", content: Array<TextPart | ImagePart | AudioPart | FilePart> }`\n *\n * @example\n * Simple text message:\n * ```typescript\n * const message = new HumanMessage({\n *   content: [{ type: \"text\", text: \"Hello!\" }]\n * });\n * const param = convertStandardContentMessageToCompletionsMessage({ message });\n * // Returns: { role: \"user\", content: [{ type: \"text\", text: \"Hello!\" }] }\n * ```\n *\n * @example\n * Multi-modal user message with image:\n * ```typescript\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"What's in this image?\" },\n *     { type: \"image\", url: \"https://example.com/image.jpg\" }\n *   ]\n * });\n * const param = convertStandardContentMessageToCompletionsMessage({ message });\n * // Returns: {\n * //   role: \"user\",\n * //   content: [\n * //     { type: \"text\", text: \"What's in this image?\" },\n * //     { type: \"image_url\", image_url: { url: \"https://example.com/image.jpg\" } }\n * //   ]\n * // }\n * ```\n */\nexport const convertStandardContentMessageToCompletionsMessage: Converter<\n  { message: BaseMessage; model?: string },\n  OpenAIClient.Chat.Completions.ChatCompletionMessageParam\n> = ({ message, model }) => {\n  let role = messageToOpenAIRole(message);\n  if (role === \"system\" && isReasoningModel(model)) {\n    role = \"developer\";\n  }\n  if (role === \"developer\") {\n    return {\n      role: \"developer\",\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"system\") {\n    return {\n      role: \"system\",\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"assistant\") {\n    return {\n      role: \"assistant\",\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"tool\" && ToolMessage.isInstance(message)) {\n    return {\n      role: \"tool\",\n      tool_call_id: message.tool_call_id,\n      content: message.contentBlocks.filter((block) => block.type === \"text\"),\n    };\n  } else if (role === \"function\") {\n    return {\n      role: \"function\",\n      name: message.name ?? \"\",\n      content: message.contentBlocks\n        .filter((block) => block.type === \"text\")\n        .join(\"\"),\n    };\n  }\n  // Default to user message handling\n  function* iterateUserContent(blocks: ContentBlock.Standard[]) {\n    for (const block of blocks) {\n      if (block.type === \"text\") {\n        yield {\n          type: \"text\" as const,\n          text: block.text,\n        };\n      }\n      const data = convertStandardContentBlockToCompletionsContentPart(block);\n      if (data) {\n        yield data;\n      }\n    }\n  }\n  return {\n    role: \"user\",\n    content: Array.from(iterateUserContent(message.contentBlocks)),\n  };\n};\n\n/**\n * Converts an array of LangChain BaseMessages to OpenAI Chat Completions API message parameters.\n *\n * This converter transforms LangChain's internal message representation into the format required\n * by OpenAI's Chat Completions API. It handles various message types, roles, content formats,\n * tool calls, function calls, audio messages, and special model-specific requirements.\n *\n * @remarks\n * The converter performs several key transformations:\n * - Maps LangChain message types to OpenAI roles (user, assistant, system, tool, function, developer)\n * - Converts standard content blocks (v1 format) using a specialized converter\n * - Handles multimodal content including text, images, audio, and data blocks\n * - Preserves tool calls and function calls with proper formatting\n * - Applies model-specific role mappings (e.g., \"system\"  \"developer\" for reasoning models)\n * - Splits audio messages into separate message parameters when needed\n *\n * @param params - Conversion parameters\n * @param params.messages - Array of LangChain BaseMessages to convert. Can include any message\n *   type: HumanMessage, AIMessage, SystemMessage, ToolMessage, FunctionMessage, etc.\n * @param params.model - Optional model name used to determine if special role mapping is needed.\n *   For reasoning models (o1, o3, etc.), \"system\" role is converted to \"developer\" role.\n *\n * @returns Array of ChatCompletionMessageParam objects formatted for OpenAI's Chat Completions API.\n *   Some messages may be split into multiple parameters (e.g., audio messages).\n *\n * @example\n * Basic message conversion:\n * ```typescript\n * const messages = [\n *   new HumanMessage(\"What's the weather like?\"),\n *   new AIMessage(\"Let me check that for you.\")\n * ];\n *\n * const params = convertMessagesToCompletionsMessageParams({\n *   messages,\n *   model: \"gpt-4\"\n * });\n * // Returns:\n * // [\n * //   { role: \"user\", content: \"What's the weather like?\" },\n * //   { role: \"assistant\", content: \"Let me check that for you.\" }\n * // ]\n * ```\n *\n * @example\n * Message with tool calls:\n * ```typescript\n * const messages = [\n *   new AIMessage({\n *     content: \"\",\n *     tool_calls: [{\n *       id: \"call_123\",\n *       name: \"get_weather\",\n *       args: { location: \"San Francisco\" }\n *     }]\n *   })\n * ];\n *\n * const params = convertMessagesToCompletionsMessageParams({ messages });\n * // Returns:\n * // [{\n * //   role: \"assistant\",\n * //   content: \"\",\n * //   tool_calls: [{\n * //     id: \"call_123\",\n * //     type: \"function\",\n * //     function: { name: \"get_weather\", arguments: '{\"location\":\"San Francisco\"}' }\n * //   }]\n * // }]\n * ```\n */\nexport const convertMessagesToCompletionsMessageParams: Converter<\n  { messages: BaseMessage[]; model?: string },\n  OpenAIClient.Chat.Completions.ChatCompletionMessageParam[]\n> = ({ messages, model }) => {\n  return messages.flatMap((message) => {\n    if (\n      \"output_version\" in message.response_metadata &&\n      message.response_metadata?.output_version === \"v1\"\n    ) {\n      return convertStandardContentMessageToCompletionsMessage({ message });\n    }\n    let role = messageToOpenAIRole(message);\n    if (role === \"system\" && isReasoningModel(model)) {\n      role = \"developer\";\n    }\n\n    const content =\n      typeof message.content === \"string\"\n        ? message.content\n        : message.content.map((m) => {\n            if (isDataContentBlock(m)) {\n              return convertToProviderContentBlock(\n                m,\n                completionsApiContentBlockConverter\n              );\n            }\n            return m;\n          });\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const completionParam: Record<string, any> = {\n      role,\n      content,\n    };\n    if (message.name != null) {\n      completionParam.name = message.name;\n    }\n    if (message.additional_kwargs.function_call != null) {\n      completionParam.function_call = message.additional_kwargs.function_call;\n    }\n    if (AIMessage.isInstance(message) && !!message.tool_calls?.length) {\n      completionParam.tool_calls = message.tool_calls.map(\n        convertLangChainToolCallToOpenAI\n      );\n    } else {\n      if (message.additional_kwargs.tool_calls != null) {\n        completionParam.tool_calls = message.additional_kwargs.tool_calls;\n      }\n      if (ToolMessage.isInstance(message) && message.tool_call_id != null) {\n        completionParam.tool_call_id = message.tool_call_id;\n      }\n    }\n\n    if (\n      message.additional_kwargs.audio &&\n      typeof message.additional_kwargs.audio === \"object\" &&\n      \"id\" in message.additional_kwargs.audio\n    ) {\n      const audioMessage = {\n        role: \"assistant\",\n        audio: {\n          id: message.additional_kwargs.audio.id,\n        },\n      };\n      return [\n        completionParam,\n        audioMessage,\n      ] as OpenAIClient.Chat.Completions.ChatCompletionMessageParam[];\n    }\n\n    return completionParam as OpenAIClient.Chat.Completions.ChatCompletionMessageParam;\n  });\n};\n"],"names":["completionsApiContentBlockConverter: StandardContentBlockConverter<{\n  text: ChatCompletionContentPartText;\n  image: ChatCompletionContentPartImage;\n  audio: ChatCompletionContentPartInputAudio;\n  file: ChatCompletionContentPart.File;\n}>","mimeType: { type: string; subtype: string }","convertCompletionsMessageToBaseMessage: Converter<\n  {\n    message: OpenAIClient.Chat.Completions.ChatCompletionMessage;\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletion;\n    includeRawResponse?: boolean;\n  },\n  BaseMessage\n>","rawToolCalls: OpenAIToolCall[] | undefined","e: any","additional_kwargs: Record<string, unknown>","response_metadata: Record<string, unknown> | undefined","convertCompletionsDeltaToBaseMessageChunk: Converter<\n  {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta: Record<string, any>;\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk;\n    includeRawResponse?: boolean;\n    defaultRole?: OpenAIClient.Chat.ChatCompletionRole;\n  },\n  BaseMessageChunk\n>","toolCallChunks: ToolCallChunk[]","convertStandardContentBlockToCompletionsContentPart: Converter<\n  ContentBlock.Standard,\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPartImage\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPartInputAudio\n  | OpenAIClient.Chat.Completions.ChatCompletionContentPart.File\n  | undefined\n>","format","convertStandardContentMessageToCompletionsMessage: Converter<\n  { message: BaseMessage; model?: string },\n  OpenAIClient.Chat.Completions.ChatCompletionMessageParam\n>","blocks: ContentBlock.Standard[]","convertMessagesToCompletionsMessageParams: Converter<\n  { messages: BaseMessage[]; model?: string },\n  OpenAIClient.Chat.Completions.ChatCompletionMessageParam[]\n>","completionParam: Record<string, any>"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6CA,MAAaA,sCAKR;IACH,cAAc;IAEd,uBAAsB,KAAA,EAAsC;QAC1D,OAAO;YAAE,MAAM;YAAQ,MAAM,MAAM,IAAA;QAAM;IAC1C;IAED,wBAAuB,KAAA,EAAuC;QAC5D,IAAI,MAAM,WAAA,KAAgB,MACxB,CAAA,OAAO;YACL,MAAM;YACN,WAAW;gBACT,KAAK,MAAM,GAAA;gBACX,GAAI,MAAM,QAAA,EAAU,SAChB;oBAAE,QAAQ,MAAM,QAAA,CAAS,MAAA;gBAAmC,IAC5D,CAAE,CAAA;YACP;QACF;QAGH,IAAI,MAAM,WAAA,KAAgB,UAAU;YAClC,MAAM,MAAM,CAAC,KAAK,EAAE,MAAM,SAAA,IAAa,GAAG,QAAQ,EAAE,MAAM,IAAA,EAAM;YAChE,OAAO;gBACL,MAAM;gBACN,WAAW;oBACT;oBACA,GAAI,MAAM,QAAA,EAAU,SAChB;wBAAE,QAAQ,MAAM,QAAA,CAAS,MAAA;oBAAmC,IAC5D,CAAE,CAAA;gBACP;YACF;QACF;QAED,MAAM,IAAI,MACR,CAAC,sCAAsC,EAAE,MAAM,WAAA,CAAY,iCAAiC,CAAC;IAEhG;IAED,wBAAuB,KAAA,EAA4C;QACjE,IAAI,MAAM,WAAA,KAAgB,OAAO;YAC/B,MAAM,WAAO,kMAAA,EAAmB;gBAAE,SAAS,MAAM,GAAA;YAAK,EAAC;YACvD,IAAI,CAAC,KACH,CAAA,MAAM,IAAI,MACR,CAAC,kCAAkC,EAAE,MAAM,WAAA,CAAY,+CAA+C,CAAC;YAI3G,MAAM,cAAc,KAAK,SAAA,IAAa,MAAM,SAAA,IAAa;YACzD,IAAIC;YAEJ,IAAI;gBACF,eAAW,6LAAA,EAAc,YAAY;YACtC,EAAA,OAAO;gBACN,MAAM,IAAI,MACR,CAAC,8BAA8B,EAAE,MAAM,WAAA,CAAY,8CAA8C,CAAC;YAErG;YAED,IACE,SAAS,IAAA,KAAS,WACjB,SAAS,OAAA,KAAY,SAAS,SAAS,OAAA,KAAY,MAEpD,CAAA,MAAM,IAAI,MACR,CAAC,8BAA8B,EAAE,MAAM,WAAA,CAAY,8CAA8C,CAAC;YAItG,OAAO;gBACL,MAAM;gBACN,aAAa;oBACX,QAAQ,SAAS,OAAA;oBACjB,MAAM,KAAK,IAAA;gBACZ;YACF;QACF;QAED,IAAI,MAAM,WAAA,KAAgB,UAAU;YAClC,IAAIA;YAEJ,IAAI;gBACF,eAAW,6LAAA,EAAc,MAAM,SAAA,IAAa,GAAG;YAChD,EAAA,OAAO;gBACN,MAAM,IAAI,MACR,CAAC,8BAA8B,EAAE,MAAM,WAAA,CAAY,8CAA8C,CAAC;YAErG;YAED,IACE,SAAS,IAAA,KAAS,WACjB,SAAS,OAAA,KAAY,SAAS,SAAS,OAAA,KAAY,MAEpD,CAAA,MAAM,IAAI,MACR,CAAC,8BAA8B,EAAE,MAAM,WAAA,CAAY,8CAA8C,CAAC;YAItG,OAAO;gBACL,MAAM;gBACN,aAAa;oBACX,QAAQ,SAAS,OAAA;oBACjB,MAAM,MAAM,IAAA;gBACb;YACF;QACF;QAED,MAAM,IAAI,MACR,CAAC,sCAAsC,EAAE,MAAM,WAAA,CAAY,iCAAiC,CAAC;IAEhG;IAED,uBAAsB,KAAA,EAAuC;QAC3D,IAAI,MAAM,WAAA,KAAgB,OAAO;YAC/B,MAAM,WAAO,kMAAA,EAAmB;gBAAE,SAAS,MAAM,GAAA;YAAK,EAAC;YAEvD,MAAM,eAAW,mMAAA,EAAgC,MAAM;YAEvD,IAAI,CAAC,KACH,CAAA,MAAM,IAAI,MACR,CAAC,iCAAiC,EAAE,MAAM,WAAA,CAAY,+CAA+C,CAAC;YAI1G,OAAO;gBACL,MAAM;gBACN,MAAM;oBACJ,WAAW,MAAM,GAAA;oBACjB,GAAI,MAAM,QAAA,EAAU,YAAY,MAAM,QAAA,EAAU,OAC5C;wBACE;oBACD,IACD,CAAE,CAAA;gBACP;YACF;QACF;QAED,IAAI,MAAM,WAAA,KAAgB,UAAU;YAClC,MAAM,eAAW,mMAAA,EAAgC,MAAM;YAEvD,OAAO;gBACL,MAAM;gBACN,MAAM;oBACJ,WAAW,CAAC,KAAK,EAAE,MAAM,SAAA,IAAa,GAAG,QAAQ,EAAE,MAAM,IAAA,EAAM;oBAC/D,GAAI,MAAM,QAAA,EAAU,YACpB,MAAM,QAAA,EAAU,QAChB,MAAM,QAAA,EAAU,QACZ;wBACE;oBACD,IACD,CAAE,CAAA;gBACP;YACF;QACF;QAED,IAAI,MAAM,WAAA,KAAgB,KACxB,CAAA,OAAO;YACL,MAAM;YACN,MAAM;gBACJ,SAAS,MAAM,EAAA;YAChB;QACF;QAGH,MAAM,IAAI,MACR,CAAC,qCAAqC,EAAE,MAAM,WAAA,CAAY,iCAAiC,CAAC;IAE/F;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA0DD,MAAaC,yCAOT,CAAC,EAAE,OAAA,EAAS,WAAA,EAAa,kBAAA,EAAoB,KAAK;IACpD,MAAMC,eAA6C,QAAQ,UAAA;IAG3D,OAAQ,QAAQ,IAAA,EAAhB;QACE,KAAK;YAAa;gBAChB,MAAM,YAAY,CAAE,CAAA;gBACpB,MAAM,mBAAmB,CAAE,CAAA;gBAC3B,KAAK,MAAM,eAAe,gBAAgB,CAAE,CAAA,CAC1C,IAAI;oBACF,UAAU,IAAA,KAAK,6NAAA,EAAc,aAAa;wBAAE,UAAU;oBAAM,EAAC,CAAC;gBAE/D,EAAA,OAAQC,GAAQ;oBACf,iBAAiB,IAAA,KAAK,mOAAA,EAAoB,aAAa,EAAE,OAAA,CAAQ,CAAC;gBACnE;gBAEH,MAAMC,oBAA6C;oBACjD,eAAe,QAAQ,aAAA;oBACvB,YAAY;gBACb;gBACD,IAAI,uBAAuB,KAAA,GACzB,kBAAkB,cAAA,GAAiB;gBAErC,MAAMC,oBAAyD;oBAC7D,gBAAgB;oBAChB,YAAY,YAAY,KAAA;oBACxB,GAAI,YAAY,kBAAA,GACZ;wBACE,OAAO;4BAAE,GAAG,YAAY,KAAA;wBAAO;wBAC/B,oBAAoB,YAAY,kBAAA;oBACjC,IACD,CAAE,CAAA;gBACP;gBAED,IAAI,QAAQ,KAAA,EACV,kBAAkB,KAAA,GAAQ,QAAQ,KAAA;gBAGpC,MAAM,cAAU,4LAAA,EACd,QAAQ,OAAA,IAAW,IACnB,YAAY,OAAA,EAAA,CAAU,EAAA,EAAI,QAC3B;gBACD,OAAO,IAAI,4KAAA,CAAU;oBACnB;oBACA,YAAY;oBACZ,oBAAoB;oBACpB;oBACA;oBACA,IAAI,YAAY,EAAA;gBACjB;YACF;QACD,QACE;YAAA,OAAO,IAAI,gLAAA,CAAY,QAAQ,OAAA,IAAW,IAAI,QAAQ,IAAA,IAAQ;IACjE;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6DD,MAAaC,4CAST,CAAC,EAAE,KAAA,EAAO,WAAA,EAAa,kBAAA,EAAoB,WAAA,EAAa,KAAK;IAC/D,MAAM,OAAO,MAAM,IAAA,IAAQ;IAC3B,MAAM,UAAU,MAAM,OAAA,IAAW;IACjC,IAAIF;IACJ,IAAI,MAAM,aAAA,EACR,oBAAoB;QAClB,eAAe,MAAM,aAAA;IACtB;aACQ,MAAM,UAAA,EACf,oBAAoB;QAClB,YAAY,MAAM,UAAA;IACnB;SAED,oBAAoB,CAAE;IAExB,IAAI,oBACF,kBAAkB,cAAA,GAAiB;IAGrC,IAAI,MAAM,KAAA,EACR,kBAAkB,KAAA,GAAQ;QACxB,GAAG,MAAM,KAAA;QACT,OAAO,YAAY,OAAA,CAAQ,EAAA,CAAG,KAAA;IAC/B;IAGH,MAAM,oBAAoB;QACxB,gBAAgB;QAChB,OAAO;YAAE,GAAG,YAAY,KAAA;QAAO;IAChC;IACD,IAAI,SAAS,OACX,CAAA,OAAO,IAAI,uLAAA,CAAkB;QAAE;QAAS;IAAmB;aAClD,SAAS,aAAa;QAC/B,MAAMG,iBAAkC,CAAE,CAAA;QAC1C,IAAI,MAAM,OAAA,CAAQ,MAAM,UAAA,CAAW,CACjC,CAAA,KAAK,MAAM,eAAe,MAAM,UAAA,CAC9B,eAAe,IAAA,CAAK;YAClB,MAAM,YAAY,QAAA,EAAU;YAC5B,MAAM,YAAY,QAAA,EAAU;YAC5B,IAAI,YAAY,EAAA;YAChB,OAAO,YAAY,KAAA;YACnB,MAAM;QACP,EAAC;QAGN,OAAO,IAAI,iLAAA,CAAe;YACxB;YACA,kBAAkB;YAClB;YACA,IAAI,YAAY,EAAA;YAChB;QACD;IACF,OAAA,IAAU,SAAS,SAClB,CAAA,OAAO,IAAI,yLAAA,CAAmB;QAAE;QAAS;IAAmB;aACnD,SAAS,YAClB,CAAA,OAAO,IAAI,yLAAA,CAAmB;QAC5B;QACA;QACA,mBAAmB;YACjB,iBAAiB;QAClB;IACF;aACQ,SAAS,WAClB,CAAA,OAAO,IAAI,6LAAA,CAAqB;QAC9B;QACA;QACA,MAAM,MAAM,IAAA;QACZ;IACD;aACQ,SAAS,OAClB,CAAA,OAAO,IAAI,qLAAA,CAAiB;QAC1B;QACA;QACA,cAAc,MAAM,YAAA;QACpB;IACD;SAED,OAAO,IAAI,qLAAA,CAAiB;QAAE;QAAS;QAAM;IAAmB;AAEnE;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA8BD,MAAaC,sDAMT,CAAC,UAAU;IACb,IAAI,MAAM,IAAA,KAAS,SACjB;YAAI,MAAM,GAAA,CACR,CAAA,OAAO;YACL,MAAM;YACN,WAAW;gBACT,KAAK,MAAM,GAAA;YACZ;QACF;iBACQ,MAAM,IAAA,CACf,CAAA,OAAO;YACL,MAAM;YACN,WAAW;gBACT,KAAK,CAAC,KAAK,EAAE,MAAM,QAAA,CAAS,QAAQ,EAAE,MAAM,IAAA,EAAM;YACnD;QACF;IACF;IAEH,IAAI,MAAM,IAAA,KAAS,SACjB;YAAI,MAAM,IAAA,EAAM;YACd,MAAM,aAAS,0KAAA,EAAK,MAAM;gBACxB,MAAM,GAAGC,SAAO,GAAG,MAAM,QAAA,CAAS,KAAA,CAAM,IAAI;gBAC5C,IAAIA,aAAW,SAASA,aAAW,MACjC,CAAA,OAAOA;gBAET,OAAO;YACR,EAAC;YACF,OAAO;gBACL,MAAM;gBACN,aAAa;oBACX,MAAM,MAAM,IAAA,CAAK,QAAA,EAAU;oBAC3B;gBACD;YACF;QACF;;IAEH,IAAI,MAAM,IAAA,KAAS,QAAQ;QACzB,IAAI,MAAM,IAAA,EAAM;YACd,MAAM,eAAW,mMAAA,EAAgC,MAAM;YAEvD,OAAO;gBACL,MAAM;gBACN,MAAM;oBACJ,WAAW,CAAC,KAAK,EAAE,MAAM,QAAA,CAAS,QAAQ,EAAE,MAAM,IAAA,EAAM;oBAC9C;gBACX;YACF;QACF;QACD,IAAI,MAAM,MAAA,CACR,CAAA,OAAO;YACL,MAAM;YACN,MAAM;gBACJ,SAAS,MAAM,MAAA;YAChB;QACF;IAEJ;IACD,OAAO,KAAA;AACR;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAmED,MAAaC,oDAGT,CAAC,EAAE,OAAA,EAAS,KAAA,EAAO,KAAK;IAC1B,IAAI,WAAO,uLAAA,EAAoB,QAAQ;IACvC,IAAI,SAAS,gBAAY,oLAAA,EAAiB,MAAM,EAC9C,OAAO;IAET,IAAI,SAAS,YACX,CAAA,OAAO;QACL,MAAM;QACN,SAAS,QAAQ,aAAA,CAAc,MAAA,CAAO,CAAC,QAAU,MAAM,IAAA,KAAS,OAAO;IACxE;aACQ,SAAS,SAClB,CAAA,OAAO;QACL,MAAM;QACN,SAAS,QAAQ,aAAA,CAAc,MAAA,CAAO,CAAC,QAAU,MAAM,IAAA,KAAS,OAAO;IACxE;aACQ,SAAS,YAClB,CAAA,OAAO;QACL,MAAM;QACN,SAAS,QAAQ,aAAA,CAAc,MAAA,CAAO,CAAC,QAAU,MAAM,IAAA,KAAS,OAAO;IACxE;aACQ,SAAS,UAAU,gLAAA,CAAY,UAAA,CAAW,QAAQ,CAC3D,CAAA,OAAO;QACL,MAAM;QACN,cAAc,QAAQ,YAAA;QACtB,SAAS,QAAQ,aAAA,CAAc,MAAA,CAAO,CAAC,QAAU,MAAM,IAAA,KAAS,OAAO;IACxE;aACQ,SAAS,WAClB,CAAA,OAAO;QACL,MAAM;QACN,MAAM,QAAQ,IAAA,IAAQ;QACtB,SAAS,QAAQ,aAAA,CACd,MAAA,CAAO,CAAC,QAAU,MAAM,IAAA,KAAS,OAAO,CACxC,IAAA,CAAK,GAAG;IACZ;IAGH,UAAU,mBAAmBC,MAAAA,EAAiC;QAC5D,KAAK,MAAM,SAAS,OAAQ;YAC1B,IAAI,MAAM,IAAA,KAAS,QACjB,MAAM;gBACJ,MAAM;gBACN,MAAM,MAAM,IAAA;YACb;YAEH,MAAM,OAAO,oDAAoD,MAAM;YACvE,IAAI,MACF,MAAM;QAET;IACF;IACD,OAAO;QACL,MAAM;QACN,SAAS,MAAM,IAAA,CAAK,mBAAmB,QAAQ,aAAA,CAAc,CAAC;IAC/D;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAyED,MAAaC,4CAGT,CAAC,EAAE,QAAA,EAAU,KAAA,EAAO,KAAK;IAC3B,OAAO,SAAS,OAAA,CAAQ,CAAC,YAAY;QACnC,IACE,oBAAoB,QAAQ,iBAAA,IAC5B,QAAQ,iBAAA,EAAmB,mBAAmB,KAE9C,CAAA,OAAO,kDAAkD;YAAE;QAAS,EAAC;QAEvE,IAAI,WAAO,uLAAA,EAAoB,QAAQ;QACvC,IAAI,SAAS,gBAAY,oLAAA,EAAiB,MAAM,EAC9C,OAAO;QAGT,MAAM,UACJ,OAAO,QAAQ,OAAA,KAAY,WACvB,QAAQ,OAAA,GACR,QAAQ,OAAA,CAAQ,GAAA,CAAI,CAAC,MAAM;YACzB,QAAI,kMAAA,EAAmB,EAAE,CACvB,CAAA,WAAO,6MAAA,EACL,GACA,oCACD;YAEH,OAAO;QACR,EAAC;QAER,MAAMC,kBAAuC;YAC3C;YACA;QACD;QACD,IAAI,QAAQ,IAAA,IAAQ,MAClB,gBAAgB,IAAA,GAAO,QAAQ,IAAA;QAEjC,IAAI,QAAQ,iBAAA,CAAkB,aAAA,IAAiB,MAC7C,gBAAgB,aAAA,GAAgB,QAAQ,iBAAA,CAAkB,aAAA;QAE5D,IAAI,4KAAA,CAAU,UAAA,CAAW,QAAQ,IAAI,CAAC,CAAC,QAAQ,UAAA,EAAY,QACzD,gBAAgB,UAAA,GAAa,QAAQ,UAAA,CAAW,GAAA,CAC9C,gPAAA,CACD;aACI;YACL,IAAI,QAAQ,iBAAA,CAAkB,UAAA,IAAc,MAC1C,gBAAgB,UAAA,GAAa,QAAQ,iBAAA,CAAkB,UAAA;YAEzD,IAAI,gLAAA,CAAY,UAAA,CAAW,QAAQ,IAAI,QAAQ,YAAA,IAAgB,MAC7D,gBAAgB,YAAA,GAAe,QAAQ,YAAA;QAE1C;QAED,IACE,QAAQ,iBAAA,CAAkB,KAAA,IAC1B,OAAO,QAAQ,iBAAA,CAAkB,KAAA,KAAU,YAC3C,QAAQ,QAAQ,iBAAA,CAAkB,KAAA,EAClC;YACA,MAAM,eAAe;gBACnB,MAAM;gBACN,OAAO;oBACL,IAAI,QAAQ,iBAAA,CAAkB,KAAA,CAAM,EAAA;gBACrC;YACF;YACD,OAAO;gBACL;gBACA,YACD;aAAA;QACF;QAED,OAAO;IACR,EAAC;AACH"}},
    {"offset": {"line": 2493, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/chat_models/completions.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/chat_models/completions.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  AIMessage,\n  AIMessageChunk,\n  type BaseMessage,\n  isAIMessage,\n  type UsageMetadata,\n  type AIMessageFields,\n  BaseMessageChunk,\n} from \"@langchain/core/messages\";\nimport {\n  ChatGenerationChunk,\n  type ChatGeneration,\n  type ChatResult,\n} from \"@langchain/core/outputs\";\nimport { NewTokenIndices } from \"@langchain/core/callbacks/base\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\nimport {\n  OpenAIToolChoice,\n  formatToOpenAIToolChoice,\n  _convertToOpenAITool,\n} from \"../utils/tools.js\";\nimport { isReasoningModel } from \"../utils/misc.js\";\nimport { BaseChatOpenAICallOptions } from \"./base.js\";\nimport { BaseChatOpenAI } from \"./base.js\";\nimport {\n  convertCompletionsDeltaToBaseMessageChunk,\n  convertCompletionsMessageToBaseMessage,\n  convertMessagesToCompletionsMessageParams,\n} from \"../converters/completions.js\";\n\nexport interface ChatOpenAICompletionsCallOptions\n  extends BaseChatOpenAICallOptions {}\n\ntype ChatCompletionsInvocationParams = Omit<\n  OpenAIClient.Chat.Completions.ChatCompletionCreateParams,\n  \"messages\"\n>;\n\n/**\n * OpenAI Completions API implementation.\n * @internal\n */\nexport class ChatOpenAICompletions<\n  CallOptions extends ChatOpenAICompletionsCallOptions = ChatOpenAICompletionsCallOptions\n> extends BaseChatOpenAI<CallOptions> {\n  /** @internal */\n  override invocationParams(\n    options?: this[\"ParsedCallOptions\"],\n    extra?: { streaming?: boolean }\n  ): ChatCompletionsInvocationParams {\n    let strict: boolean | undefined;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n\n    let streamOptionsConfig = {};\n    if (options?.stream_options !== undefined) {\n      streamOptionsConfig = { stream_options: options.stream_options };\n    } else if (this.streamUsage && (this.streaming || extra?.streaming)) {\n      streamOptionsConfig = { stream_options: { include_usage: true } };\n    }\n\n    const params: Partial<ChatCompletionsInvocationParams> = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      logprobs: this.logprobs,\n      top_logprobs: this.topLogprobs,\n      n: this.n,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stopSequences,\n      user: this.user,\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      functions: options?.functions,\n      function_call: options?.function_call,\n      tools: options?.tools?.length\n        ? options.tools.map((tool) =>\n            this._convertChatOpenAIToolToCompletionsTool(tool, { strict })\n          )\n        : undefined,\n      tool_choice: formatToOpenAIToolChoice(\n        options?.tool_choice as OpenAIToolChoice\n      ),\n      response_format: this._getResponseFormat(options?.response_format),\n      seed: options?.seed,\n      ...streamOptionsConfig,\n      parallel_tool_calls: options?.parallel_tool_calls,\n      ...(this.audio || options?.audio\n        ? { audio: this.audio || options?.audio }\n        : {}),\n      ...(this.modalities || options?.modalities\n        ? { modalities: this.modalities || options?.modalities }\n        : {}),\n      ...this.modelKwargs,\n      prompt_cache_key: options?.promptCacheKey ?? this.promptCacheKey,\n      prompt_cache_retention:\n        options?.promptCacheRetention ?? this.promptCacheRetention,\n      verbosity: options?.verbosity ?? this.verbosity,\n    };\n    if (options?.prediction !== undefined) {\n      params.prediction = options.prediction;\n    }\n    if (this.service_tier !== undefined) {\n      params.service_tier = this.service_tier;\n    }\n    if (options?.service_tier !== undefined) {\n      params.service_tier = options.service_tier;\n    }\n    const reasoning = this._getReasoningParams(options);\n    if (reasoning !== undefined && reasoning.effort !== undefined) {\n      params.reasoning_effort = reasoning.effort;\n    }\n    if (isReasoningModel(params.model)) {\n      params.max_completion_tokens =\n        this.maxTokens === -1 ? undefined : this.maxTokens;\n    } else {\n      params.max_tokens = this.maxTokens === -1 ? undefined : this.maxTokens;\n    }\n\n    return params as ChatCompletionsInvocationParams;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const usageMetadata = {} as UsageMetadata;\n    const params = this.invocationParams(options);\n    const messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[] =\n      convertMessagesToCompletionsMessageParams({\n        messages,\n        model: this.model,\n      });\n\n    if (params.stream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks: Record<number, ChatGenerationChunk> = {};\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata,\n        };\n        const index =\n          (chunk.generationInfo as NewTokenIndices)?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks)\n        .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n        .map(([_, value]) => value);\n\n      const { functions, function_call } = this.invocationParams(options);\n\n      // OpenAI does not support token usage report under stream mode,\n      // fallback to estimation.\n\n      const promptTokenUsage = await this._getEstimatedTokenCountFromPrompt(\n        messages,\n        functions,\n        function_call\n      );\n      const completionTokenUsage = await this._getNumTokensFromGenerations(\n        generations\n      );\n\n      usageMetadata.input_tokens = promptTokenUsage;\n      usageMetadata.output_tokens = completionTokenUsage;\n      usageMetadata.total_tokens = promptTokenUsage + completionTokenUsage;\n      return {\n        generations,\n        llmOutput: {\n          estimatedTokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens,\n          },\n        },\n      };\n    } else {\n      const data = await this.completionWithRetry(\n        {\n          ...params,\n          stream: false,\n          messages: messagesMapped,\n        },\n        {\n          signal: options?.signal,\n          ...options?.options,\n        }\n      );\n\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens,\n        prompt_tokens_details: promptTokensDetails,\n        completion_tokens_details: completionTokensDetails,\n      } = data?.usage ?? {};\n\n      if (completionTokens) {\n        usageMetadata.output_tokens =\n          (usageMetadata.output_tokens ?? 0) + completionTokens;\n      }\n\n      if (promptTokens) {\n        usageMetadata.input_tokens =\n          (usageMetadata.input_tokens ?? 0) + promptTokens;\n      }\n\n      if (totalTokens) {\n        usageMetadata.total_tokens =\n          (usageMetadata.total_tokens ?? 0) + totalTokens;\n      }\n\n      if (\n        promptTokensDetails?.audio_tokens !== null ||\n        promptTokensDetails?.cached_tokens !== null\n      ) {\n        usageMetadata.input_token_details = {\n          ...(promptTokensDetails?.audio_tokens !== null && {\n            audio: promptTokensDetails?.audio_tokens,\n          }),\n          ...(promptTokensDetails?.cached_tokens !== null && {\n            cache_read: promptTokensDetails?.cached_tokens,\n          }),\n        };\n      }\n\n      if (\n        completionTokensDetails?.audio_tokens !== null ||\n        completionTokensDetails?.reasoning_tokens !== null\n      ) {\n        usageMetadata.output_token_details = {\n          ...(completionTokensDetails?.audio_tokens !== null && {\n            audio: completionTokensDetails?.audio_tokens,\n          }),\n          ...(completionTokensDetails?.reasoning_tokens !== null && {\n            reasoning: completionTokensDetails?.reasoning_tokens,\n          }),\n        };\n      }\n\n      const generations: ChatGeneration[] = [];\n      for (const part of data?.choices ?? []) {\n        const text = part.message?.content ?? \"\";\n        const generation: ChatGeneration = {\n          text,\n          message: this._convertCompletionsMessageToBaseMessage(\n            part.message ?? { role: \"assistant\" },\n            data\n          ),\n        };\n        generation.generationInfo = {\n          ...(part.finish_reason ? { finish_reason: part.finish_reason } : {}),\n          ...(part.logprobs ? { logprobs: part.logprobs } : {}),\n        };\n        if (isAIMessage(generation.message)) {\n          generation.message.usage_metadata = usageMetadata;\n        }\n        // Fields are not serialized unless passed to the constructor\n        // Doing this ensures all fields on the message are serialized\n        generation.message = new AIMessage(\n          Object.fromEntries(\n            Object.entries(generation.message).filter(\n              ([key]) => !key.startsWith(\"lc_\")\n            )\n          ) as AIMessageFields\n        );\n        generations.push(generation);\n      }\n      return {\n        generations,\n        llmOutput: {\n          tokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens,\n          },\n        },\n      };\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[] =\n      convertMessagesToCompletionsMessageParams({\n        messages,\n        model: this.model,\n      });\n\n    const params = {\n      ...this.invocationParams(options, {\n        streaming: true,\n      }),\n      messages: messagesMapped,\n      stream: true as const,\n    };\n    let defaultRole: OpenAIClient.Chat.ChatCompletionRole | undefined;\n\n    const streamIterable = await this.completionWithRetry(params, options);\n    let usage: OpenAIClient.Completions.CompletionUsage | undefined;\n    for await (const data of streamIterable) {\n      const choice = data?.choices?.[0];\n      if (data.usage) {\n        usage = data.usage;\n      }\n      if (!choice) {\n        continue;\n      }\n\n      const { delta } = choice;\n      if (!delta) {\n        continue;\n      }\n      const chunk = this._convertCompletionsDeltaToBaseMessageChunk(\n        delta,\n        data,\n        defaultRole\n      );\n      defaultRole = delta.role ?? defaultRole;\n      const newTokenIndices = {\n        prompt: options.promptIndex ?? 0,\n        completion: choice.index ?? 0,\n      };\n      if (typeof chunk.content !== \"string\") {\n        console.log(\n          \"[WARNING]: Received non-string content from OpenAI. This is currently not supported.\"\n        );\n        continue;\n      }\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      const generationInfo: Record<string, any> = { ...newTokenIndices };\n      if (choice.finish_reason != null) {\n        generationInfo.finish_reason = choice.finish_reason;\n        // Only include system fingerprint in the last chunk for now\n        // to avoid concatenation issues\n        generationInfo.system_fingerprint = data.system_fingerprint;\n        generationInfo.model_name = data.model;\n        generationInfo.service_tier = data.service_tier;\n      }\n      if (this.logprobs) {\n        generationInfo.logprobs = choice.logprobs;\n      }\n      const generationChunk = new ChatGenerationChunk({\n        message: chunk,\n        text: chunk.content,\n        generationInfo,\n      });\n      yield generationChunk;\n      await runManager?.handleLLMNewToken(\n        generationChunk.text ?? \"\",\n        newTokenIndices,\n        undefined,\n        undefined,\n        undefined,\n        { chunk: generationChunk }\n      );\n    }\n    if (usage) {\n      const inputTokenDetails = {\n        ...(usage.prompt_tokens_details?.audio_tokens !== null && {\n          audio: usage.prompt_tokens_details?.audio_tokens,\n        }),\n        ...(usage.prompt_tokens_details?.cached_tokens !== null && {\n          cache_read: usage.prompt_tokens_details?.cached_tokens,\n        }),\n      };\n      const outputTokenDetails = {\n        ...(usage.completion_tokens_details?.audio_tokens !== null && {\n          audio: usage.completion_tokens_details?.audio_tokens,\n        }),\n        ...(usage.completion_tokens_details?.reasoning_tokens !== null && {\n          reasoning: usage.completion_tokens_details?.reasoning_tokens,\n        }),\n      };\n      const generationChunk = new ChatGenerationChunk({\n        message: new AIMessageChunk({\n          content: \"\",\n          response_metadata: {\n            usage: { ...usage },\n          },\n          usage_metadata: {\n            input_tokens: usage.prompt_tokens,\n            output_tokens: usage.completion_tokens,\n            total_tokens: usage.total_tokens,\n            ...(Object.keys(inputTokenDetails).length > 0 && {\n              input_token_details: inputTokenDetails,\n            }),\n            ...(Object.keys(outputTokenDetails).length > 0 && {\n              output_token_details: outputTokenDetails,\n            }),\n          },\n        }),\n        text: \"\",\n      });\n      yield generationChunk;\n    }\n    if (options.signal?.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParamsStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Chat.Completions.ChatCompletionChunk>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParamsNonStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<OpenAIClient.Chat.Completions.ChatCompletion>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParams,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<\n    | AsyncIterable<OpenAIClient.Chat.Completions.ChatCompletionChunk>\n    | OpenAIClient.Chat.Completions.ChatCompletion\n  > {\n    const clientOptions = this._getClientOptions(requestOptions);\n    const isParseableFormat =\n      request.response_format && request.response_format.type === \"json_schema\";\n    return this.caller.call(async () => {\n      try {\n        if (isParseableFormat && !request.stream) {\n          return await this.client.chat.completions.parse(\n            request,\n            clientOptions\n          );\n        } else {\n          return await this.client.chat.completions.create(\n            request,\n            clientOptions\n          );\n        }\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /**\n   * @deprecated\n   * This function was hoisted into a publicly accessible function from a\n   * different export, but to maintain backwards compatibility with chat models\n   * that depend on ChatOpenAICompletions, we'll keep it here as an overridable\n   * method. This will be removed in a future release\n   */\n  protected _convertCompletionsDeltaToBaseMessageChunk(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta: Record<string, any>,\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk,\n    defaultRole?: OpenAIClient.Chat.ChatCompletionRole\n  ): BaseMessageChunk {\n    return convertCompletionsDeltaToBaseMessageChunk({\n      delta,\n      rawResponse,\n      includeRawResponse: this.__includeRawResponse,\n      defaultRole,\n    });\n  }\n\n  /**\n   * @deprecated\n   * This function was hoisted into a publicly accessible function from a\n   * different export, but to maintain backwards compatibility with chat models\n   * that depend on ChatOpenAICompletions, we'll keep it here as an overridable\n   * method. This will be removed in a future release\n   */\n  protected _convertCompletionsMessageToBaseMessage(\n    message: OpenAIClient.ChatCompletionMessage,\n    rawResponse: OpenAIClient.ChatCompletion\n  ): BaseMessage {\n    return convertCompletionsMessageToBaseMessage({\n      message,\n      rawResponse,\n      includeRawResponse: this.__includeRawResponse,\n    });\n  }\n}\n"],"names":["options?: this[\"ParsedCallOptions\"]","extra?: { streaming?: boolean }","strict: boolean | undefined","params: Partial<ChatCompletionsInvocationParams>","messages: BaseMessage[]","options: this[\"ParsedCallOptions\"]","runManager?: CallbackManagerForLLMRun","messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[]","finalChunks: Record<number, ChatGenerationChunk>","generations: ChatGeneration[]","generation: ChatGeneration","defaultRole: OpenAIClient.Chat.ChatCompletionRole | undefined","usage: OpenAIClient.Completions.CompletionUsage | undefined","generationInfo: Record<string, any>","request: OpenAIClient.Chat.ChatCompletionCreateParams","requestOptions?: OpenAIClient.RequestOptions","delta: Record<string, any>","rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk","defaultRole?: OpenAIClient.Chat.ChatCompletionRole","message: OpenAIClient.ChatCompletionMessage","rawResponse: OpenAIClient.ChatCompletion"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;GA4CA,IAAa,wBAAb,cAEU,wLAAA,CAA4B;qBAE3B,iBACPA,OAAAA,EACAC,KAAAA,EACiC;QACjC,IAAIC;QACJ,IAAI,SAAS,WAAW,KAAA,GACtB,SAAS,QAAQ,MAAA;iBACR,IAAA,CAAK,yBAAA,KAA8B,KAAA,GAC5C,SAAS,IAAA,CAAK,yBAAA;QAGhB,IAAI,sBAAsB,CAAE;QAC5B,IAAI,SAAS,mBAAmB,KAAA,GAC9B,sBAAsB;YAAE,gBAAgB,QAAQ,cAAA;QAAgB;iBACvD,IAAA,CAAK,WAAA,IAAA,CAAgB,IAAA,CAAK,SAAA,IAAa,OAAO,SAAA,GACvD,sBAAsB;YAAE,gBAAgB;gBAAE,eAAe;YAAM;QAAE;QAGnE,MAAMC,SAAmD;YACvD,OAAO,IAAA,CAAK,KAAA;YACZ,aAAa,IAAA,CAAK,WAAA;YAClB,OAAO,IAAA,CAAK,IAAA;YACZ,mBAAmB,IAAA,CAAK,gBAAA;YACxB,kBAAkB,IAAA,CAAK,eAAA;YACvB,UAAU,IAAA,CAAK,QAAA;YACf,cAAc,IAAA,CAAK,WAAA;YACnB,GAAG,IAAA,CAAK,CAAA;YACR,YAAY,IAAA,CAAK,SAAA;YACjB,MAAM,SAAS,QAAQ,IAAA,CAAK,aAAA;YAC5B,MAAM,IAAA,CAAK,IAAA;YAEX,QAAQ,IAAA,CAAK,SAAA;YACb,WAAW,SAAS;YACpB,eAAe,SAAS;YACxB,OAAO,SAAS,OAAO,SACnB,QAAQ,KAAA,CAAM,GAAA,CAAI,CAAC,OACjB,IAAA,CAAK,uCAAA,CAAwC,MAAM;oBAAE;gBAAQ,EAAC,CAC/D,GACD,KAAA;YACJ,iBAAa,6LAAA,EACX,SAAS,YACV;YACD,iBAAiB,IAAA,CAAK,kBAAA,CAAmB,SAAS,gBAAgB;YAClE,MAAM,SAAS;YACf,GAAG,mBAAA;YACH,qBAAqB,SAAS;YAC9B,GAAI,IAAA,CAAK,KAAA,IAAS,SAAS,QACvB;gBAAE,OAAO,IAAA,CAAK,KAAA,IAAS,SAAS;YAAO,IACvC,CAAE,CAAA;YACN,GAAI,IAAA,CAAK,UAAA,IAAc,SAAS,aAC5B;gBAAE,YAAY,IAAA,CAAK,UAAA,IAAc,SAAS;YAAY,IACtD,CAAE,CAAA;YACN,GAAG,IAAA,CAAK,WAAA;YACR,kBAAkB,SAAS,kBAAkB,IAAA,CAAK,cAAA;YAClD,wBACE,SAAS,wBAAwB,IAAA,CAAK,oBAAA;YACxC,WAAW,SAAS,aAAa,IAAA,CAAK,SAAA;QACvC;QACD,IAAI,SAAS,eAAe,KAAA,GAC1B,OAAO,UAAA,GAAa,QAAQ,UAAA;QAE9B,IAAI,IAAA,CAAK,YAAA,KAAiB,KAAA,GACxB,OAAO,YAAA,GAAe,IAAA,CAAK,YAAA;QAE7B,IAAI,SAAS,iBAAiB,KAAA,GAC5B,OAAO,YAAA,GAAe,QAAQ,YAAA;QAEhC,MAAM,YAAY,IAAA,CAAK,mBAAA,CAAoB,QAAQ;QACnD,IAAI,cAAc,KAAA,KAAa,UAAU,MAAA,KAAW,KAAA,GAClD,OAAO,gBAAA,GAAmB,UAAU,MAAA;QAEtC,QAAI,oLAAA,EAAiB,OAAO,KAAA,CAAM,EAChC,OAAO,qBAAA,GACL,IAAA,CAAK,SAAA,KAAc,CAAA,IAAK,KAAA,IAAY,IAAA,CAAK,SAAA;aAE3C,OAAO,UAAA,GAAa,IAAA,CAAK,SAAA,KAAc,CAAA,IAAK,KAAA,IAAY,IAAA,CAAK,SAAA;QAG/D,OAAO;IACR;IAED,MAAM,UACJC,QAAAA,EACAC,OAAAA,EACAC,UAAAA,EACqB;QACrB,MAAM,gBAAgB,CAAE;QACxB,MAAM,SAAS,IAAA,CAAK,gBAAA,CAAiB,QAAQ;QAC7C,MAAMC,qBACJ,yNAAA,EAA0C;YACxC;YACA,OAAO,IAAA,CAAK,KAAA;QACb,EAAC;QAEJ,IAAI,OAAO,MAAA,EAAQ;YACjB,MAAM,SAAS,IAAA,CAAK,qBAAA,CAAsB,UAAU,SAAS,WAAW;YACxE,MAAMC,cAAmD,CAAE;YAC3D,WAAW,MAAM,SAAS,OAAQ;gBAChC,MAAM,OAAA,CAAQ,iBAAA,GAAoB;oBAChC,GAAG,MAAM,cAAA;oBACT,GAAG,MAAM,OAAA,CAAQ,iBAAA;gBAClB;gBACD,MAAM,QACH,MAAM,cAAA,EAAoC,cAAc;gBAC3D,IAAI,WAAA,CAAY,MAAA,KAAW,KAAA,GACzB,WAAA,CAAY,MAAA,GAAS;qBAErB,WAAA,CAAY,MAAA,GAAS,WAAA,CAAY,MAAA,CAAO,MAAA,CAAO,MAAM;YAExD;YACD,MAAM,cAAc,OAAO,OAAA,CAAQ,YAAY,CAC5C,IAAA,CAAK,CAAC,CAAC,KAAK,EAAE,CAAC,KAAK,GAAK,SAAS,MAAM,GAAG,GAAG,SAAS,MAAM,GAAG,CAAC,CACjE,GAAA,CAAI,CAAC,CAAC,GAAG,MAAM,GAAK,MAAM;YAE7B,MAAM,EAAE,SAAA,EAAW,aAAA,EAAe,GAAG,IAAA,CAAK,gBAAA,CAAiB,QAAQ;YAKnE,MAAM,mBAAmB,MAAM,IAAA,CAAK,iCAAA,CAClC,UACA,WACA,cACD;YACD,MAAM,uBAAuB,MAAM,IAAA,CAAK,4BAAA,CACtC,YACD;YAED,cAAc,YAAA,GAAe;YAC7B,cAAc,aAAA,GAAgB;YAC9B,cAAc,YAAA,GAAe,mBAAmB;YAChD,OAAO;gBACL;gBACA,WAAW;oBACT,qBAAqB;wBACnB,cAAc,cAAc,YAAA;wBAC5B,kBAAkB,cAAc,aAAA;wBAChC,aAAa,cAAc,YAAA;oBAC5B;gBACF;YACF;QACF,OAAM;YACL,MAAM,OAAO,MAAM,IAAA,CAAK,mBAAA,CACtB;gBACE,GAAG,MAAA;gBACH,QAAQ;gBACR,UAAU;YACX,GACD;gBACE,QAAQ,SAAS;gBACjB,GAAG,SAAS,OAAA;YACb,EACF;YAED,MAAM,EACJ,mBAAmB,gBAAA,EACnB,eAAe,YAAA,EACf,cAAc,WAAA,EACd,uBAAuB,mBAAA,EACvB,2BAA2B,uBAAA,EAC5B,GAAG,MAAM,SAAS,CAAE;YAErB,IAAI,kBACF,cAAc,aAAA,GAAA,CACX,cAAc,aAAA,IAAiB,CAAA,IAAK;YAGzC,IAAI,cACF,cAAc,YAAA,GAAA,CACX,cAAc,YAAA,IAAgB,CAAA,IAAK;YAGxC,IAAI,aACF,cAAc,YAAA,GAAA,CACX,cAAc,YAAA,IAAgB,CAAA,IAAK;YAGxC,IACE,qBAAqB,iBAAiB,QACtC,qBAAqB,kBAAkB,MAEvC,cAAc,mBAAA,GAAsB;gBAClC,GAAI,qBAAqB,iBAAiB,QAAQ;oBAChD,OAAO,qBAAqB;gBAC7B,CAAA;gBACD,GAAI,qBAAqB,kBAAkB,QAAQ;oBACjD,YAAY,qBAAqB;gBAClC,CAAA;YACF;YAGH,IACE,yBAAyB,iBAAiB,QAC1C,yBAAyB,qBAAqB,MAE9C,cAAc,oBAAA,GAAuB;gBACnC,GAAI,yBAAyB,iBAAiB,QAAQ;oBACpD,OAAO,yBAAyB;gBACjC,CAAA;gBACD,GAAI,yBAAyB,qBAAqB,QAAQ;oBACxD,WAAW,yBAAyB;gBACrC,CAAA;YACF;YAGH,MAAMC,cAAgC,CAAE,CAAA;YACxC,KAAK,MAAM,QAAQ,MAAM,WAAW,CAAE,CAAA,CAAE;gBACtC,MAAM,OAAO,KAAK,OAAA,EAAS,WAAW;gBACtC,MAAMC,aAA6B;oBACjC;oBACA,SAAS,IAAA,CAAK,uCAAA,CACZ,KAAK,OAAA,IAAW;wBAAE,MAAM;oBAAa,GACrC,KACD;gBACF;gBACD,WAAW,cAAA,GAAiB;oBAC1B,GAAI,KAAK,aAAA,GAAgB;wBAAE,eAAe,KAAK,aAAA;oBAAe,IAAG,CAAE,CAAA;oBACnE,GAAI,KAAK,QAAA,GAAW;wBAAE,UAAU,KAAK,QAAA;oBAAU,IAAG,CAAE,CAAA;gBACrD;gBACD,QAAI,8KAAA,EAAY,WAAW,OAAA,CAAQ,EACjC,WAAW,OAAA,CAAQ,cAAA,GAAiB;gBAItC,WAAW,OAAA,GAAU,IAAI,4KAAA,CACvB,OAAO,WAAA,CACL,OAAO,OAAA,CAAQ,WAAW,OAAA,CAAQ,CAAC,MAAA,CACjC,CAAC,CAAC,IAAI,GAAK,CAAC,IAAI,UAAA,CAAW,MAAM,CAClC,CACF;gBAEH,YAAY,IAAA,CAAK,WAAW;YAC7B;YACD,OAAO;gBACL;gBACA,WAAW;oBACT,YAAY;wBACV,cAAc,cAAc,YAAA;wBAC5B,kBAAkB,cAAc,aAAA;wBAChC,aAAa,cAAc,YAAA;oBAC5B;gBACF;YACF;QACF;IACF;IAED,OAAO,sBACLN,QAAAA,EACAC,OAAAA,EACAC,UAAAA,EACqC;QACrC,MAAMC,qBACJ,yNAAA,EAA0C;YACxC;YACA,OAAO,IAAA,CAAK,KAAA;QACb,EAAC;QAEJ,MAAM,SAAS;YACb,GAAG,IAAA,CAAK,gBAAA,CAAiB,SAAS;gBAChC,WAAW;YACZ,EAAC;YACF,UAAU;YACV,QAAQ;QACT;QACD,IAAII;QAEJ,MAAM,iBAAiB,MAAM,IAAA,CAAK,mBAAA,CAAoB,QAAQ,QAAQ;QACtE,IAAIC;QACJ,WAAW,MAAM,QAAQ,eAAgB;YACvC,MAAM,SAAS,MAAM,SAAA,CAAU,EAAA;YAC/B,IAAI,KAAK,KAAA,EACP,QAAQ,KAAK,KAAA;YAEf,IAAI,CAAC,OACH,CAAA;YAGF,MAAM,EAAE,KAAA,EAAO,GAAG;YAClB,IAAI,CAAC,MACH,CAAA;YAEF,MAAM,QAAQ,IAAA,CAAK,0CAAA,CACjB,OACA,MACA,YACD;YACD,cAAc,MAAM,IAAA,IAAQ;YAC5B,MAAM,kBAAkB;gBACtB,QAAQ,QAAQ,WAAA,IAAe;gBAC/B,YAAY,OAAO,KAAA,IAAS;YAC7B;YACD,IAAI,OAAO,MAAM,OAAA,KAAY,UAAU;gBACrC,QAAQ,GAAA,CACN,uFACD;gBACD;YACD;YAED,MAAMC,iBAAsC;gBAAE,GAAG,eAAA;YAAiB;YAClE,IAAI,OAAO,aAAA,IAAiB,MAAM;gBAChC,eAAe,aAAA,GAAgB,OAAO,aAAA;gBAGtC,eAAe,kBAAA,GAAqB,KAAK,kBAAA;gBACzC,eAAe,UAAA,GAAa,KAAK,KAAA;gBACjC,eAAe,YAAA,GAAe,KAAK,YAAA;YACpC;YACD,IAAI,IAAA,CAAK,QAAA,EACP,eAAe,QAAA,GAAW,OAAO,QAAA;YAEnC,MAAM,kBAAkB,IAAI,+KAAA,CAAoB;gBAC9C,SAAS;gBACT,MAAM,MAAM,OAAA;gBACZ;YACD;YACD,MAAM;YACN,MAAM,YAAY,kBAChB,gBAAgB,IAAA,IAAQ,IACxB,iBACA,KAAA,GACA,KAAA,GACA,KAAA,GACA;gBAAE,OAAO;YAAiB,EAC3B;QACF;QACD,IAAI,OAAO;YACT,MAAM,oBAAoB;gBACxB,GAAI,MAAM,qBAAA,EAAuB,iBAAiB,QAAQ;oBACxD,OAAO,MAAM,qBAAA,EAAuB;gBACrC,CAAA;gBACD,GAAI,MAAM,qBAAA,EAAuB,kBAAkB,QAAQ;oBACzD,YAAY,MAAM,qBAAA,EAAuB;gBAC1C,CAAA;YACF;YACD,MAAM,qBAAqB;gBACzB,GAAI,MAAM,yBAAA,EAA2B,iBAAiB,QAAQ;oBAC5D,OAAO,MAAM,yBAAA,EAA2B;gBACzC,CAAA;gBACD,GAAI,MAAM,yBAAA,EAA2B,qBAAqB,QAAQ;oBAChE,WAAW,MAAM,yBAAA,EAA2B;gBAC7C,CAAA;YACF;YACD,MAAM,kBAAkB,IAAI,+KAAA,CAAoB;gBAC9C,SAAS,IAAI,iLAAA,CAAe;oBAC1B,SAAS;oBACT,mBAAmB;wBACjB,OAAO;4BAAE,GAAG,KAAA;wBAAO;oBACpB;oBACD,gBAAgB;wBACd,cAAc,MAAM,aAAA;wBACpB,eAAe,MAAM,iBAAA;wBACrB,cAAc,MAAM,YAAA;wBACpB,GAAI,OAAO,IAAA,CAAK,kBAAkB,CAAC,MAAA,GAAS,KAAK;4BAC/C,qBAAqB;wBACtB,CAAA;wBACD,GAAI,OAAO,IAAA,CAAK,mBAAmB,CAAC,MAAA,GAAS,KAAK;4BAChD,sBAAsB;wBACvB,CAAA;oBACF;gBACF;gBACD,MAAM;YACP;YACD,MAAM;QACP;QACD,IAAI,QAAQ,MAAA,EAAQ,QAClB,CAAA,MAAM,IAAI,MAAM;IAEnB;IAYD,MAAM,oBACJC,OAAAA,EACAC,cAAAA,EAIA;QACA,MAAM,gBAAgB,IAAA,CAAK,iBAAA,CAAkB,eAAe;QAC5D,MAAM,oBACJ,QAAQ,eAAA,IAAmB,QAAQ,eAAA,CAAgB,IAAA,KAAS;QAC9D,OAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,YAAY;YAClC,IAAI;gBACF,IAAI,qBAAqB,CAAC,QAAQ,MAAA,CAChC,CAAA,OAAO,MAAM,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,WAAA,CAAY,KAAA,CACxC,SACA,cACD;qBAED,OAAO,MAAM,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,WAAA,CAAY,MAAA,CACxC,SACA,cACD;YAEJ,EAAA,OAAQ,GAAG;gBACV,MAAM,YAAQ,2LAAA,EAAsB,EAAE;gBACtC,MAAM;YACP;QACF,EAAC;IACH;;;;;;;IASS,2CAERC,KAAAA,EACAC,WAAAA,EACAC,WAAAA,EACkB;QAClB,WAAO,yNAAA,EAA0C;YAC/C;YACA;YACA,oBAAoB,IAAA,CAAK,oBAAA;YACzB;QACD,EAAC;IACH;;;;;;;IASS,wCACRC,OAAAA,EACAC,WAAAA,EACa;QACb,WAAO,sNAAA,EAAuC;YAC5C;YACA;YACA,oBAAoB,IAAA,CAAK,oBAAA;QAC1B,EAAC;IACH;AACF"}},
    {"offset": {"line": 2812, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/azure/chat_models/common.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/azure/chat_models/common.ts"],"sourcesContent":["import { AzureOpenAI as AzureOpenAIClient, type ClientOptions } from \"openai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport type { Serialized } from \"@langchain/core/load/serializable\";\nimport { ChatOpenAICallOptions } from \"../../chat_models/index.js\";\nimport {\n  OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"../../utils/azure.js\";\nimport { AzureOpenAIChatInput, OpenAICoreRequestOptions } from \"../../types.js\";\nimport {\n  BaseChatOpenAI,\n  BaseChatOpenAIFields,\n} from \"../../chat_models/base.js\";\n\nexport const AZURE_ALIASES = {\n  openAIApiKey: \"openai_api_key\",\n  openAIApiVersion: \"openai_api_version\",\n  openAIBasePath: \"openai_api_base\",\n  deploymentName: \"deployment_name\",\n  azureOpenAIEndpoint: \"azure_endpoint\",\n  azureOpenAIApiVersion: \"openai_api_version\",\n  azureOpenAIBasePath: \"openai_api_base\",\n  azureOpenAIApiDeploymentName: \"deployment_name\",\n};\n\nexport const AZURE_SECRETS = {\n  azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n};\n\nexport const AZURE_SERIALIZABLE_KEYS = [\n  \"azureOpenAIApiKey\",\n  \"azureOpenAIApiVersion\",\n  \"azureOpenAIBasePath\",\n  \"azureOpenAIEndpoint\",\n  \"azureOpenAIApiInstanceName\",\n  \"azureOpenAIApiDeploymentName\",\n  \"deploymentName\",\n  \"openAIApiKey\",\n  \"openAIApiVersion\",\n];\n\nexport interface AzureChatOpenAIFields\n  extends BaseChatOpenAIFields,\n    Partial<AzureOpenAIChatInput> {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi?: boolean;\n}\n\nexport function _constructAzureFields(\n  this: Partial<AzureOpenAIChatInput>,\n  fields?: AzureChatOpenAIFields\n) {\n  this.azureOpenAIApiKey =\n    fields?.azureOpenAIApiKey ??\n    (typeof fields?.openAIApiKey === \"string\"\n      ? fields?.openAIApiKey\n      : undefined) ??\n    (typeof fields?.apiKey === \"string\" ? fields?.apiKey : undefined) ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n\n  this.azureOpenAIApiInstanceName =\n    fields?.azureOpenAIApiInstanceName ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n\n  this.azureOpenAIApiDeploymentName =\n    fields?.azureOpenAIApiDeploymentName ??\n    fields?.deploymentName ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\");\n\n  this.azureOpenAIApiVersion =\n    fields?.azureOpenAIApiVersion ??\n    fields?.openAIApiVersion ??\n    getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n\n  this.azureOpenAIBasePath =\n    fields?.azureOpenAIBasePath ??\n    getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n\n  this.azureOpenAIEndpoint =\n    fields?.azureOpenAIEndpoint ??\n    getEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\");\n\n  this.azureADTokenProvider = fields?.azureADTokenProvider;\n\n  if (!this.azureOpenAIApiKey && !this.apiKey && !this.azureADTokenProvider) {\n    throw new Error(\"Azure OpenAI API key or Token Provider not found\");\n  }\n}\n\nexport function _getAzureClientOptions(\n  this: BaseChatOpenAI<ChatOpenAICallOptions> & Partial<AzureOpenAIChatInput>,\n  options: OpenAICoreRequestOptions | undefined\n): OpenAICoreRequestOptions {\n  if (!this.client) {\n    const openAIEndpointConfig: OpenAIEndpointConfig = {\n      azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n      azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n      azureOpenAIApiKey: this.azureOpenAIApiKey,\n      azureOpenAIBasePath: this.azureOpenAIBasePath,\n      azureADTokenProvider: this.azureADTokenProvider,\n      baseURL: this.clientConfig.baseURL,\n      azureOpenAIEndpoint: this.azureOpenAIEndpoint,\n    };\n\n    const endpoint = getEndpoint(openAIEndpointConfig);\n\n    const { apiKey: existingApiKey, ...clientConfigRest } = this.clientConfig;\n    const params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string } = {\n      ...clientConfigRest,\n      baseURL: endpoint,\n      timeout: this.timeout,\n      maxRetries: 0,\n    };\n\n    if (!this.azureADTokenProvider) {\n      params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;\n    }\n\n    if (!params.baseURL) {\n      delete params.baseURL;\n    }\n\n    params.defaultHeaders = getHeadersWithUserAgent(\n      params.defaultHeaders,\n      true,\n      \"2.0.0\"\n    );\n\n    this.client = new AzureOpenAIClient({\n      apiVersion: this.azureOpenAIApiVersion,\n      azureADTokenProvider: this.azureADTokenProvider,\n      deployment: this.azureOpenAIApiDeploymentName,\n      ...params,\n    });\n  }\n  const requestOptions = {\n    ...this.clientConfig,\n    ...options,\n  } as OpenAICoreRequestOptions;\n  if (this.azureOpenAIApiKey) {\n    requestOptions.headers = {\n      \"api-key\": this.azureOpenAIApiKey,\n      ...requestOptions.headers,\n    };\n    requestOptions.query = {\n      \"api-version\": this.azureOpenAIApiVersion,\n      ...requestOptions.query,\n    };\n  }\n  return requestOptions;\n}\n\nexport function _serializeAzureChat(\n  this: BaseChatOpenAI<ChatOpenAICallOptions> & Partial<AzureOpenAIChatInput>,\n  input: Serialized\n) {\n  const json = input;\n\n  function isRecord(obj: unknown): obj is Record<string, unknown> {\n    return typeof obj === \"object\" && obj != null;\n  }\n\n  if (isRecord(json) && isRecord(json.kwargs)) {\n    delete json.kwargs.azure_openai_base_path;\n    delete json.kwargs.azure_openai_api_deployment_name;\n    delete json.kwargs.azure_openai_api_key;\n    delete json.kwargs.azure_openai_api_version;\n    delete json.kwargs.azure_open_ai_base_path;\n\n    if (!json.kwargs.azure_endpoint && this.azureOpenAIEndpoint) {\n      json.kwargs.azure_endpoint = this.azureOpenAIEndpoint;\n    }\n    if (!json.kwargs.azure_endpoint && this.azureOpenAIBasePath) {\n      const parts = this.azureOpenAIBasePath.split(\"/openai/deployments/\");\n      if (parts.length === 2 && parts[0].startsWith(\"http\")) {\n        const [endpoint] = parts;\n        json.kwargs.azure_endpoint = endpoint;\n      }\n    }\n    if (!json.kwargs.azure_endpoint && this.azureOpenAIApiInstanceName) {\n      json.kwargs.azure_endpoint = `https://${this.azureOpenAIApiInstanceName}.openai.azure.com/`;\n    }\n    if (!json.kwargs.deployment_name && this.azureOpenAIApiDeploymentName) {\n      json.kwargs.deployment_name = this.azureOpenAIApiDeploymentName;\n    }\n    if (!json.kwargs.deployment_name && this.azureOpenAIBasePath) {\n      const parts = this.azureOpenAIBasePath.split(\"/openai/deployments/\");\n      if (parts.length === 2) {\n        const [, deployment] = parts;\n        json.kwargs.deployment_name = deployment;\n      }\n    }\n\n    if (\n      json.kwargs.azure_endpoint &&\n      json.kwargs.deployment_name &&\n      json.kwargs.openai_api_base\n    ) {\n      delete json.kwargs.openai_api_base;\n    }\n    if (\n      json.kwargs.azure_openai_api_instance_name &&\n      json.kwargs.azure_endpoint\n    ) {\n      delete json.kwargs.azure_openai_api_instance_name;\n    }\n  }\n\n  return json;\n}\n"],"names":["fields?: AzureChatOpenAIFields","options: OpenAICoreRequestOptions | undefined","openAIEndpointConfig: OpenAIEndpointConfig","params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string }","AzureOpenAIClient","input: Serialized","obj: unknown"],"mappings":";;;;;;;;;;;;;;;;;;;;;;AAeA,MAAa,gBAAgB;IAC3B,cAAc;IACd,kBAAkB;IAClB,gBAAgB;IAChB,gBAAgB;IAChB,qBAAqB;IACrB,uBAAuB;IACvB,qBAAqB;IACrB,8BAA8B;AAC/B;AAED,MAAa,gBAAgB;IAC3B,mBAAmB;AACpB;AAED,MAAa,0BAA0B;IACrC;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;CACD;AAYD,SAAgB,sBAEdA,MAAAA,EACA;IACA,IAAA,CAAK,iBAAA,GACH,QAAQ,qBAAA,CACP,OAAO,QAAQ,iBAAiB,WAC7B,QAAQ,eACR,KAAA,CAAA,KAAA,CACH,OAAO,QAAQ,WAAW,WAAW,QAAQ,SAAS,KAAA,CAAA,SACvD,uLAAA,EAAuB,uBAAuB;IAEhD,IAAA,CAAK,0BAAA,GACH,QAAQ,kCACR,uLAAA,EAAuB,iCAAiC;IAE1D,IAAA,CAAK,4BAAA,GACH,QAAQ,gCACR,QAAQ,sBACR,uLAAA,EAAuB,mCAAmC;IAE5D,IAAA,CAAK,qBAAA,GACH,QAAQ,yBACR,QAAQ,wBACR,uLAAA,EAAuB,2BAA2B;IAEpD,IAAA,CAAK,mBAAA,GACH,QAAQ,2BACR,uLAAA,EAAuB,yBAAyB;IAElD,IAAA,CAAK,mBAAA,GACH,QAAQ,2BACR,uLAAA,EAAuB,wBAAwB;IAEjD,IAAA,CAAK,oBAAA,GAAuB,QAAQ;IAEpC,IAAI,CAAC,IAAA,CAAK,iBAAA,IAAqB,CAAC,IAAA,CAAK,MAAA,IAAU,CAAC,IAAA,CAAK,oBAAA,CACnD,CAAA,MAAM,IAAI,MAAM;AAEnB;AAED,SAAgB,uBAEdC,OAAAA,EAC0B;IAC1B,IAAI,CAAC,IAAA,CAAK,MAAA,EAAQ;QAChB,MAAMC,uBAA6C;YACjD,8BAA8B,IAAA,CAAK,4BAAA;YACnC,4BAA4B,IAAA,CAAK,0BAAA;YACjC,mBAAmB,IAAA,CAAK,iBAAA;YACxB,qBAAqB,IAAA,CAAK,mBAAA;YAC1B,sBAAsB,IAAA,CAAK,oBAAA;YAC3B,SAAS,IAAA,CAAK,YAAA,CAAa,OAAA;YAC3B,qBAAqB,IAAA,CAAK,mBAAA;QAC3B;QAED,MAAM,eAAW,gLAAA,EAAY,qBAAqB;QAElD,MAAM,EAAE,QAAQ,cAAA,EAAgB,GAAG,kBAAkB,GAAG,IAAA,CAAK,YAAA;QAC7D,MAAMC,SAA8D;YAClE,GAAG,gBAAA;YACH,SAAS;YACT,SAAS,IAAA,CAAK,OAAA;YACd,YAAY;QACb;QAED,IAAI,CAAC,IAAA,CAAK,oBAAA,EACR,OAAO,MAAA,GAAS,qBAAqB,iBAAA;QAGvC,IAAI,CAAC,OAAO,OAAA,EACV,OAAO,OAAO,OAAA;QAGhB,OAAO,cAAA,OAAiB,4LAAA,EACtB,OAAO,cAAA,EACP,MACA,QACD;QAED,IAAA,CAAK,MAAA,GAAS,IAAIC,iJAAAA,CAAkB;YAClC,YAAY,IAAA,CAAK,qBAAA;YACjB,sBAAsB,IAAA,CAAK,oBAAA;YAC3B,YAAY,IAAA,CAAK,4BAAA;YACjB,GAAG,MAAA;QACJ;IACF;IACD,MAAM,iBAAiB;QACrB,GAAG,IAAA,CAAK,YAAA;QACR,GAAG,OAAA;IACJ;IACD,IAAI,IAAA,CAAK,iBAAA,EAAmB;QAC1B,eAAe,OAAA,GAAU;YACvB,WAAW,IAAA,CAAK,iBAAA;YAChB,GAAG,eAAe,OAAA;QACnB;QACD,eAAe,KAAA,GAAQ;YACrB,eAAe,IAAA,CAAK,qBAAA;YACpB,GAAG,eAAe,KAAA;QACnB;IACF;IACD,OAAO;AACR;AAED,SAAgB,oBAEdC,KAAAA,EACA;IACA,MAAM,OAAO;IAEb,SAAS,SAASC,GAAAA,EAA8C;QAC9D,OAAO,OAAO,QAAQ,YAAY,OAAO;IAC1C;IAED,IAAI,SAAS,KAAK,IAAI,SAAS,KAAK,MAAA,CAAO,EAAE;QAC3C,OAAO,KAAK,MAAA,CAAO,sBAAA;QACnB,OAAO,KAAK,MAAA,CAAO,gCAAA;QACnB,OAAO,KAAK,MAAA,CAAO,oBAAA;QACnB,OAAO,KAAK,MAAA,CAAO,wBAAA;QACnB,OAAO,KAAK,MAAA,CAAO,uBAAA;QAEnB,IAAI,CAAC,KAAK,MAAA,CAAO,cAAA,IAAkB,IAAA,CAAK,mBAAA,EACtC,KAAK,MAAA,CAAO,cAAA,GAAiB,IAAA,CAAK,mBAAA;QAEpC,IAAI,CAAC,KAAK,MAAA,CAAO,cAAA,IAAkB,IAAA,CAAK,mBAAA,EAAqB;YAC3D,MAAM,QAAQ,IAAA,CAAK,mBAAA,CAAoB,KAAA,CAAM,uBAAuB;YACpE,IAAI,MAAM,MAAA,KAAW,KAAK,KAAA,CAAM,EAAA,CAAG,UAAA,CAAW,OAAO,EAAE;gBACrD,MAAM,CAAC,SAAS,GAAG;gBACnB,KAAK,MAAA,CAAO,cAAA,GAAiB;YAC9B;QACF;QACD,IAAI,CAAC,KAAK,MAAA,CAAO,cAAA,IAAkB,IAAA,CAAK,0BAAA,EACtC,KAAK,MAAA,CAAO,cAAA,GAAiB,CAAC,QAAQ,EAAE,IAAA,CAAK,0BAAA,CAA2B,kBAAkB,CAAC;QAE7F,IAAI,CAAC,KAAK,MAAA,CAAO,eAAA,IAAmB,IAAA,CAAK,4BAAA,EACvC,KAAK,MAAA,CAAO,eAAA,GAAkB,IAAA,CAAK,4BAAA;QAErC,IAAI,CAAC,KAAK,MAAA,CAAO,eAAA,IAAmB,IAAA,CAAK,mBAAA,EAAqB;YAC5D,MAAM,QAAQ,IAAA,CAAK,mBAAA,CAAoB,KAAA,CAAM,uBAAuB;YACpE,IAAI,MAAM,MAAA,KAAW,GAAG;gBACtB,MAAM,GAAG,WAAW,GAAG;gBACvB,KAAK,MAAA,CAAO,eAAA,GAAkB;YAC/B;QACF;QAED,IACE,KAAK,MAAA,CAAO,cAAA,IACZ,KAAK,MAAA,CAAO,eAAA,IACZ,KAAK,MAAA,CAAO,eAAA,EAEZ,OAAO,KAAK,MAAA,CAAO,eAAA;QAErB,IACE,KAAK,MAAA,CAAO,8BAAA,IACZ,KAAK,MAAA,CAAO,cAAA,EAEZ,OAAO,KAAK,MAAA,CAAO,8BAAA;IAEtB;IAED,OAAO;AACR"}},
    {"offset": {"line": 2952, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/azure/chat_models/completions.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/azure/chat_models/completions.ts"],"sourcesContent":["import { LangSmithParams } from \"@langchain/core/language_models/chat_models\";\nimport { Serialized } from \"@langchain/core/load/serializable\";\nimport {\n  ChatOpenAICompletions,\n  ChatOpenAICompletionsCallOptions,\n} from \"../../chat_models/completions.js\";\nimport { AzureOpenAIChatInput, OpenAICoreRequestOptions } from \"../../types.js\";\nimport {\n  _constructAzureFields,\n  _getAzureClientOptions,\n  _serializeAzureChat,\n  AZURE_ALIASES,\n  AZURE_SECRETS,\n  AZURE_SERIALIZABLE_KEYS,\n  AzureChatOpenAIFields,\n} from \"./common.js\";\n\nexport class AzureChatOpenAICompletions<\n    CallOptions extends ChatOpenAICompletionsCallOptions = ChatOpenAICompletionsCallOptions\n  >\n  extends ChatOpenAICompletions<CallOptions>\n  implements Partial<AzureOpenAIChatInput>\n{\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  _llmType(): string {\n    return \"azure_openai\";\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      ...AZURE_ALIASES,\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      ...AZURE_SECRETS,\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, ...AZURE_SERIALIZABLE_KEYS];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = super.getLsParams(options);\n    params.ls_provider = \"azure\";\n    return params;\n  }\n\n  constructor(fields?: AzureChatOpenAIFields) {\n    super(fields);\n    _constructAzureFields.call(this, fields);\n  }\n\n  override _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    return _getAzureClientOptions.call(this, options);\n  }\n\n  override toJSON(): Serialized {\n    return _serializeAzureChat.call(this, super.toJSON());\n  }\n}\n"],"names":["options: this[\"ParsedCallOptions\"]","fields?: AzureChatOpenAIFields","options: OpenAICoreRequestOptions | undefined"],"mappings":";;;;;;;;;AAiBA,IAAa,6BAAb,cAGU,sMAAA,CAEV;IACE,sBAAA;IAEA,kBAAA;IAEA,qBAAA;IAEA,2BAAA;IAEA,6BAAA;IAEA,oBAAA;IAEA,oBAAA;IAEA,WAAmB;QACjB,OAAO;IACR;IAED,IAAI,aAAqC;QACvC,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,GAAG,kMAAA;QACJ;IACF;IAED,IAAI,aAAoD;QACtD,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,GAAG,kMAAA;QACJ;IACF;IAED,IAAI,uBAAiC;QACnC,OAAO,CAAC;eAAG,KAAA,CAAM,sBAAsB;eAAG,4MAAwB;SAAA;IACnE;IAED,YAAYA,OAAAA,EAAqD;QAC/D,MAAM,SAAS,KAAA,CAAM,YAAY,QAAQ;QACzC,OAAO,WAAA,GAAc;QACrB,OAAO;IACR;IAED,YAAYC,MAAAA,CAAgC;QAC1C,KAAA,CAAM,OAAO;QACb,0MAAA,CAAsB,IAAA,CAAK,IAAA,EAAM,OAAO;IACzC;IAEQ,kBACPC,OAAAA,EAC0B;QAC1B,OAAO,2MAAA,CAAuB,IAAA,CAAK,IAAA,EAAM,QAAQ;IAClD;IAEQ,SAAqB;QAC5B,OAAO,wMAAA,CAAoB,IAAA,CAAK,IAAA,EAAM,KAAA,CAAM,QAAQ,CAAC;IACtD;AACF"}},
    {"offset": {"line": 3012, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/converters/responses.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/converters/responses.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport {\n  AIMessage,\n  AIMessageChunk,\n  type BaseMessage,\n  type UsageMetadata,\n  type BaseMessageFields,\n  type MessageContent,\n  type InvalidToolCall,\n  MessageContentImageUrl,\n  isDataContentBlock,\n  convertToProviderContentBlock,\n  ContentBlock,\n} from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport {\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport type {\n  ToolCall,\n  ToolCallChunk,\n  ToolMessage,\n} from \"@langchain/core/messages/tool\";\nimport { ResponseInputMessageContentList } from \"openai/resources/responses/responses.js\";\nimport { ChatOpenAIReasoningSummary } from \"../types.js\";\nimport {\n  isComputerToolCall,\n  isCustomToolCall,\n  parseComputerCall,\n  parseCustomToolCall,\n} from \"../utils/tools.js\";\nimport {\n  getRequiredFilenameFromMetadata,\n  iife,\n  isReasoningModel,\n  messageToOpenAIRole,\n} from \"../utils/misc.js\";\nimport { Converter } from \"@langchain/core/utils/format\";\nimport { completionsApiContentBlockConverter } from \"./completions.js\";\n\nconst _FUNCTION_CALL_IDS_MAP_KEY = \"__openai_function_call_ids__\";\n\ntype ExcludeController<T> = T extends { controller: unknown } ? never : T;\n\nexport type ResponsesCreate = OpenAIClient.Responses[\"create\"];\nexport type ResponsesParse = OpenAIClient.Responses[\"parse\"];\n\nexport type ResponsesCreateInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesCreate>>\n>;\nexport type ResponsesParseInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesParse>>\n>;\n\nexport type ResponsesInputItem = OpenAIClient.Responses.ResponseInputItem;\n\n/**\n * Converts OpenAI Responses API usage statistics to LangChain's UsageMetadata format.\n *\n * This converter transforms token usage information from OpenAI's Responses API into\n * the standardized UsageMetadata format used throughout LangChain. It handles both\n * basic token counts and detailed token breakdowns including cached tokens and\n * reasoning tokens.\n *\n * @param usage - The usage statistics object from OpenAI's Responses API containing\n *                token counts and optional detailed breakdowns.\n *\n * @returns A UsageMetadata object containing:\n *   - `input_tokens`: Total number of tokens in the input/prompt (defaults to 0 if not provided)\n *   - `output_tokens`: Total number of tokens in the model's output (defaults to 0 if not provided)\n *   - `total_tokens`: Combined total of input and output tokens (defaults to 0 if not provided)\n *   - `input_token_details`: Object containing detailed input token information:\n *     - `cache_read`: Number of tokens read from cache (only included if available)\n *   - `output_token_details`: Object containing detailed output token information:\n *     - `reasoning`: Number of tokens used for reasoning (only included if available)\n *\n * @example\n * ```typescript\n * const usage = {\n *   input_tokens: 100,\n *   output_tokens: 50,\n *   total_tokens: 150,\n *   input_tokens_details: { cached_tokens: 20 },\n *   output_tokens_details: { reasoning_tokens: 10 }\n * };\n *\n * const metadata = convertResponsesUsageToUsageMetadata(usage);\n * // Returns:\n * // {\n * //   input_tokens: 100,\n * //   output_tokens: 50,\n * //   total_tokens: 150,\n * //   input_token_details: { cache_read: 20 },\n * //   output_token_details: { reasoning: 10 }\n * // }\n * ```\n *\n * @remarks\n * - The function safely handles undefined or null values by using optional chaining\n *   and nullish coalescing operators\n * - Detailed token information (cache_read, reasoning) is only included in the result\n *   if the corresponding values are present in the input\n * - Token counts default to 0 if not provided in the usage object\n * - This converter is specifically designed for OpenAI's Responses API format and\n *   may differ from other OpenAI API endpoints\n */\nexport const convertResponsesUsageToUsageMetadata: Converter<\n  OpenAIClient.Responses.ResponseUsage | undefined,\n  UsageMetadata\n> = (usage) => {\n  const inputTokenDetails = {\n    ...(usage?.input_tokens_details?.cached_tokens != null && {\n      cache_read: usage?.input_tokens_details?.cached_tokens,\n    }),\n  };\n  const outputTokenDetails = {\n    ...(usage?.output_tokens_details?.reasoning_tokens != null && {\n      reasoning: usage?.output_tokens_details?.reasoning_tokens,\n    }),\n  };\n  return {\n    input_tokens: usage?.input_tokens ?? 0,\n    output_tokens: usage?.output_tokens ?? 0,\n    total_tokens: usage?.total_tokens ?? 0,\n    input_token_details: inputTokenDetails,\n    output_token_details: outputTokenDetails,\n  };\n};\n\n/**\n * Converts an OpenAI Responses API response to a LangChain AIMessage.\n *\n * This converter processes the output from OpenAI's Responses API (both `create` and `parse` methods)\n * and transforms it into a LangChain AIMessage object with all relevant metadata, tool calls, and content.\n *\n * @param response - The response object from OpenAI's Responses API. Can be either:\n *   - ResponsesCreateInvoke: Result from `responses.create()`\n *   - ResponsesParseInvoke: Result from `responses.parse()`\n *\n * @returns An AIMessage containing:\n *   - `id`: The message ID from the response output\n *   - `content`: Array of message content blocks (text, images, etc.)\n *   - `tool_calls`: Array of successfully parsed tool calls\n *   - `invalid_tool_calls`: Array of tool calls that failed to parse\n *   - `usage_metadata`: Token usage information converted to LangChain format\n *   - `additional_kwargs`: Extra data including:\n *     - `refusal`: Refusal text if the model refused to respond\n *     - `reasoning`: Reasoning output for reasoning models\n *     - `tool_outputs`: Results from built-in tools (web search, file search, etc.)\n *     - `parsed`: Parsed structured output when using json_schema format\n *     - Function call ID mappings for tracking\n *   - `response_metadata`: Metadata about the response including model, timestamps, status, etc.\n *\n * @throws Error if the response contains an error object. The error message and code are extracted\n *   from the response.error field.\n *\n * @example\n * ```typescript\n * const response = await client.responses.create({\n *   model: \"gpt-4\",\n *   input: [{ type: \"message\", content: \"Hello\" }]\n * });\n * const message = convertResponsesMessageToAIMessage(response);\n * console.log(message.content); // Message content\n * console.log(message.tool_calls); // Any tool calls made\n * ```\n *\n * @remarks\n * The converter handles multiple output item types:\n * - `message`: Text and structured content from the model\n * - `function_call`: Tool/function calls that need to be executed\n * - `reasoning`: Reasoning traces from reasoning models (o1, o3, etc.)\n * - `custom_tool_call`: Custom tool invocations\n * - Built-in tool outputs: web_search, file_search, code_interpreter, etc.\n *\n * Tool calls are parsed and validated. Invalid tool calls (malformed JSON, etc.) are captured\n * in the `invalid_tool_calls` array rather than throwing errors.\n */\nexport const convertResponsesMessageToAIMessage: Converter<\n  ResponsesCreateInvoke | ResponsesParseInvoke,\n  AIMessage\n> = (response) => {\n  if (response.error) {\n    // TODO: add support for `addLangChainErrorFields`\n    const error = new Error(response.error.message);\n    error.name = response.error.code;\n    throw error;\n  }\n\n  let messageId: string | undefined;\n  const content: MessageContent = [];\n  const tool_calls: ToolCall[] = [];\n  const invalid_tool_calls: InvalidToolCall[] = [];\n  const response_metadata: Record<string, unknown> = {\n    model_provider: \"openai\",\n    model: response.model,\n    created_at: response.created_at,\n    id: response.id,\n    incomplete_details: response.incomplete_details,\n    metadata: response.metadata,\n    object: response.object,\n    status: response.status,\n    user: response.user,\n    service_tier: response.service_tier,\n    // for compatibility with chat completion calls.\n    model_name: response.model,\n  };\n\n  const additional_kwargs: {\n    [key: string]: unknown;\n    refusal?: string;\n    reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n    tool_outputs?: unknown[];\n    parsed?: unknown;\n    [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n  } = {};\n\n  for (const item of response.output) {\n    if (item.type === \"message\") {\n      messageId = item.id;\n      content.push(\n        ...item.content.flatMap((part) => {\n          if (part.type === \"output_text\") {\n            if (\"parsed\" in part && part.parsed != null) {\n              additional_kwargs.parsed = part.parsed;\n            }\n            return {\n              type: \"text\",\n              text: part.text,\n              annotations: part.annotations,\n            };\n          }\n\n          if (part.type === \"refusal\") {\n            additional_kwargs.refusal = part.refusal;\n            return [];\n          }\n\n          return part;\n        })\n      );\n    } else if (item.type === \"function_call\") {\n      const fnAdapter = {\n        function: { name: item.name, arguments: item.arguments },\n        id: item.call_id,\n      };\n\n      try {\n        tool_calls.push(parseToolCall(fnAdapter, { returnId: true }));\n      } catch (e: unknown) {\n        let errMessage: string | undefined;\n        if (\n          typeof e === \"object\" &&\n          e != null &&\n          \"message\" in e &&\n          typeof e.message === \"string\"\n        ) {\n          errMessage = e.message;\n        }\n        invalid_tool_calls.push(makeInvalidToolCall(fnAdapter, errMessage));\n      }\n\n      additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] ??= {};\n      if (item.id) {\n        additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY][item.call_id] = item.id;\n      }\n    } else if (item.type === \"reasoning\") {\n      additional_kwargs.reasoning = item;\n    } else if (item.type === \"custom_tool_call\") {\n      const parsed = parseCustomToolCall(item);\n      if (parsed) {\n        tool_calls.push(parsed);\n      } else {\n        invalid_tool_calls.push(\n          makeInvalidToolCall(item, \"Malformed custom tool call\")\n        );\n      }\n    } else if (item.type === \"computer_call\") {\n      const parsed = parseComputerCall(item);\n      if (parsed) {\n        tool_calls.push(parsed);\n      } else {\n        invalid_tool_calls.push(\n          makeInvalidToolCall(item, \"Malformed computer call\")\n        );\n      }\n    } else {\n      additional_kwargs.tool_outputs ??= [];\n      additional_kwargs.tool_outputs.push(item);\n    }\n  }\n\n  return new AIMessage({\n    id: messageId,\n    content,\n    tool_calls,\n    invalid_tool_calls,\n    usage_metadata: convertResponsesUsageToUsageMetadata(response.usage),\n    additional_kwargs,\n    response_metadata,\n  });\n};\n\n/**\n * Converts a LangChain ChatOpenAI reasoning summary to an OpenAI Responses API reasoning item.\n *\n * This converter transforms reasoning summaries that have been accumulated during streaming\n * (where summary parts may arrive in multiple chunks with the same index) into the final\n * consolidated format expected by OpenAI's Responses API. It combines summary parts that\n * share the same index and removes the index field from the final output.\n *\n * @param reasoning - A ChatOpenAI reasoning summary object containing:\n *   - `id`: The reasoning item ID\n *   - `type`: The type of reasoning (typically \"reasoning\")\n *   - `summary`: Array of summary parts, each with:\n *     - `text`: The summary text content\n *     - `type`: The summary type (e.g., \"summary_text\")\n *     - `index`: The index used to group related summary parts during streaming\n *\n * @returns An OpenAI Responses API ResponseReasoningItem with:\n *   - All properties from the input reasoning object\n *   - `summary`: Consolidated array of summary objects with:\n *     - `text`: Combined text from all parts with the same index\n *     - `type`: The summary type\n *     - No `index` field (removed after consolidation)\n *\n * @example\n * ```typescript\n * // Input: Reasoning summary with multiple parts at the same index\n * const reasoning = {\n *   id: \"reasoning_123\",\n *   type: \"reasoning\",\n *   summary: [\n *     { text: \"First \", type: \"summary_text\", index: 0 },\n *     { text: \"part\", type: \"summary_text\", index: 0 },\n *     { text: \"Second part\", type: \"summary_text\", index: 1 }\n *   ]\n * };\n *\n * const result = convertReasoningSummaryToResponsesReasoningItem(reasoning);\n * // Returns:\n * // {\n * //   id: \"reasoning_123\",\n * //   type: \"reasoning\",\n * //   summary: [\n * //     { text: \"First part\", type: \"summary_text\" },\n * //     { text: \"Second part\", type: \"summary_text\" }\n * //   ]\n * // }\n * ```\n *\n * @remarks\n * - This converter is primarily used when reconstructing complete reasoning items from\n *   streaming chunks, where summary parts may arrive incrementally with index markers\n * - Summary parts with the same index are concatenated in the order they appear\n * - If the reasoning summary contains only one part, no reduction is performed\n * - The index field is used internally during streaming to track which summary parts\n *   belong together, but is removed from the final output as it's not part of the\n *   OpenAI Responses API schema\n * - This is the inverse operation of the streaming accumulation that happens in\n *   `convertResponsesDeltaToChatGenerationChunk`\n */\nexport const convertReasoningSummaryToResponsesReasoningItem: Converter<\n  ChatOpenAIReasoningSummary,\n  OpenAIClient.Responses.ResponseReasoningItem\n> = (reasoning) => {\n  // combine summary parts that have the the same index and then remove the indexes\n  const summary = (\n    reasoning.summary.length > 1\n      ? reasoning.summary.reduce(\n          (acc, curr) => {\n            const last = acc[acc.length - 1];\n\n            if (last!.index === curr.index) {\n              last!.text += curr.text;\n            } else {\n              acc.push(curr);\n            }\n            return acc;\n          },\n          [{ ...reasoning.summary[0] }]\n        )\n      : reasoning.summary\n  ).map((s) =>\n    Object.fromEntries(Object.entries(s).filter(([k]) => k !== \"index\"))\n  ) as OpenAIClient.Responses.ResponseReasoningItem.Summary[];\n\n  return {\n    ...reasoning,\n    summary,\n  } as OpenAIClient.Responses.ResponseReasoningItem;\n};\n\n/**\n * Converts OpenAI Responses API stream events to LangChain ChatGenerationChunk objects.\n *\n * This converter processes streaming events from OpenAI's Responses API and transforms them\n * into LangChain ChatGenerationChunk objects that can be used in streaming chat applications.\n * It handles various event types including text deltas, tool calls, reasoning, and metadata updates.\n *\n * @param event - A streaming event from OpenAI's Responses API\n *\n * @returns A ChatGenerationChunk containing:\n *   - `text`: Concatenated text content from all text parts in the event\n *   - `message`: An AIMessageChunk with:\n *     - `id`: Message ID (set when a message output item is added)\n *     - `content`: Array of content blocks (text with optional annotations)\n *     - `tool_call_chunks`: Incremental tool call data (name, args, id)\n *     - `usage_metadata`: Token usage information (only in completion events)\n *     - `additional_kwargs`: Extra data including:\n *       - `refusal`: Refusal text if the model refused to respond\n *       - `reasoning`: Reasoning output for reasoning models (id, type, summary)\n *       - `tool_outputs`: Results from built-in tools (web search, file search, etc.)\n *       - `parsed`: Parsed structured output when using json_schema format\n *       - Function call ID mappings for tracking\n *     - `response_metadata`: Metadata about the response (model, id, etc.)\n *   - `generationInfo`: Additional generation information (e.g., tool output status)\n *\n *   Returns `null` for events that don't produce meaningful chunks:\n *   - Partial image generation events (to avoid storing all partial images in history)\n *   - Unrecognized event types\n *\n * @example\n * ```typescript\n * const stream = await client.responses.create({\n *   model: \"gpt-4\",\n *   input: [{ type: \"message\", content: \"Hello\" }],\n *   stream: true\n * });\n *\n * for await (const event of stream) {\n *   const chunk = convertResponsesDeltaToChatGenerationChunk(event);\n *   if (chunk) {\n *     console.log(chunk.text); // Incremental text\n *     console.log(chunk.message.tool_call_chunks); // Tool call updates\n *   }\n * }\n * ```\n *\n * @remarks\n * - Text content is accumulated in an array with index tracking for proper ordering\n * - Tool call chunks include incremental arguments that need to be concatenated by the consumer\n * - Reasoning summaries are built incrementally across multiple events\n * - Function call IDs are tracked in `additional_kwargs` to map call_id to item id\n * - The `text` field is provided for legacy compatibility with `onLLMNewToken` callbacks\n * - Usage metadata is only available in `response.completed` events\n * - Partial images are intentionally ignored to prevent memory bloat in conversation history\n */\nexport const convertResponsesDeltaToChatGenerationChunk: Converter<\n  OpenAIClient.Responses.ResponseStreamEvent,\n  ChatGenerationChunk | null\n> = (event) => {\n  const content: Record<string, unknown>[] = [];\n  let generationInfo: Record<string, unknown> = {};\n  let usage_metadata: UsageMetadata | undefined;\n  const tool_call_chunks: ToolCallChunk[] = [];\n  const response_metadata: Record<string, unknown> = {\n    model_provider: \"openai\",\n  };\n  const additional_kwargs: {\n    [key: string]: unknown;\n    reasoning?: Partial<ChatOpenAIReasoningSummary>;\n    tool_outputs?: unknown[];\n  } = {};\n  let id: string | undefined;\n  if (event.type === \"response.output_text.delta\") {\n    content.push({\n      type: \"text\",\n      text: event.delta,\n      index: event.content_index,\n    });\n  } else if (event.type === \"response.output_text.annotation.added\") {\n    content.push({\n      type: \"text\",\n      text: \"\",\n      annotations: [event.annotation],\n      index: event.content_index,\n    });\n  } else if (\n    event.type === \"response.output_item.added\" &&\n    event.item.type === \"message\"\n  ) {\n    id = event.item.id;\n  } else if (\n    event.type === \"response.output_item.added\" &&\n    event.item.type === \"function_call\"\n  ) {\n    tool_call_chunks.push({\n      type: \"tool_call_chunk\",\n      name: event.item.name,\n      args: event.item.arguments,\n      id: event.item.call_id,\n      index: event.output_index,\n    });\n\n    additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] = {\n      [event.item.call_id]: event.item.id,\n    };\n  } else if (\n    event.type === \"response.output_item.done\" &&\n    event.item.type === \"computer_call\"\n  ) {\n    // Handle computer_call as a tool call so ToolNode can process it\n    tool_call_chunks.push({\n      type: \"tool_call_chunk\",\n      name: \"computer_use\",\n      args: JSON.stringify({ action: event.item.action }),\n      id: event.item.call_id,\n      index: event.output_index,\n    });\n    // Also store the raw item for additional context (pending_safety_checks, etc.)\n    additional_kwargs.tool_outputs = [event.item];\n  } else if (\n    event.type === \"response.output_item.done\" &&\n    [\n      \"web_search_call\",\n      \"file_search_call\",\n      \"code_interpreter_call\",\n      \"mcp_call\",\n      \"mcp_list_tools\",\n      \"mcp_approval_request\",\n      \"image_generation_call\",\n      \"custom_tool_call\",\n    ].includes(event.item.type)\n  ) {\n    additional_kwargs.tool_outputs = [event.item];\n  } else if (event.type === \"response.created\") {\n    response_metadata.id = event.response.id;\n    response_metadata.model_name = event.response.model;\n    response_metadata.model = event.response.model;\n  } else if (event.type === \"response.completed\") {\n    const msg = convertResponsesMessageToAIMessage(event.response);\n\n    usage_metadata = convertResponsesUsageToUsageMetadata(event.response.usage);\n\n    if (event.response.text?.format?.type === \"json_schema\") {\n      additional_kwargs.parsed ??= JSON.parse(msg.text);\n    }\n    for (const [key, value] of Object.entries(event.response)) {\n      if (key !== \"id\") response_metadata[key] = value;\n    }\n  } else if (\n    event.type === \"response.function_call_arguments.delta\" ||\n    event.type === \"response.custom_tool_call_input.delta\"\n  ) {\n    tool_call_chunks.push({\n      type: \"tool_call_chunk\",\n      args: event.delta,\n      index: event.output_index,\n    });\n  } else if (\n    event.type === \"response.web_search_call.completed\" ||\n    event.type === \"response.file_search_call.completed\"\n  ) {\n    generationInfo = {\n      tool_outputs: {\n        id: event.item_id,\n        type: event.type.replace(\"response.\", \"\").replace(\".completed\", \"\"),\n        status: \"completed\",\n      },\n    };\n  } else if (event.type === \"response.refusal.done\") {\n    additional_kwargs.refusal = event.refusal;\n  } else if (\n    event.type === \"response.output_item.added\" &&\n    \"item\" in event &&\n    event.item.type === \"reasoning\"\n  ) {\n    const summary: ChatOpenAIReasoningSummary[\"summary\"] | undefined = event\n      .item.summary\n      ? event.item.summary.map((s, index) => ({\n          ...s,\n          index,\n        }))\n      : undefined;\n\n    additional_kwargs.reasoning = {\n      // We only capture ID in the first event or else the concatenated result of all chunks will\n      // have an ID field that is repeated once per event. There is special handling for the `type`\n      // field that prevents this, however.\n      id: event.item.id,\n      type: event.item.type,\n      ...(summary ? { summary } : {}),\n    };\n  } else if (event.type === \"response.reasoning_summary_part.added\") {\n    additional_kwargs.reasoning = {\n      type: \"reasoning\",\n      summary: [{ ...event.part, index: event.summary_index }],\n    };\n  } else if (event.type === \"response.reasoning_summary_text.delta\") {\n    additional_kwargs.reasoning = {\n      type: \"reasoning\",\n      summary: [\n        {\n          text: event.delta,\n          type: \"summary_text\",\n          index: event.summary_index,\n        },\n      ],\n    };\n  } else if (event.type === \"response.image_generation_call.partial_image\") {\n    // noop/fixme: retaining partial images in a message chunk means that _all_\n    // partial images get kept in history, so we don't do anything here.\n    return null;\n  } else {\n    return null;\n  }\n\n  return new ChatGenerationChunk({\n    // Legacy reasons, `onLLMNewToken` should pulls this out\n    text: content.map((part) => part.text).join(\"\"),\n    message: new AIMessageChunk({\n      id,\n      content: content as MessageContent,\n      tool_call_chunks,\n      usage_metadata,\n      additional_kwargs,\n      response_metadata,\n    }),\n    generationInfo,\n  });\n};\n\n/**\n * Converts a single LangChain BaseMessage to OpenAI Responses API input format.\n *\n * This converter transforms a LangChain message into one or more ResponseInputItem objects\n * that can be used with OpenAI's Responses API. It handles complex message structures including\n * tool calls, reasoning blocks, multimodal content, and various content block types.\n *\n * @param message - The LangChain BaseMessage to convert. Can be any message type including\n *   HumanMessage, AIMessage, SystemMessage, ToolMessage, etc.\n *\n * @returns An array of ResponseInputItem objects.\n *\n * @example\n * Basic text message conversion:\n * ```typescript\n * const message = new HumanMessage(\"Hello, how are you?\");\n * const items = convertStandardContentMessageToResponsesInput(message);\n * // Returns: [{ type: \"message\", role: \"user\", content: [{ type: \"input_text\", text: \"Hello, how are you?\" }] }]\n * ```\n *\n * @example\n * AI message with tool calls:\n * ```typescript\n * const message = new AIMessage({\n *   content: \"I'll check the weather for you.\",\n *   tool_calls: [{\n *     id: \"call_123\",\n *     name: \"get_weather\",\n *     args: { location: \"San Francisco\" }\n *   }]\n * });\n * const items = convertStandardContentMessageToResponsesInput(message);\n * // Returns:\n * // [\n * //   { type: \"message\", role: \"assistant\", content: [{ type: \"input_text\", text: \"I'll check the weather for you.\" }] },\n * //   { type: \"function_call\", call_id: \"call_123\", name: \"get_weather\", arguments: '{\"location\":\"San Francisco\"}' }\n * // ]\n * ```\n */\nexport const convertStandardContentMessageToResponsesInput: Converter<\n  BaseMessage,\n  OpenAIClient.Responses.ResponseInputItem[]\n> = (message) => {\n  const isResponsesMessage =\n    AIMessage.isInstance(message) &&\n    message.response_metadata?.model_provider === \"openai\";\n\n  function* iterateItems(): Generator<OpenAIClient.Responses.ResponseInputItem> {\n    const messageRole = iife(() => {\n      try {\n        const role = messageToOpenAIRole(message);\n        if (\n          role === \"system\" ||\n          role === \"developer\" ||\n          role === \"assistant\" ||\n          role === \"user\"\n        ) {\n          return role;\n        }\n        return \"assistant\";\n      } catch {\n        return \"assistant\";\n      }\n    });\n\n    let currentMessage: OpenAIClient.Responses.EasyInputMessage | undefined =\n      undefined;\n\n    const functionCallIdsWithBlocks = new Set<string>();\n    const serverFunctionCallIdsWithBlocks = new Set<string>();\n\n    const pendingFunctionChunks = new Map<\n      string,\n      { name?: string; args: string[] }\n    >();\n    const pendingServerFunctionChunks = new Map<\n      string,\n      { name?: string; args: string[] }\n    >();\n\n    function* flushMessage() {\n      if (!currentMessage) return;\n      const content = currentMessage.content;\n      if (\n        (typeof content === \"string\" && content.length > 0) ||\n        (Array.isArray(content) && content.length > 0)\n      ) {\n        yield currentMessage;\n      }\n      currentMessage = undefined;\n    }\n\n    const pushMessageContent: (\n      content: OpenAIClient.Responses.ResponseInputMessageContentList\n    ) => void = (content) => {\n      if (!currentMessage) {\n        currentMessage = {\n          type: \"message\",\n          role: messageRole,\n          content: [],\n        };\n      }\n      if (typeof currentMessage.content === \"string\") {\n        currentMessage.content =\n          currentMessage.content.length > 0\n            ? [{ type: \"input_text\", text: currentMessage.content }, ...content]\n            : [...content];\n      } else {\n        currentMessage.content.push(...content);\n      }\n    };\n\n    const toJsonString = (value: unknown) => {\n      if (typeof value === \"string\") {\n        return value;\n      }\n      try {\n        return JSON.stringify(value ?? {});\n      } catch {\n        return \"{}\";\n      }\n    };\n\n    const resolveImageItem = (\n      block: ContentBlock.Multimodal.Image\n    ): OpenAIClient.Responses.ResponseInputImage | undefined => {\n      const detail = iife(() => {\n        const raw = block.metadata?.detail;\n        if (raw === \"low\" || raw === \"high\" || raw === \"auto\") {\n          return raw;\n        }\n        return \"auto\";\n      });\n      if (block.fileId) {\n        return {\n          type: \"input_image\",\n          detail,\n          file_id: block.fileId,\n        };\n      }\n      if (block.url) {\n        return {\n          type: \"input_image\",\n          detail,\n          image_url: block.url,\n        };\n      }\n      if (block.data) {\n        const base64Data =\n          typeof block.data === \"string\"\n            ? block.data\n            : Buffer.from(block.data).toString(\"base64\");\n        const mimeType = block.mimeType ?? \"image/png\";\n        return {\n          type: \"input_image\",\n          detail,\n          image_url: `data:${mimeType};base64,${base64Data}`,\n        };\n      }\n      return undefined;\n    };\n\n    const resolveFileItem = (\n      block: ContentBlock.Multimodal.File | ContentBlock.Multimodal.Video\n    ): OpenAIClient.Responses.ResponseInputFile | undefined => {\n      const filename = getRequiredFilenameFromMetadata(block);\n\n      if (block.fileId && typeof filename === \"string\") {\n        return {\n          type: \"input_file\",\n          file_id: block.fileId,\n          ...(filename ? { filename } : {}),\n        };\n      }\n      if (block.url && typeof filename === \"string\") {\n        return {\n          type: \"input_file\",\n          file_url: block.url,\n          ...(filename ? { filename } : {}),\n        };\n      }\n      if (block.data && typeof filename === \"string\") {\n        const encoded =\n          typeof block.data === \"string\"\n            ? block.data\n            : Buffer.from(block.data).toString(\"base64\");\n        const mimeType = block.mimeType ?? \"application/octet-stream\";\n        return {\n          type: \"input_file\",\n          file_data: `data:${mimeType};base64,${encoded}`,\n          ...(filename ? { filename } : {}),\n        };\n      }\n      return undefined;\n    };\n\n    const convertReasoningBlock = (\n      block: ContentBlock.Reasoning\n    ): OpenAIClient.Responses.ResponseReasoningItem => {\n      const summaryEntries = iife(() => {\n        if (Array.isArray(block.summary)) {\n          const candidate = block.summary;\n          const mapped =\n            candidate\n              ?.map((item) => item?.text)\n              .filter((text): text is string => typeof text === \"string\") ?? [];\n          if (mapped.length > 0) {\n            return mapped;\n          }\n        }\n        return block.reasoning ? [block.reasoning] : [];\n      });\n\n      const summary =\n        summaryEntries.length > 0\n          ? summaryEntries.map((text) => ({\n              type: \"summary_text\" as const,\n              text,\n            }))\n          : [{ type: \"summary_text\" as const, text: \"\" }];\n\n      const reasoningItem: OpenAIClient.Responses.ResponseReasoningItem = {\n        type: \"reasoning\",\n        id: block.id ?? \"\",\n        summary,\n      };\n\n      if (block.reasoning) {\n        reasoningItem.content = [\n          {\n            type: \"reasoning_text\" as const,\n            text: block.reasoning,\n          },\n        ];\n      }\n      return reasoningItem;\n    };\n\n    const convertFunctionCall = (\n      block: ContentBlock.Tools.ToolCall | ContentBlock.Tools.ServerToolCall\n    ): OpenAIClient.Responses.ResponseFunctionToolCall => ({\n      type: \"function_call\",\n      name: block.name ?? \"\",\n      call_id: block.id ?? \"\",\n      arguments: toJsonString(block.args),\n    });\n\n    const convertFunctionCallOutput = (\n      block: ContentBlock.Tools.ServerToolCallResult\n    ): OpenAIClient.Responses.ResponseInputItem.FunctionCallOutput => {\n      const output = toJsonString(block.output);\n      const status =\n        block.status === \"success\"\n          ? \"completed\"\n          : block.status === \"error\"\n          ? \"incomplete\"\n          : undefined;\n      return {\n        type: \"function_call_output\",\n        call_id: block.toolCallId ?? \"\",\n        output,\n        ...(status ? { status } : {}),\n      };\n    };\n\n    for (const block of message.contentBlocks) {\n      if (block.type === \"text\") {\n        pushMessageContent([{ type: \"input_text\", text: block.text }]);\n      } else if (block.type === \"invalid_tool_call\") {\n        // no-op\n      } else if (block.type === \"reasoning\") {\n        yield* flushMessage();\n        yield convertReasoningBlock(\n          block as ContentBlock.Standard & { type: \"reasoning\" }\n        );\n      } else if (block.type === \"tool_call\") {\n        yield* flushMessage();\n        const id = block.id ?? \"\";\n        if (id) {\n          functionCallIdsWithBlocks.add(id);\n          pendingFunctionChunks.delete(id);\n        }\n        yield convertFunctionCall(\n          block as ContentBlock.Standard & { type: \"tool_call\" }\n        );\n      } else if (block.type === \"tool_call_chunk\") {\n        if (block.id) {\n          const existing = pendingFunctionChunks.get(block.id) ?? {\n            name: block.name,\n            args: [],\n          };\n          if (block.name) existing.name = block.name;\n          if (block.args) existing.args.push(block.args);\n          pendingFunctionChunks.set(block.id, existing);\n        }\n      } else if (block.type === \"server_tool_call\") {\n        yield* flushMessage();\n        const id = block.id ?? \"\";\n        if (id) {\n          serverFunctionCallIdsWithBlocks.add(id);\n          pendingServerFunctionChunks.delete(id);\n        }\n        yield convertFunctionCall(block);\n      } else if (block.type === \"server_tool_call_chunk\") {\n        if (block.id) {\n          const existing = pendingServerFunctionChunks.get(block.id) ?? {\n            name: block.name,\n            args: [],\n          };\n          if (block.name) existing.name = block.name;\n          if (block.args) existing.args.push(block.args);\n          pendingServerFunctionChunks.set(block.id, existing);\n        }\n      } else if (block.type === \"server_tool_call_result\") {\n        yield* flushMessage();\n        yield convertFunctionCallOutput(block);\n      } else if (block.type === \"audio\") {\n        // no-op\n      } else if (block.type === \"file\") {\n        const fileItem = resolveFileItem(block);\n        if (fileItem) {\n          pushMessageContent([fileItem]);\n        }\n      } else if (block.type === \"image\") {\n        const imageItem = resolveImageItem(block);\n        if (imageItem) {\n          pushMessageContent([imageItem]);\n        }\n      } else if (block.type === \"video\") {\n        const videoItem = resolveFileItem(block);\n        if (videoItem) {\n          pushMessageContent([videoItem]);\n        }\n      } else if (block.type === \"text-plain\") {\n        if (block.text) {\n          pushMessageContent([\n            {\n              type: \"input_text\",\n              text: block.text,\n            },\n          ]);\n        }\n      } else if (block.type === \"non_standard\" && isResponsesMessage) {\n        yield* flushMessage();\n        yield block.value as ResponsesInputItem;\n      }\n    }\n    yield* flushMessage();\n\n    for (const [id, chunk] of pendingFunctionChunks) {\n      if (!id || functionCallIdsWithBlocks.has(id)) continue;\n      const args = chunk.args.join(\"\");\n      if (!chunk.name && !args) continue;\n      yield {\n        type: \"function_call\",\n        call_id: id,\n        name: chunk.name ?? \"\",\n        arguments: args,\n      };\n    }\n\n    for (const [id, chunk] of pendingServerFunctionChunks) {\n      if (!id || serverFunctionCallIdsWithBlocks.has(id)) continue;\n      const args = chunk.args.join(\"\");\n      if (!chunk.name && !args) continue;\n      yield {\n        type: \"function_call\",\n        call_id: id,\n        name: chunk.name ?? \"\",\n        arguments: args,\n      };\n    }\n  }\n  return Array.from(iterateItems());\n};\n\n/**\n * - MCP (Model Context Protocol) approval responses\n * - Zero Data Retention (ZDR) mode handling\n *\n * @param params - Conversion parameters\n * @param params.messages - Array of LangChain BaseMessages to convert\n * @param params.zdrEnabled - Whether Zero Data Retention mode is enabled. When true, certain\n *   metadata like message IDs and function call IDs are omitted from the output\n * @param params.model - The model name being used. Used to determine if special role mapping\n *   is needed (e.g., \"system\" -> \"developer\" for reasoning models)\n *\n * @returns Array of ResponsesInputItem objects formatted for the OpenAI Responses API\n *\n * @throws {Error} When a function message is encountered (not supported)\n * @throws {Error} When computer call output format is invalid\n *\n * @example\n * ```typescript\n * const messages = [\n *   new HumanMessage(\"Hello\"),\n *   new AIMessage({ content: \"Hi there!\", tool_calls: [...] })\n * ];\n *\n * const input = convertMessagesToResponsesInput({\n *   messages,\n *   zdrEnabled: false,\n *   model: \"gpt-4\"\n * });\n * ```\n */\nexport const convertMessagesToResponsesInput: Converter<\n  { messages: BaseMessage[]; zdrEnabled: boolean; model: string },\n  ResponsesInputItem[]\n> = ({ messages, zdrEnabled, model }) => {\n  return messages.flatMap(\n    (lcMsg): ResponsesInputItem | ResponsesInputItem[] => {\n      const responseMetadata = lcMsg.response_metadata as\n        | Record<string, unknown>\n        | undefined;\n      if (responseMetadata?.output_version === \"v1\") {\n        return convertStandardContentMessageToResponsesInput(lcMsg);\n      }\n\n      const additional_kwargs = lcMsg.additional_kwargs as\n        | BaseMessageFields[\"additional_kwargs\"] & {\n            [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n            reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n            type?: string;\n            refusal?: string;\n          };\n\n      let role = messageToOpenAIRole(lcMsg);\n      if (role === \"system\" && isReasoningModel(model)) role = \"developer\";\n\n      if (role === \"function\") {\n        throw new Error(\"Function messages are not supported in Responses API\");\n      }\n\n      if (role === \"tool\") {\n        const toolMessage = lcMsg as ToolMessage;\n\n        // Handle computer call output\n        if (additional_kwargs?.type === \"computer_call_output\") {\n          const output = (() => {\n            if (typeof toolMessage.content === \"string\") {\n              return {\n                type: \"input_image\" as const,\n                image_url: toolMessage.content,\n              };\n            }\n\n            if (Array.isArray(toolMessage.content)) {\n              /**\n               * Check for input_image type first (computer-use-preview format)\n               */\n              const inputImage = toolMessage.content.find(\n                (i) => i.type === \"input_image\"\n              ) as { type: \"input_image\"; image_url: string } | undefined;\n\n              if (inputImage) return inputImage;\n\n              /**\n               * Check for computer_screenshot type (legacy format)\n               */\n              const oaiScreenshot = toolMessage.content.find(\n                (i) => i.type === \"computer_screenshot\"\n              ) as\n                | { type: \"computer_screenshot\"; image_url: string }\n                | undefined;\n\n              if (oaiScreenshot) return oaiScreenshot;\n\n              /**\n               * Convert image_url content block to input_image format\n               */\n              const lcImage = toolMessage.content.find(\n                (i) => i.type === \"image_url\"\n              ) as MessageContentImageUrl;\n\n              if (lcImage) {\n                return {\n                  type: \"input_image\" as const,\n                  image_url:\n                    typeof lcImage.image_url === \"string\"\n                      ? lcImage.image_url\n                      : lcImage.image_url.url,\n                };\n              }\n            }\n\n            throw new Error(\"Invalid computer call output\");\n          })();\n\n          /**\n           * Cast needed because OpenAI SDK types don't yet include input_image\n           * for computer-use-preview model output format\n           */\n          return {\n            type: \"computer_call_output\",\n            output,\n            call_id: toolMessage.tool_call_id,\n          } as ResponsesInputItem;\n        }\n\n        // Handle custom tool output\n        if (toolMessage.additional_kwargs?.customTool) {\n          return {\n            type: \"custom_tool_call_output\",\n            call_id: toolMessage.tool_call_id,\n            output: toolMessage.content as string,\n          };\n        }\n\n        // Check if content contains provider-native OpenAI content blocks\n        // that should be passed through without stringification\n        const isProviderNativeContent =\n          Array.isArray(toolMessage.content) &&\n          toolMessage.content.every(\n            (item) =>\n              typeof item === \"object\" &&\n              item !== null &&\n              \"type\" in item &&\n              (item.type === \"input_file\" ||\n                item.type === \"input_image\" ||\n                item.type === \"input_text\")\n          );\n\n        return {\n          type: \"function_call_output\",\n          call_id: toolMessage.tool_call_id,\n          id: toolMessage.id?.startsWith(\"fc_\") ? toolMessage.id : undefined,\n          output: isProviderNativeContent\n            ? (toolMessage.content as OpenAIClient.Responses.ResponseFunctionCallOutputItemList)\n            : typeof toolMessage.content !== \"string\"\n            ? JSON.stringify(toolMessage.content)\n            : toolMessage.content,\n        };\n      }\n\n      if (role === \"assistant\") {\n        // if we have the original response items, just reuse them\n        if (\n          !zdrEnabled &&\n          responseMetadata?.output != null &&\n          Array.isArray(responseMetadata?.output) &&\n          responseMetadata?.output.length > 0 &&\n          responseMetadata?.output.every((item) => \"type\" in item)\n        ) {\n          return responseMetadata?.output;\n        }\n\n        // otherwise, try to reconstruct the response from what we have\n\n        const input: ResponsesInputItem[] = [];\n\n        // reasoning items\n        if (additional_kwargs?.reasoning && !zdrEnabled) {\n          const reasoningItem = convertReasoningSummaryToResponsesReasoningItem(\n            additional_kwargs.reasoning\n          );\n          input.push(reasoningItem);\n        }\n\n        // ai content\n        let { content } = lcMsg as { content: ContentBlock[] };\n        if (additional_kwargs?.refusal) {\n          if (typeof content === \"string\") {\n            content = [{ type: \"output_text\", text: content, annotations: [] }];\n          }\n          content = [\n            ...(content as ContentBlock[]),\n            { type: \"refusal\", refusal: additional_kwargs.refusal },\n          ];\n        }\n\n        if (typeof content === \"string\" || content.length > 0) {\n          input.push({\n            type: \"message\",\n            role: \"assistant\",\n            ...(lcMsg.id && !zdrEnabled && lcMsg.id.startsWith(\"msg_\")\n              ? { id: lcMsg.id }\n              : {}),\n            content: iife(() => {\n              if (typeof content === \"string\") {\n                return content;\n              }\n              return content.flatMap((item) => {\n                if (item.type === \"text\") {\n                  return {\n                    type: \"output_text\",\n                    text: item.text,\n                    annotations: item.annotations ?? [],\n                  };\n                }\n\n                if (item.type === \"output_text\" || item.type === \"refusal\") {\n                  return item;\n                }\n\n                return [];\n              });\n            }) as ResponseInputMessageContentList,\n          });\n        }\n\n        const functionCallIds = additional_kwargs?.[_FUNCTION_CALL_IDS_MAP_KEY];\n\n        if (AIMessage.isInstance(lcMsg) && !!lcMsg.tool_calls?.length) {\n          input.push(\n            ...lcMsg.tool_calls.map((toolCall): ResponsesInputItem => {\n              if (isCustomToolCall(toolCall)) {\n                return {\n                  type: \"custom_tool_call\",\n                  id: toolCall.call_id,\n                  call_id: toolCall.id ?? \"\",\n                  input: toolCall.args.input,\n                  name: toolCall.name,\n                };\n              }\n              if (isComputerToolCall(toolCall)) {\n                return {\n                  type: \"computer_call\",\n                  id: toolCall.call_id,\n                  call_id: toolCall.id ?? \"\",\n                  action: toolCall.args.action,\n                } as ResponsesInputItem;\n              }\n              return {\n                type: \"function_call\",\n                name: toolCall.name,\n                arguments: JSON.stringify(toolCall.args),\n                call_id: toolCall.id!,\n                ...(!zdrEnabled ? { id: functionCallIds?.[toolCall.id!] } : {}),\n              };\n            })\n          );\n        } else if (additional_kwargs?.tool_calls) {\n          input.push(\n            ...additional_kwargs.tool_calls.map(\n              (toolCall): ResponsesInputItem => ({\n                type: \"function_call\",\n                name: toolCall.function.name,\n                call_id: toolCall.id,\n                arguments: toolCall.function.arguments,\n                ...(!zdrEnabled ? { id: functionCallIds?.[toolCall.id] } : {}),\n              })\n            )\n          );\n        }\n\n        const toolOutputs = (\n          responseMetadata?.output as Array<ResponsesInputItem>\n        )?.length\n          ? responseMetadata?.output\n          : additional_kwargs.tool_outputs;\n\n        const fallthroughCallTypes: ResponsesInputItem[\"type\"][] = [\n          \"computer_call\",\n          \"mcp_call\",\n          \"code_interpreter_call\",\n          \"image_generation_call\",\n        ];\n\n        if (toolOutputs != null) {\n          const castToolOutputs = toolOutputs as Array<ResponsesInputItem>;\n          const fallthroughCalls = castToolOutputs?.filter((item) =>\n            fallthroughCallTypes.includes(item.type)\n          );\n          if (fallthroughCalls.length > 0) input.push(...fallthroughCalls);\n        }\n\n        return input;\n      }\n\n      if (role === \"user\" || role === \"system\" || role === \"developer\") {\n        if (typeof lcMsg.content === \"string\") {\n          return { type: \"message\", role, content: lcMsg.content };\n        }\n\n        const messages: ResponsesInputItem[] = [];\n        const content = (lcMsg.content as ContentBlock[]).flatMap((item) => {\n          if (item.type === \"mcp_approval_response\") {\n            messages.push({\n              type: \"mcp_approval_response\",\n              approval_request_id: item.approval_request_id as string,\n              approve: item.approve as boolean,\n            });\n          }\n          if (isDataContentBlock(item)) {\n            return convertToProviderContentBlock(\n              item,\n              completionsApiContentBlockConverter\n            );\n          }\n          if (item.type === \"text\") {\n            return {\n              type: \"input_text\",\n              text: item.text,\n            };\n          }\n          if (item.type === \"image_url\") {\n            const imageUrl = iife(() => {\n              if (typeof item.image_url === \"string\") {\n                return item.image_url;\n              } else if (\n                typeof item.image_url === \"object\" &&\n                item.image_url !== null &&\n                \"url\" in item.image_url\n              ) {\n                return item.image_url.url;\n              }\n              return undefined;\n            });\n            const detail = iife(() => {\n              if (typeof item.image_url === \"string\") {\n                return \"auto\";\n              } else if (\n                typeof item.image_url === \"object\" &&\n                item.image_url !== null &&\n                \"detail\" in item.image_url\n              ) {\n                return item.image_url.detail;\n              }\n              return undefined;\n            });\n            return {\n              type: \"input_image\",\n              image_url: imageUrl,\n              detail,\n            };\n          }\n          if (\n            item.type === \"input_text\" ||\n            item.type === \"input_image\" ||\n            item.type === \"input_file\"\n          ) {\n            return item;\n          }\n          return [];\n        });\n\n        if (content.length > 0) {\n          messages.push({\n            type: \"message\",\n            role,\n            content: content as ResponseInputMessageContentList,\n          });\n        }\n        return messages;\n      }\n\n      console.warn(\n        `Unsupported role found when converting to OpenAI Responses API: ${role}`\n      );\n      return [];\n    }\n  );\n};\n"],"names":["convertResponsesUsageToUsageMetadata: Converter<\n  OpenAIClient.Responses.ResponseUsage | undefined,\n  UsageMetadata\n>","convertResponsesMessageToAIMessage: Converter<\n  ResponsesCreateInvoke | ResponsesParseInvoke,\n  AIMessage\n>","messageId: string | undefined","content: MessageContent","tool_calls: ToolCall[]","invalid_tool_calls: InvalidToolCall[]","response_metadata: Record<string, unknown>","additional_kwargs: {\n    [key: string]: unknown;\n    refusal?: string;\n    reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n    tool_outputs?: unknown[];\n    parsed?: unknown;\n    [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n  }","e: unknown","errMessage: string | undefined","convertReasoningSummaryToResponsesReasoningItem: Converter<\n  ChatOpenAIReasoningSummary,\n  OpenAIClient.Responses.ResponseReasoningItem\n>","convertResponsesDeltaToChatGenerationChunk: Converter<\n  OpenAIClient.Responses.ResponseStreamEvent,\n  ChatGenerationChunk | null\n>","content: Record<string, unknown>[]","generationInfo: Record<string, unknown>","usage_metadata: UsageMetadata | undefined","tool_call_chunks: ToolCallChunk[]","additional_kwargs: {\n    [key: string]: unknown;\n    reasoning?: Partial<ChatOpenAIReasoningSummary>;\n    tool_outputs?: unknown[];\n  }","id: string | undefined","summary: ChatOpenAIReasoningSummary[\"summary\"] | undefined","convertStandardContentMessageToResponsesInput: Converter<\n  BaseMessage,\n  OpenAIClient.Responses.ResponseInputItem[]\n>","currentMessage: OpenAIClient.Responses.EasyInputMessage | undefined","pushMessageContent: (\n      content: OpenAIClient.Responses.ResponseInputMessageContentList\n    ) => void","value: unknown","block: ContentBlock.Multimodal.Image","block: ContentBlock.Multimodal.File | ContentBlock.Multimodal.Video","block: ContentBlock.Reasoning","reasoningItem: OpenAIClient.Responses.ResponseReasoningItem","block: ContentBlock.Tools.ToolCall | ContentBlock.Tools.ServerToolCall","block: ContentBlock.Tools.ServerToolCallResult","convertMessagesToResponsesInput: Converter<\n  { messages: BaseMessage[]; zdrEnabled: boolean; model: string },\n  ResponsesInputItem[]\n>","input: ResponsesInputItem[]","fallthroughCallTypes: ResponsesInputItem[\"type\"][]","messages: ResponsesInputItem[]","messages"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAyCA,MAAM,6BAA6B;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAkEnC,MAAaA,uCAGT,CAAC,UAAU;IACb,MAAM,oBAAoB;QACxB,GAAI,OAAO,sBAAsB,iBAAiB,QAAQ;YACxD,YAAY,OAAO,sBAAsB;QAC1C,CAAA;IACF;IACD,MAAM,qBAAqB;QACzB,GAAI,OAAO,uBAAuB,oBAAoB,QAAQ;YAC5D,WAAW,OAAO,uBAAuB;QAC1C,CAAA;IACF;IACD,OAAO;QACL,cAAc,OAAO,gBAAgB;QACrC,eAAe,OAAO,iBAAiB;QACvC,cAAc,OAAO,gBAAgB;QACrC,qBAAqB;QACrB,sBAAsB;IACvB;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAmDD,MAAaC,qCAGT,CAAC,aAAa;IAChB,IAAI,SAAS,KAAA,EAAO;QAElB,MAAM,QAAQ,IAAI,MAAM,SAAS,KAAA,CAAM,OAAA;QACvC,MAAM,IAAA,GAAO,SAAS,KAAA,CAAM,IAAA;QAC5B,MAAM;IACP;IAED,IAAIC;IACJ,MAAMC,UAA0B,CAAE,CAAA;IAClC,MAAMC,aAAyB,CAAE,CAAA;IACjC,MAAMC,qBAAwC,CAAE,CAAA;IAChD,MAAMC,oBAA6C;QACjD,gBAAgB;QAChB,OAAO,SAAS,KAAA;QAChB,YAAY,SAAS,UAAA;QACrB,IAAI,SAAS,EAAA;QACb,oBAAoB,SAAS,kBAAA;QAC7B,UAAU,SAAS,QAAA;QACnB,QAAQ,SAAS,MAAA;QACjB,QAAQ,SAAS,MAAA;QACjB,MAAM,SAAS,IAAA;QACf,cAAc,SAAS,YAAA;QAEvB,YAAY,SAAS,KAAA;IACtB;IAED,MAAMC,oBAOF,CAAE;IAEN,KAAK,MAAM,QAAQ,SAAS,MAAA,CAC1B,IAAI,KAAK,IAAA,KAAS,WAAW;QAC3B,YAAY,KAAK,EAAA;QACjB,QAAQ,IAAA,CACN,GAAG,KAAK,OAAA,CAAQ,OAAA,CAAQ,CAAC,SAAS;YAChC,IAAI,KAAK,IAAA,KAAS,eAAe;gBAC/B,IAAI,YAAY,QAAQ,KAAK,MAAA,IAAU,MACrC,kBAAkB,MAAA,GAAS,KAAK,MAAA;gBAElC,OAAO;oBACL,MAAM;oBACN,MAAM,KAAK,IAAA;oBACX,aAAa,KAAK,WAAA;gBACnB;YACF;YAED,IAAI,KAAK,IAAA,KAAS,WAAW;gBAC3B,kBAAkB,OAAA,GAAU,KAAK,OAAA;gBACjC,OAAO,CAAE,CAAA;YACV;YAED,OAAO;QACR,EAAC,CACH;IACF,OAAA,IAAU,KAAK,IAAA,KAAS,iBAAiB;QACxC,MAAM,YAAY;YAChB,UAAU;gBAAE,MAAM,KAAK,IAAA;gBAAM,WAAW,KAAK,SAAA;YAAW;YACxD,IAAI,KAAK,OAAA;QACV;QAED,IAAI;YACF,WAAW,IAAA,KAAK,6NAAA,EAAc,WAAW;gBAAE,UAAU;YAAM,EAAC,CAAC;QAC9D,EAAA,OAAQC,GAAY;YACnB,IAAIC;YACJ,IACE,OAAO,MAAM,YACb,KAAK,QACL,aAAa,KACb,OAAO,EAAE,OAAA,KAAY,UAErB,aAAa,EAAE,OAAA;YAEjB,mBAAmB,IAAA,KAAK,mOAAA,EAAoB,WAAW,WAAW,CAAC;QACpE;QAED,iBAAA,CAAkB,2BAAA,KAAgC,CAAE;QACpD,IAAI,KAAK,EAAA,EACP,iBAAA,CAAkB,2BAAA,CAA4B,KAAK,OAAA,CAAA,GAAW,KAAK,EAAA;IAEtE,OAAA,IAAU,KAAK,IAAA,KAAS,aACvB,kBAAkB,SAAA,GAAY;aACrB,KAAK,IAAA,KAAS,oBAAoB;QAC3C,MAAM,aAAS,wLAAA,EAAoB,KAAK;QACxC,IAAI,QACF,WAAW,IAAA,CAAK,OAAO;aAEvB,mBAAmB,IAAA,KACjB,mOAAA,EAAoB,MAAM,6BAA6B,CACxD;IAEJ,OAAA,IAAU,KAAK,IAAA,KAAS,iBAAiB;QACxC,MAAM,aAAS,sLAAA,EAAkB,KAAK;QACtC,IAAI,QACF,WAAW,IAAA,CAAK,OAAO;aAEvB,mBAAmB,IAAA,KACjB,mOAAA,EAAoB,MAAM,0BAA0B,CACrD;IAEJ,OAAM;QACL,kBAAkB,YAAA,KAAiB,CAAE,CAAA;QACrC,kBAAkB,YAAA,CAAa,IAAA,CAAK,KAAK;IAC1C;IAGH,OAAO,IAAI,4KAAA,CAAU;QACnB,IAAI;QACJ;QACA;QACA;QACA,gBAAgB,qCAAqC,SAAS,KAAA,CAAM;QACpE;QACA;IACD;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6DD,MAAaC,kDAGT,CAAC,cAAc;IAEjB,MAAM,UAAA,CACJ,UAAU,OAAA,CAAQ,MAAA,GAAS,IACvB,UAAU,OAAA,CAAQ,MAAA,CAChB,CAAC,KAAK,SAAS;QACb,MAAM,OAAO,GAAA,CAAI,IAAI,MAAA,GAAS,EAAA;QAE9B,IAAI,KAAM,KAAA,KAAU,KAAK,KAAA,EACvB,KAAM,IAAA,IAAQ,KAAK,IAAA;aAEnB,IAAI,IAAA,CAAK,KAAK;QAEhB,OAAO;IACR,GACD;QAAC;YAAE,GAAG,UAAU,OAAA,CAAQ,EAAA;QAAI,CAAC;KAAA,CAC9B,GACD,UAAU,OAAA,EACd,GAAA,CAAI,CAAC,IACL,OAAO,WAAA,CAAY,OAAO,OAAA,CAAQ,EAAE,CAAC,MAAA,CAAO,CAAC,CAAC,EAAE,GAAK,MAAM,QAAQ,CAAC,CACrE;IAED,OAAO;QACL,GAAG,SAAA;QACH;IACD;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAyDD,MAAaC,6CAGT,CAAC,UAAU;IACb,MAAMC,UAAqC,CAAE,CAAA;IAC7C,IAAIC,iBAA0C,CAAE;IAChD,IAAIC;IACJ,MAAMC,mBAAoC,CAAE,CAAA;IAC5C,MAAMT,oBAA6C;QACjD,gBAAgB;IACjB;IACD,MAAMU,oBAIF,CAAE;IACN,IAAIC;IACJ,IAAI,MAAM,IAAA,KAAS,8BACjB,QAAQ,IAAA,CAAK;QACX,MAAM;QACN,MAAM,MAAM,KAAA;QACZ,OAAO,MAAM,aAAA;IACd,EAAC;aACO,MAAM,IAAA,KAAS,yCACxB,QAAQ,IAAA,CAAK;QACX,MAAM;QACN,MAAM;QACN,aAAa;YAAC,MAAM,UAAW;SAAA;QAC/B,OAAO,MAAM,aAAA;IACd,EAAC;aAEF,MAAM,IAAA,KAAS,gCACf,MAAM,IAAA,CAAK,IAAA,KAAS,WAEpB,KAAK,MAAM,IAAA,CAAK,EAAA;aAEhB,MAAM,IAAA,KAAS,gCACf,MAAM,IAAA,CAAK,IAAA,KAAS,iBACpB;QACA,iBAAiB,IAAA,CAAK;YACpB,MAAM;YACN,MAAM,MAAM,IAAA,CAAK,IAAA;YACjB,MAAM,MAAM,IAAA,CAAK,SAAA;YACjB,IAAI,MAAM,IAAA,CAAK,OAAA;YACf,OAAO,MAAM,YAAA;QACd,EAAC;QAEF,iBAAA,CAAkB,2BAAA,GAA8B;YAAA,CAC7C,MAAM,IAAA,CAAK,OAAA,CAAA,EAAU,MAAM,IAAA,CAAK,EAAA;QAClC;IACF,OAAA,IACC,MAAM,IAAA,KAAS,+BACf,MAAM,IAAA,CAAK,IAAA,KAAS,iBACpB;QAEA,iBAAiB,IAAA,CAAK;YACpB,MAAM;YACN,MAAM;YACN,MAAM,KAAK,SAAA,CAAU;gBAAE,QAAQ,MAAM,IAAA,CAAK,MAAA;YAAQ,EAAC;YACnD,IAAI,MAAM,IAAA,CAAK,OAAA;YACf,OAAO,MAAM,YAAA;QACd,EAAC;QAEF,kBAAkB,YAAA,GAAe;YAAC,MAAM,IAAK;SAAA;IAC9C,OAAA,IACC,MAAM,IAAA,KAAS,+BACf;QACE;QACA;QACA;QACA;QACA;QACA;QACA;QACA;KACD,CAAC,QAAA,CAAS,MAAM,IAAA,CAAK,IAAA,CAAK,EAE3B,kBAAkB,YAAA,GAAe;QAAC,MAAM,IAAK;KAAA;aACpC,MAAM,IAAA,KAAS,oBAAoB;QAC5C,kBAAkB,EAAA,GAAK,MAAM,QAAA,CAAS,EAAA;QACtC,kBAAkB,UAAA,GAAa,MAAM,QAAA,CAAS,KAAA;QAC9C,kBAAkB,KAAA,GAAQ,MAAM,QAAA,CAAS,KAAA;IAC1C,OAAA,IAAU,MAAM,IAAA,KAAS,sBAAsB;QAC9C,MAAM,MAAM,mCAAmC,MAAM,QAAA,CAAS;QAE9D,iBAAiB,qCAAqC,MAAM,QAAA,CAAS,KAAA,CAAM;QAE3E,IAAI,MAAM,QAAA,CAAS,IAAA,EAAM,QAAQ,SAAS,eACxC,kBAAkB,MAAA,KAAW,KAAK,KAAA,CAAM,IAAI,IAAA,CAAK;QAEnD,KAAK,MAAM,CAAC,KAAK,MAAM,IAAI,OAAO,OAAA,CAAQ,MAAM,QAAA,CAAS,CACvD,IAAI,QAAQ,MAAM,iBAAA,CAAkB,IAAA,GAAO;IAE9C,OAAA,IACC,MAAM,IAAA,KAAS,4CACf,MAAM,IAAA,KAAS,yCAEf,iBAAiB,IAAA,CAAK;QACpB,MAAM;QACN,MAAM,MAAM,KAAA;QACZ,OAAO,MAAM,YAAA;IACd,EAAC;aAEF,MAAM,IAAA,KAAS,wCACf,MAAM,IAAA,KAAS,uCAEf,iBAAiB;QACf,cAAc;YACZ,IAAI,MAAM,OAAA;YACV,MAAM,MAAM,IAAA,CAAK,OAAA,CAAQ,aAAa,GAAG,CAAC,OAAA,CAAQ,cAAc,GAAG;YACnE,QAAQ;QACT;IACF;aACQ,MAAM,IAAA,KAAS,yBACxB,kBAAkB,OAAA,GAAU,MAAM,OAAA;aAElC,MAAM,IAAA,KAAS,gCACf,UAAU,SACV,MAAM,IAAA,CAAK,IAAA,KAAS,aACpB;QACA,MAAMC,UAA6D,MAChE,IAAA,CAAK,OAAA,GACJ,MAAM,IAAA,CAAK,OAAA,CAAQ,GAAA,CAAI,CAAC,GAAG,QAAA,CAAW;gBACpC,GAAG,CAAA;gBACH;YACD,CAAA,EAAE,GACH,KAAA;QAEJ,kBAAkB,SAAA,GAAY;YAI5B,IAAI,MAAM,IAAA,CAAK,EAAA;YACf,MAAM,MAAM,IAAA,CAAK,IAAA;YACjB,GAAI,UAAU;gBAAE;YAAS,IAAG,CAAE,CAAA;QAC/B;IACF,OAAA,IAAU,MAAM,IAAA,KAAS,yCACxB,kBAAkB,SAAA,GAAY;QAC5B,MAAM;QACN,SAAS;YAAC;gBAAE,GAAG,MAAM,IAAA;gBAAM,OAAO,MAAM,aAAA;YAAe,CAAC;SAAA;IACzD;aACQ,MAAM,IAAA,KAAS,yCACxB,kBAAkB,SAAA,GAAY;QAC5B,MAAM;QACN,SAAS;YACP;gBACE,MAAM,MAAM,KAAA;gBACZ,MAAM;gBACN,OAAO,MAAM,aAAA;YACd,CACF;SAAA;IACF;aACQ,MAAM,IAAA,KAAS,+CAGxB,CAAA,OAAO;SAEP,OAAO;IAGT,OAAO,IAAI,+KAAA,CAAoB;QAE7B,MAAM,QAAQ,GAAA,CAAI,CAAC,OAAS,KAAK,IAAA,CAAK,CAAC,IAAA,CAAK,GAAG;QAC/C,SAAS,IAAI,iLAAA,CAAe;YAC1B;YACS;YACT;YACA;YACA;YACA;QACD;QACD;IACD;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAyCD,MAAaC,gDAGT,CAAC,YAAY;IACf,MAAM,qBACJ,4KAAA,CAAU,UAAA,CAAW,QAAQ,IAC7B,QAAQ,iBAAA,EAAmB,mBAAmB;IAEhD,UAAU,eAAoE;QAC5E,MAAM,kBAAc,wKAAA,EAAK,MAAM;YAC7B,IAAI;gBACF,MAAM,WAAO,uLAAA,EAAoB,QAAQ;gBACzC,IACE,SAAS,YACT,SAAS,eACT,SAAS,eACT,SAAS,OAET,CAAA,OAAO;gBAET,OAAO;YACR,EAAA,OAAO;gBACN,OAAO;YACR;QACF,EAAC;QAEF,IAAIC,iBACF,KAAA;QAEF,MAAM,4BAAA,aAAA,GAA4B,IAAI;QACtC,MAAM,kCAAA,aAAA,GAAkC,IAAI;QAE5C,MAAM,wBAAA,aAAA,GAAwB,IAAI;QAIlC,MAAM,8BAAA,aAAA,GAA8B,IAAI;QAKxC,UAAU,eAAe;YACvB,IAAI,CAAC,eAAgB,CAAA;YACrB,MAAM,UAAU,eAAe,OAAA;YAC/B,IACG,OAAO,YAAY,YAAY,QAAQ,MAAA,GAAS,KAChD,MAAM,OAAA,CAAQ,QAAQ,IAAI,QAAQ,MAAA,GAAS,GAE5C,MAAM;YAER,iBAAiB,KAAA;QAClB;QAED,MAAMC,qBAEM,CAAC,YAAY;YACvB,IAAI,CAAC,gBACH,iBAAiB;gBACf,MAAM;gBACN,MAAM;gBACN,SAAS,CAAE,CAAA;YACZ;YAEH,IAAI,OAAO,eAAe,OAAA,KAAY,UACpC,eAAe,OAAA,GACb,eAAe,OAAA,CAAQ,MAAA,GAAS,IAC5B;gBAAC;oBAAE,MAAM;oBAAc,MAAM,eAAe,OAAA;gBAAS,GAAE;mBAAG,OAAQ;aAAA,GAClE,CAAC;mBAAG,OAAQ;aAAA;iBAElB,eAAe,OAAA,CAAQ,IAAA,CAAK,GAAG,QAAQ;QAE1C;QAED,MAAM,eAAe,CAACC,UAAmB;YACvC,IAAI,OAAO,UAAU,SACnB,CAAA,OAAO;YAET,IAAI;gBACF,OAAO,KAAK,SAAA,CAAU,SAAS,CAAE,EAAC;YACnC,EAAA,OAAO;gBACN,OAAO;YACR;QACF;QAED,MAAM,mBAAmB,CACvBC,UAC0D;YAC1D,MAAM,aAAS,wKAAA,EAAK,MAAM;gBACxB,MAAM,MAAM,MAAM,QAAA,EAAU;gBAC5B,IAAI,QAAQ,SAAS,QAAQ,UAAU,QAAQ,OAC7C,CAAA,OAAO;gBAET,OAAO;YACR,EAAC;YACF,IAAI,MAAM,MAAA,CACR,CAAA,OAAO;gBACL,MAAM;gBACN;gBACA,SAAS,MAAM,MAAA;YAChB;YAEH,IAAI,MAAM,GAAA,CACR,CAAA,OAAO;gBACL,MAAM;gBACN;gBACA,WAAW,MAAM,GAAA;YAClB;YAEH,IAAI,MAAM,IAAA,EAAM;gBACd,MAAM,aACJ,OAAO,MAAM,IAAA,KAAS,WAClB,MAAM,IAAA,GACN,OAAO,IAAA,CAAK,MAAM,IAAA,CAAK,CAAC,QAAA,CAAS,SAAS;gBAChD,MAAM,WAAW,MAAM,QAAA,IAAY;gBACnC,OAAO;oBACL,MAAM;oBACN;oBACA,WAAW,CAAC,KAAK,EAAE,SAAS,QAAQ,EAAE,YAAY;gBACnD;YACF;YACD,OAAO,KAAA;QACR;QAED,MAAM,kBAAkB,CACtBC,UACyD;YACzD,MAAM,eAAW,mMAAA,EAAgC,MAAM;YAEvD,IAAI,MAAM,MAAA,IAAU,OAAO,aAAa,SACtC,CAAA,OAAO;gBACL,MAAM;gBACN,SAAS,MAAM,MAAA;gBACf,GAAI,WAAW;oBAAE;gBAAU,IAAG,CAAE,CAAA;YACjC;YAEH,IAAI,MAAM,GAAA,IAAO,OAAO,aAAa,SACnC,CAAA,OAAO;gBACL,MAAM;gBACN,UAAU,MAAM,GAAA;gBAChB,GAAI,WAAW;oBAAE;gBAAU,IAAG,CAAE,CAAA;YACjC;YAEH,IAAI,MAAM,IAAA,IAAQ,OAAO,aAAa,UAAU;gBAC9C,MAAM,UACJ,OAAO,MAAM,IAAA,KAAS,WAClB,MAAM,IAAA,GACN,OAAO,IAAA,CAAK,MAAM,IAAA,CAAK,CAAC,QAAA,CAAS,SAAS;gBAChD,MAAM,WAAW,MAAM,QAAA,IAAY;gBACnC,OAAO;oBACL,MAAM;oBACN,WAAW,CAAC,KAAK,EAAE,SAAS,QAAQ,EAAE,SAAS;oBAC/C,GAAI,WAAW;wBAAE;oBAAU,IAAG,CAAE,CAAA;gBACjC;YACF;YACD,OAAO,KAAA;QACR;QAED,MAAM,wBAAwB,CAC5BC,UACiD;YACjD,MAAM,qBAAiB,wKAAA,EAAK,MAAM;gBAChC,IAAI,MAAM,OAAA,CAAQ,MAAM,OAAA,CAAQ,EAAE;oBAChC,MAAM,YAAY,MAAM,OAAA;oBACxB,MAAM,SACJ,WACI,IAAI,CAAC,OAAS,MAAM,KAAK,CAC1B,OAAO,CAAC,OAAyB,OAAO,SAAS,SAAS,IAAI,CAAE,CAAA;oBACrE,IAAI,OAAO,MAAA,GAAS,EAClB,CAAA,OAAO;gBAEV;gBACD,OAAO,MAAM,SAAA,GAAY;oBAAC,MAAM,SAAU;iBAAA,GAAG,CAAE,CAAA;YAChD,EAAC;YAEF,MAAM,UACJ,eAAe,MAAA,GAAS,IACpB,eAAe,GAAA,CAAI,CAAC,OAAA,CAAU;oBAC5B,MAAM;oBACN;gBACD,CAAA,EAAE,GACH;gBAAC;oBAAE,MAAM;oBAAyB,MAAM;gBAAI,CAAC;aAAA;YAEnD,MAAMC,gBAA8D;gBAClE,MAAM;gBACN,IAAI,MAAM,EAAA,IAAM;gBAChB;YACD;YAED,IAAI,MAAM,SAAA,EACR,cAAc,OAAA,GAAU;gBACtB;oBACE,MAAM;oBACN,MAAM,MAAM,SAAA;gBACb,CACF;aAAA;YAEH,OAAO;QACR;QAED,MAAM,sBAAsB,CAC1BC,QAAAA,CACqD;gBACrD,MAAM;gBACN,MAAM,MAAM,IAAA,IAAQ;gBACpB,SAAS,MAAM,EAAA,IAAM;gBACrB,WAAW,aAAa,MAAM,IAAA,CAAK;YACpC,CAAA;QAED,MAAM,4BAA4B,CAChCC,UACgE;YAChE,MAAM,SAAS,aAAa,MAAM,MAAA,CAAO;YACzC,MAAM,SACJ,MAAM,MAAA,KAAW,YACb,cACA,MAAM,MAAA,KAAW,UACjB,eACA,KAAA;YACN,OAAO;gBACL,MAAM;gBACN,SAAS,MAAM,UAAA,IAAc;gBAC7B;gBACA,GAAI,SAAS;oBAAE;gBAAQ,IAAG,CAAE,CAAA;YAC7B;QACF;QAED,KAAK,MAAM,SAAS,QAAQ,aAAA,CAC1B,IAAI,MAAM,IAAA,KAAS,QACjB,mBAAmB;YAAC;gBAAE,MAAM;gBAAc,MAAM,MAAM,IAAA;YAAM,CAAC;SAAA,CAAC;iBACrD,MAAM,IAAA,KAAS,qBAAqB,CAE9C,OAAA,IAAU,MAAM,IAAA,KAAS,aAAa;YACrC,OAAO,cAAc;YACrB,MAAM,sBACJ,MACD;QACF,OAAA,IAAU,MAAM,IAAA,KAAS,aAAa;YACrC,OAAO,cAAc;YACrB,MAAM,KAAK,MAAM,EAAA,IAAM;YACvB,IAAI,IAAI;gBACN,0BAA0B,GAAA,CAAI,GAAG;gBACjC,sBAAsB,MAAA,CAAO,GAAG;YACjC;YACD,MAAM,oBACJ,MACD;QACF,OAAA,IAAU,MAAM,IAAA,KAAS,mBACxB;gBAAI,MAAM,EAAA,EAAI;gBACZ,MAAM,WAAW,sBAAsB,GAAA,CAAI,MAAM,EAAA,CAAG,IAAI;oBACtD,MAAM,MAAM,IAAA;oBACZ,MAAM,CAAE,CAAA;gBACT;gBACD,IAAI,MAAM,IAAA,EAAM,SAAS,IAAA,GAAO,MAAM,IAAA;gBACtC,IAAI,MAAM,IAAA,EAAM,SAAS,IAAA,CAAK,IAAA,CAAK,MAAM,IAAA,CAAK;gBAC9C,sBAAsB,GAAA,CAAI,MAAM,EAAA,EAAI,SAAS;YAC9C;mBACQ,MAAM,IAAA,KAAS,oBAAoB;YAC5C,OAAO,cAAc;YACrB,MAAM,KAAK,MAAM,EAAA,IAAM;YACvB,IAAI,IAAI;gBACN,gCAAgC,GAAA,CAAI,GAAG;gBACvC,4BAA4B,MAAA,CAAO,GAAG;YACvC;YACD,MAAM,oBAAoB,MAAM;QACjC,OAAA,IAAU,MAAM,IAAA,KAAS,0BACxB;gBAAI,MAAM,EAAA,EAAI;gBACZ,MAAM,WAAW,4BAA4B,GAAA,CAAI,MAAM,EAAA,CAAG,IAAI;oBAC5D,MAAM,MAAM,IAAA;oBACZ,MAAM,CAAE,CAAA;gBACT;gBACD,IAAI,MAAM,IAAA,EAAM,SAAS,IAAA,GAAO,MAAM,IAAA;gBACtC,IAAI,MAAM,IAAA,EAAM,SAAS,IAAA,CAAK,IAAA,CAAK,MAAM,IAAA,CAAK;gBAC9C,4BAA4B,GAAA,CAAI,MAAM,EAAA,EAAI,SAAS;YACpD;mBACQ,MAAM,IAAA,KAAS,2BAA2B;YACnD,OAAO,cAAc;YACrB,MAAM,0BAA0B,MAAM;QACvC,OAAA,IAAU,MAAM,IAAA,KAAS,SAAS,CAElC,OAAA,IAAU,MAAM,IAAA,KAAS,QAAQ;YAChC,MAAM,WAAW,gBAAgB,MAAM;YACvC,IAAI,UACF,mBAAmB;gBAAC,QAAS;aAAA,CAAC;QAEjC,OAAA,IAAU,MAAM,IAAA,KAAS,SAAS;YACjC,MAAM,YAAY,iBAAiB,MAAM;YACzC,IAAI,WACF,mBAAmB;gBAAC,SAAU;aAAA,CAAC;QAElC,OAAA,IAAU,MAAM,IAAA,KAAS,SAAS;YACjC,MAAM,YAAY,gBAAgB,MAAM;YACxC,IAAI,WACF,mBAAmB;gBAAC,SAAU;aAAA,CAAC;QAElC,OAAA,IAAU,MAAM,IAAA,KAAS,cACxB;gBAAI,MAAM,IAAA,EACR,mBAAmB;gBACjB;oBACE,MAAM;oBACN,MAAM,MAAM,IAAA;gBACb,CACF;aAAA,CAAC;QACH,OAAA,IACQ,MAAM,IAAA,KAAS,kBAAkB,oBAAoB;YAC9D,OAAO,cAAc;YACrB,MAAM,MAAM,KAAA;QACb;QAEH,OAAO,cAAc;QAErB,KAAK,MAAM,CAAC,IAAI,MAAM,IAAI,sBAAuB;YAC/C,IAAI,CAAC,MAAM,0BAA0B,GAAA,CAAI,GAAG,CAAE,CAAA;YAC9C,MAAM,OAAO,MAAM,IAAA,CAAK,IAAA,CAAK,GAAG;YAChC,IAAI,CAAC,MAAM,IAAA,IAAQ,CAAC,KAAM,CAAA;YAC1B,MAAM;gBACJ,MAAM;gBACN,SAAS;gBACT,MAAM,MAAM,IAAA,IAAQ;gBACpB,WAAW;YACZ;QACF;QAED,KAAK,MAAM,CAAC,IAAI,MAAM,IAAI,4BAA6B;YACrD,IAAI,CAAC,MAAM,gCAAgC,GAAA,CAAI,GAAG,CAAE,CAAA;YACpD,MAAM,OAAO,MAAM,IAAA,CAAK,IAAA,CAAK,GAAG;YAChC,IAAI,CAAC,MAAM,IAAA,IAAQ,CAAC,KAAM,CAAA;YAC1B,MAAM;gBACJ,MAAM;gBACN,SAAS;gBACT,MAAM,MAAM,IAAA,IAAQ;gBACpB,WAAW;YACZ;QACF;IACF;IACD,OAAO,MAAM,IAAA,CAAK,cAAc,CAAC;AAClC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAgCD,MAAaC,kCAGT,CAAC,EAAE,QAAA,EAAU,UAAA,EAAY,KAAA,EAAO,KAAK;IACvC,OAAO,SAAS,OAAA,CACd,CAAC,UAAqD;QACpD,MAAM,mBAAmB,MAAM,iBAAA;QAG/B,IAAI,kBAAkB,mBAAmB,KACvC,CAAA,OAAO,8CAA8C,MAAM;QAG7D,MAAM,oBAAoB,MAAM,iBAAA;QAQhC,IAAI,WAAO,uLAAA,EAAoB,MAAM;QACrC,IAAI,SAAS,gBAAY,oLAAA,EAAiB,MAAM,EAAE,OAAO;QAEzD,IAAI,SAAS,WACX,CAAA,MAAM,IAAI,MAAM;QAGlB,IAAI,SAAS,QAAQ;YACnB,MAAM,cAAc;YAGpB,IAAI,mBAAmB,SAAS,wBAAwB;gBACtD,MAAM,SAAA,CAAU,MAAM;oBACpB,IAAI,OAAO,YAAY,OAAA,KAAY,SACjC,CAAA,OAAO;wBACL,MAAM;wBACN,WAAW,YAAY,OAAA;oBACxB;oBAGH,IAAI,MAAM,OAAA,CAAQ,YAAY,OAAA,CAAQ,EAAE;;;SAItC,MAAM,aAAa,YAAY,OAAA,CAAQ,IAAA,CACrC,CAAC,IAAM,EAAE,IAAA,KAAS,cACnB;wBAED,IAAI,WAAY,CAAA,OAAO;;;SAKvB,MAAM,gBAAgB,YAAY,OAAA,CAAQ,IAAA,CACxC,CAAC,IAAM,EAAE,IAAA,KAAS,sBACnB;wBAID,IAAI,cAAe,CAAA,OAAO;;;SAK1B,MAAM,UAAU,YAAY,OAAA,CAAQ,IAAA,CAClC,CAAC,IAAM,EAAE,IAAA,KAAS,YACnB;wBAED,IAAI,QACF,CAAA,OAAO;4BACL,MAAM;4BACN,WACE,OAAO,QAAQ,SAAA,KAAc,WACzB,QAAQ,SAAA,GACR,QAAQ,SAAA,CAAU,GAAA;wBACzB;oBAEJ;oBAED,MAAM,IAAI,MAAM;gBACjB,CAAA,GAAG;;;;OAMJ,OAAO;oBACL,MAAM;oBACN;oBACA,SAAS,YAAY,YAAA;gBACtB;YACF;YAGD,IAAI,YAAY,iBAAA,EAAmB,WACjC,CAAA,OAAO;gBACL,MAAM;gBACN,SAAS,YAAY,YAAA;gBACrB,QAAQ,YAAY,OAAA;YACrB;YAKH,MAAM,0BACJ,MAAM,OAAA,CAAQ,YAAY,OAAA,CAAQ,IAClC,YAAY,OAAA,CAAQ,KAAA,CAClB,CAAC,OACC,OAAO,SAAS,YAChB,SAAS,QACT,UAAU,QAAA,CACT,KAAK,IAAA,KAAS,gBACb,KAAK,IAAA,KAAS,iBACd,KAAK,IAAA,KAAS,YAAA,EACnB;YAEH,OAAO;gBACL,MAAM;gBACN,SAAS,YAAY,YAAA;gBACrB,IAAI,YAAY,EAAA,EAAI,WAAW,MAAM,GAAG,YAAY,EAAA,GAAK,KAAA;gBACzD,QAAQ,0BACH,YAAY,OAAA,GACb,OAAO,YAAY,OAAA,KAAY,WAC/B,KAAK,SAAA,CAAU,YAAY,OAAA,CAAQ,GACnC,YAAY,OAAA;YACjB;QACF;QAED,IAAI,SAAS,aAAa;YAExB,IACE,CAAC,cACD,kBAAkB,UAAU,QAC5B,MAAM,OAAA,CAAQ,kBAAkB,OAAO,IACvC,kBAAkB,OAAO,SAAS,KAClC,kBAAkB,OAAO,MAAM,CAAC,OAAS,UAAU,KAAK,CAExD,CAAA,OAAO,kBAAkB;YAK3B,MAAMC,QAA8B,CAAE,CAAA;YAGtC,IAAI,mBAAmB,aAAa,CAAC,YAAY;gBAC/C,MAAM,gBAAgB,gDACpB,kBAAkB,SAAA,CACnB;gBACD,MAAM,IAAA,CAAK,cAAc;YAC1B;YAGD,IAAI,EAAE,OAAA,EAAS,GAAG;YAClB,IAAI,mBAAmB,SAAS;gBAC9B,IAAI,OAAO,YAAY,UACrB,UAAU;oBAAC;wBAAE,MAAM;wBAAe,MAAM;wBAAS,aAAa,CAAE,CAAA;oBAAE,CAAC;iBAAA;gBAErE,UAAU,CACR;uBAAI;oBACJ;wBAAE,MAAM;wBAAW,SAAS,kBAAkB,OAAA;oBAAS,CACxD;iBAAA;YACF;YAED,IAAI,OAAO,YAAY,YAAY,QAAQ,MAAA,GAAS,GAClD,MAAM,IAAA,CAAK;gBACT,MAAM;gBACN,MAAM;gBACN,GAAI,MAAM,EAAA,IAAM,CAAC,cAAc,MAAM,EAAA,CAAG,UAAA,CAAW,OAAO,GACtD;oBAAE,IAAI,MAAM,EAAA;gBAAI,IAChB,CAAE,CAAA;gBACN,aAAS,wKAAA,EAAK,MAAM;oBAClB,IAAI,OAAO,YAAY,SACrB,CAAA,OAAO;oBAET,OAAO,QAAQ,OAAA,CAAQ,CAAC,SAAS;wBAC/B,IAAI,KAAK,IAAA,KAAS,OAChB,CAAA,OAAO;4BACL,MAAM;4BACN,MAAM,KAAK,IAAA;4BACX,aAAa,KAAK,WAAA,IAAe,CAAE,CAAA;wBACpC;wBAGH,IAAI,KAAK,IAAA,KAAS,iBAAiB,KAAK,IAAA,KAAS,UAC/C,CAAA,OAAO;wBAGT,OAAO,CAAE,CAAA;oBACV,EAAC;gBACH,EAAC;YACH,EAAC;YAGJ,MAAM,kBAAkB,mBAAA,CAAoB,2BAAA;YAE5C,IAAI,4KAAA,CAAU,UAAA,CAAW,MAAM,IAAI,CAAC,CAAC,MAAM,UAAA,EAAY,QACrD,MAAM,IAAA,CACJ,GAAG,MAAM,UAAA,CAAW,GAAA,CAAI,CAAC,aAAiC;gBACxD,QAAI,qLAAA,EAAiB,SAAS,CAC5B,CAAA,OAAO;oBACL,MAAM;oBACN,IAAI,SAAS,OAAA;oBACb,SAAS,SAAS,EAAA,IAAM;oBACxB,OAAO,SAAS,IAAA,CAAK,KAAA;oBACrB,MAAM,SAAS,IAAA;gBAChB;gBAEH,QAAI,uLAAA,EAAmB,SAAS,CAC9B,CAAA,OAAO;oBACL,MAAM;oBACN,IAAI,SAAS,OAAA;oBACb,SAAS,SAAS,EAAA,IAAM;oBACxB,QAAQ,SAAS,IAAA,CAAK,MAAA;gBACvB;gBAEH,OAAO;oBACL,MAAM;oBACN,MAAM,SAAS,IAAA;oBACf,WAAW,KAAK,SAAA,CAAU,SAAS,IAAA,CAAK;oBACxC,SAAS,SAAS,EAAA;oBAClB,GAAI,CAAC,aAAa;wBAAE,IAAI,iBAAA,CAAkB,SAAS,EAAA,CAAA;oBAAM,IAAG,CAAE,CAAA;gBAC/D;YACF,EAAC,CACH;qBACQ,mBAAmB,YAC5B,MAAM,IAAA,CACJ,GAAG,kBAAkB,UAAA,CAAW,GAAA,CAC9B,CAAC,WAAA,CAAkC;oBACjC,MAAM;oBACN,MAAM,SAAS,QAAA,CAAS,IAAA;oBACxB,SAAS,SAAS,EAAA;oBAClB,WAAW,SAAS,QAAA,CAAS,SAAA;oBAC7B,GAAI,CAAC,aAAa;wBAAE,IAAI,iBAAA,CAAkB,SAAS,EAAA,CAAA;oBAAK,IAAG,CAAE,CAAA;gBAC9D,CAAA,EACF,CACF;YAGH,MAAM,cAAA,AACJ,kBAAkB,QACjB,SACC,kBAAkB,SAClB,kBAAkB,YAAA;YAEtB,MAAMC,uBAAqD;gBACzD;gBACA;gBACA;gBACA;aACD;YAED,IAAI,eAAe,MAAM;gBACvB,MAAM,kBAAkB;gBACxB,MAAM,mBAAmB,iBAAiB,OAAO,CAAC,OAChD,qBAAqB,QAAA,CAAS,KAAK,IAAA,CAAK,CACzC;gBACD,IAAI,iBAAiB,MAAA,GAAS,GAAG,MAAM,IAAA,CAAK,GAAG,iBAAiB;YACjE;YAED,OAAO;QACR;QAED,IAAI,SAAS,UAAU,SAAS,YAAY,SAAS,aAAa;YAChE,IAAI,OAAO,MAAM,OAAA,KAAY,SAC3B,CAAA,OAAO;gBAAE,MAAM;gBAAW;gBAAM,SAAS,MAAM,OAAA;YAAS;YAG1D,MAAMC,aAAiC,CAAE,CAAA;YACzC,MAAM,UAAW,MAAM,OAAA,CAA2B,OAAA,CAAQ,CAAC,SAAS;gBAClE,IAAI,KAAK,IAAA,KAAS,yBAChBC,WAAS,IAAA,CAAK;oBACZ,MAAM;oBACN,qBAAqB,KAAK,mBAAA;oBAC1B,SAAS,KAAK,OAAA;gBACf,EAAC;gBAEJ,QAAI,kMAAA,EAAmB,KAAK,CAC1B,CAAA,WAAO,6MAAA,EACL,MACA,mNAAA,CACD;gBAEH,IAAI,KAAK,IAAA,KAAS,OAChB,CAAA,OAAO;oBACL,MAAM;oBACN,MAAM,KAAK,IAAA;gBACZ;gBAEH,IAAI,KAAK,IAAA,KAAS,aAAa;oBAC7B,MAAM,eAAW,wKAAA,EAAK,MAAM;wBAC1B,IAAI,OAAO,KAAK,SAAA,KAAc,SAC5B,CAAA,OAAO,KAAK,SAAA;iCAEZ,OAAO,KAAK,SAAA,KAAc,YAC1B,KAAK,SAAA,KAAc,QACnB,SAAS,KAAK,SAAA,CAEd,CAAA,OAAO,KAAK,SAAA,CAAU,GAAA;wBAExB,OAAO,KAAA;oBACR,EAAC;oBACF,MAAM,aAAS,wKAAA,EAAK,MAAM;wBACxB,IAAI,OAAO,KAAK,SAAA,KAAc,SAC5B,CAAA,OAAO;iCAEP,OAAO,KAAK,SAAA,KAAc,YAC1B,KAAK,SAAA,KAAc,QACnB,YAAY,KAAK,SAAA,CAEjB,CAAA,OAAO,KAAK,SAAA,CAAU,MAAA;wBAExB,OAAO,KAAA;oBACR,EAAC;oBACF,OAAO;wBACL,MAAM;wBACN,WAAW;wBACX;oBACD;gBACF;gBACD,IACE,KAAK,IAAA,KAAS,gBACd,KAAK,IAAA,KAAS,iBACd,KAAK,IAAA,KAAS,aAEd,CAAA,OAAO;gBAET,OAAO,CAAE,CAAA;YACV,EAAC;YAEF,IAAI,QAAQ,MAAA,GAAS,GACnBA,WAAS,IAAA,CAAK;gBACZ,MAAM;gBACN;gBACS;YACV,EAAC;YAEJ,OAAOA;QACR;QAED,QAAQ,IAAA,CACN,CAAC,gEAAgE,EAAE,MAAM,CAC1E;QACD,OAAO,CAAE,CAAA;IACV,EACF;AACF"}},
    {"offset": {"line": 4023, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/chat_models/responses.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/chat_models/responses.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { AIMessage, type BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatResult } from \"@langchain/core/outputs\";\nimport { isOpenAITool as isOpenAIFunctionTool } from \"@langchain/core/language_models/base\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\nimport {\n  ChatOpenAIToolType,\n  convertCompletionsCustomTool,\n  formatToOpenAIToolChoice,\n  isBuiltInTool,\n  isBuiltInToolChoice,\n  isCustomTool,\n  isOpenAICustomTool,\n  ResponsesTool,\n} from \"../utils/tools.js\";\nimport { BaseChatOpenAI, BaseChatOpenAICallOptions } from \"./base.js\";\nimport {\n  convertMessagesToResponsesInput,\n  convertResponsesDeltaToChatGenerationChunk,\n  convertResponsesMessageToAIMessage,\n} from \"../converters/responses.js\";\nimport { OpenAIVerbosityParam } from \"../types.js\";\n\nexport interface ChatOpenAIResponsesCallOptions\n  extends BaseChatOpenAICallOptions {\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data.\n   */\n  text?: OpenAIClient.Responses.ResponseCreateParams[\"text\"];\n\n  /**\n   * The truncation strategy to use for the model response.\n   */\n  truncation?: OpenAIClient.Responses.ResponseCreateParams[\"truncation\"];\n\n  /**\n   * Specify additional output data to include in the model response.\n   */\n  include?: OpenAIClient.Responses.ResponseCreateParams[\"include\"];\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create multi-turn\n   * conversations.\n   */\n  previous_response_id?: OpenAIClient.Responses.ResponseCreateParams[\"previous_response_id\"];\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n}\n\nexport type ChatResponsesInvocationParams = Omit<\n  OpenAIClient.Responses.ResponseCreateParams,\n  \"input\"\n>;\n\n/**\n * OpenAI Responses API implementation.\n *\n * Will be exported in a later version of @langchain/openai.\n *\n * @internal\n */\nexport class ChatOpenAIResponses<\n  CallOptions extends ChatOpenAIResponsesCallOptions = ChatOpenAIResponsesCallOptions\n> extends BaseChatOpenAI<CallOptions> {\n  override invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): ChatResponsesInvocationParams {\n    let strict: boolean | undefined;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    }\n    if (strict === undefined && this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n\n    const params: ChatResponsesInvocationParams = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      user: this.user,\n\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      previous_response_id: options?.previous_response_id,\n      truncation: options?.truncation,\n      include: options?.include,\n      tools: options?.tools?.length\n        ? this._reduceChatOpenAITools(options.tools, {\n            stream: this.streaming,\n            strict,\n          })\n        : undefined,\n      tool_choice: isBuiltInToolChoice(options?.tool_choice)\n        ? options?.tool_choice\n        : (() => {\n            const formatted = formatToOpenAIToolChoice(options?.tool_choice);\n            if (typeof formatted === \"object\" && \"type\" in formatted) {\n              if (formatted.type === \"function\") {\n                return { type: \"function\", name: formatted.function.name };\n              } else if (formatted.type === \"allowed_tools\") {\n                return {\n                  type: \"allowed_tools\",\n                  mode: formatted.allowed_tools.mode,\n                  tools: formatted.allowed_tools.tools,\n                };\n              } else if (formatted.type === \"custom\") {\n                return {\n                  type: \"custom\",\n                  name: formatted.custom.name,\n                };\n              }\n            }\n            return undefined;\n          })(),\n      text: (() => {\n        if (options?.text) return options.text;\n        const format = this._getResponseFormat(options?.response_format);\n        if (format?.type === \"json_schema\") {\n          if (format.json_schema.schema != null) {\n            return {\n              format: {\n                type: \"json_schema\",\n                schema: format.json_schema.schema,\n                description: format.json_schema.description,\n                name: format.json_schema.name,\n                strict: format.json_schema.strict,\n              },\n              verbosity: options?.verbosity,\n            };\n          }\n          return undefined;\n        }\n        return { format, verbosity: options?.verbosity };\n      })(),\n      parallel_tool_calls: options?.parallel_tool_calls,\n      max_output_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n      prompt_cache_key: options?.promptCacheKey ?? this.promptCacheKey,\n      prompt_cache_retention:\n        options?.promptCacheRetention ?? this.promptCacheRetention,\n      ...(this.zdrEnabled ? { store: false } : {}),\n      ...this.modelKwargs,\n    };\n\n    const reasoning = this._getReasoningParams(options);\n\n    if (reasoning !== undefined) {\n      params.reasoning = reasoning;\n    }\n\n    return params;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const invocationParams = this.invocationParams(options);\n    if (invocationParams.stream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      let finalChunk: ChatGenerationChunk | undefined;\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata,\n        };\n        finalChunk = finalChunk?.concat(chunk) ?? chunk;\n      }\n\n      return {\n        generations: finalChunk ? [finalChunk] : [],\n        llmOutput: {\n          estimatedTokenUsage: (finalChunk?.message as AIMessage | undefined)\n            ?.usage_metadata,\n        },\n      };\n    } else {\n      const data = await this.completionWithRetry(\n        {\n          input: convertMessagesToResponsesInput({\n            messages,\n            zdrEnabled: this.zdrEnabled ?? false,\n            model: this.model,\n          }),\n          ...invocationParams,\n          stream: false,\n        },\n        { signal: options?.signal, ...options?.options }\n      );\n\n      return {\n        generations: [\n          {\n            text: data.output_text,\n            message: convertResponsesMessageToAIMessage(data),\n          },\n        ],\n        llmOutput: {\n          id: data.id,\n          estimatedTokenUsage: data.usage\n            ? {\n                promptTokens: data.usage.input_tokens,\n                completionTokens: data.usage.output_tokens,\n                totalTokens: data.usage.total_tokens,\n              }\n            : undefined,\n        },\n      };\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const streamIterable = await this.completionWithRetry(\n      {\n        ...this.invocationParams(options),\n        input: convertMessagesToResponsesInput({\n          messages,\n          zdrEnabled: this.zdrEnabled ?? false,\n          model: this.model,\n        }),\n        stream: true,\n      },\n      options\n    );\n\n    for await (const data of streamIterable) {\n      const chunk = convertResponsesDeltaToChatGenerationChunk(data);\n      if (chunk == null) continue;\n      yield chunk;\n      await runManager?.handleLLMNewToken(\n        chunk.text || \"\",\n        {\n          prompt: options.promptIndex ?? 0,\n          completion: 0,\n        },\n        undefined,\n        undefined,\n        undefined,\n        { chunk }\n      );\n    }\n  }\n\n  /**\n   * Calls the Responses API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsNonStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<OpenAIClient.Responses.Response>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParams,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<\n    | AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>\n    | OpenAIClient.Responses.Response\n  > {\n    return this.caller.call(async () => {\n      const clientOptions = this._getClientOptions(requestOptions);\n      try {\n        // use parse if dealing with json_schema\n        if (request.text?.format?.type === \"json_schema\" && !request.stream) {\n          return await this.client.responses.parse(request, clientOptions);\n        }\n        return await this.client.responses.create(request, clientOptions);\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /** @internal */\n  protected _reduceChatOpenAITools(\n    tools: ChatOpenAIToolType[],\n    fields: { stream?: boolean; strict?: boolean }\n  ): ResponsesTool[] {\n    const reducedTools: ResponsesTool[] = [];\n    for (const tool of tools) {\n      if (isBuiltInTool(tool)) {\n        if (tool.type === \"image_generation\" && fields?.stream) {\n          // OpenAI sends a 400 error if partial_images is not set and we want to stream.\n          // We also set it to 1 since we don't support partial images yet.\n          tool.partial_images = 1;\n        }\n        reducedTools.push(tool);\n      } else if (isCustomTool(tool)) {\n        const customToolData = tool.metadata.customTool;\n        reducedTools.push({\n          type: \"custom\",\n          name: customToolData.name,\n          description: customToolData.description,\n          format: customToolData.format,\n        } as ResponsesTool);\n      } else if (isOpenAIFunctionTool(tool)) {\n        reducedTools.push({\n          type: \"function\",\n          name: tool.function.name,\n          parameters: tool.function.parameters,\n          description: tool.function.description,\n          strict: fields?.strict ?? null,\n        });\n      } else if (isOpenAICustomTool(tool)) {\n        reducedTools.push(convertCompletionsCustomTool(tool));\n      }\n    }\n    return reducedTools;\n  }\n}\n"],"names":["options?: this[\"ParsedCallOptions\"]","strict: boolean | undefined","params: ChatResponsesInvocationParams","messages: BaseMessage[]","options: this[\"ParsedCallOptions\"]","runManager?: CallbackManagerForLLMRun","finalChunk: ChatGenerationChunk | undefined","request: OpenAIClient.Responses.ResponseCreateParams","requestOptions?: OpenAIClient.RequestOptions","tools: ChatOpenAIToolType[]","fields: { stream?: boolean; strict?: boolean }","reducedTools: ResponsesTool[]","isOpenAIFunctionTool"],"mappings":";;;;;;;;;;;;;;;;;;;;;GAkEA,IAAa,sBAAb,cAEU,wLAAA,CAA4B;IAC3B,iBACPA,OAAAA,EAC+B;QAC/B,IAAIC;QACJ,IAAI,SAAS,WAAW,KAAA,GACtB,SAAS,QAAQ,MAAA;QAEnB,IAAI,WAAW,KAAA,KAAa,IAAA,CAAK,yBAAA,KAA8B,KAAA,GAC7D,SAAS,IAAA,CAAK,yBAAA;QAGhB,MAAMC,SAAwC;YAC5C,OAAO,IAAA,CAAK,KAAA;YACZ,aAAa,IAAA,CAAK,WAAA;YAClB,OAAO,IAAA,CAAK,IAAA;YACZ,MAAM,IAAA,CAAK,IAAA;YAGX,QAAQ,IAAA,CAAK,SAAA;YACb,sBAAsB,SAAS;YAC/B,YAAY,SAAS;YACrB,SAAS,SAAS;YAClB,OAAO,SAAS,OAAO,SACnB,IAAA,CAAK,sBAAA,CAAuB,QAAQ,KAAA,EAAO;gBACzC,QAAQ,IAAA,CAAK,SAAA;gBACb;YACD,EAAC,GACF,KAAA;YACJ,iBAAa,wLAAA,EAAoB,SAAS,YAAY,GAClD,SAAS,cAAA,CACR,MAAM;gBACL,MAAM,gBAAY,6LAAA,EAAyB,SAAS,YAAY;gBAChE,IAAI,OAAO,cAAc,YAAY,UAAU,WAC7C;wBAAI,UAAU,IAAA,KAAS,WACrB,CAAA,OAAO;wBAAE,MAAM;wBAAY,MAAM,UAAU,QAAA,CAAS,IAAA;oBAAM;6BACjD,UAAU,IAAA,KAAS,gBAC5B,CAAA,OAAO;wBACL,MAAM;wBACN,MAAM,UAAU,aAAA,CAAc,IAAA;wBAC9B,OAAO,UAAU,aAAA,CAAc,KAAA;oBAChC;6BACQ,UAAU,IAAA,KAAS,SAC5B,CAAA,OAAO;wBACL,MAAM;wBACN,MAAM,UAAU,MAAA,CAAO,IAAA;oBACxB;gBACF;gBAEH,OAAO,KAAA;YACR,CAAA,GAAG;YACR,MAAA,CAAO,MAAM;gBACX,IAAI,SAAS,KAAM,CAAA,OAAO,QAAQ,IAAA;gBAClC,MAAM,SAAS,IAAA,CAAK,kBAAA,CAAmB,SAAS,gBAAgB;gBAChE,IAAI,QAAQ,SAAS,eAAe;oBAClC,IAAI,OAAO,WAAA,CAAY,MAAA,IAAU,KAC/B,CAAA,OAAO;wBACL,QAAQ;4BACN,MAAM;4BACN,QAAQ,OAAO,WAAA,CAAY,MAAA;4BAC3B,aAAa,OAAO,WAAA,CAAY,WAAA;4BAChC,MAAM,OAAO,WAAA,CAAY,IAAA;4BACzB,QAAQ,OAAO,WAAA,CAAY,MAAA;wBAC5B;wBACD,WAAW,SAAS;oBACrB;oBAEH,OAAO,KAAA;gBACR;gBACD,OAAO;oBAAE;oBAAQ,WAAW,SAAS;gBAAW;YACjD,CAAA,GAAG;YACJ,qBAAqB,SAAS;YAC9B,mBAAmB,IAAA,CAAK,SAAA,KAAc,CAAA,IAAK,KAAA,IAAY,IAAA,CAAK,SAAA;YAC5D,kBAAkB,SAAS,kBAAkB,IAAA,CAAK,cAAA;YAClD,wBACE,SAAS,wBAAwB,IAAA,CAAK,oBAAA;YACxC,GAAI,IAAA,CAAK,UAAA,GAAa;gBAAE,OAAO;YAAO,IAAG,CAAE,CAAA;YAC3C,GAAG,IAAA,CAAK,WAAA;QACT;QAED,MAAM,YAAY,IAAA,CAAK,mBAAA,CAAoB,QAAQ;QAEnD,IAAI,cAAc,KAAA,GAChB,OAAO,SAAA,GAAY;QAGrB,OAAO;IACR;IAED,MAAM,UACJC,QAAAA,EACAC,OAAAA,EACAC,UAAAA,EACqB;QACrB,MAAM,mBAAmB,IAAA,CAAK,gBAAA,CAAiB,QAAQ;QACvD,IAAI,iBAAiB,MAAA,EAAQ;YAC3B,MAAM,SAAS,IAAA,CAAK,qBAAA,CAAsB,UAAU,SAAS,WAAW;YACxE,IAAIC;YACJ,WAAW,MAAM,SAAS,OAAQ;gBAChC,MAAM,OAAA,CAAQ,iBAAA,GAAoB;oBAChC,GAAG,MAAM,cAAA;oBACT,GAAG,MAAM,OAAA,CAAQ,iBAAA;gBAClB;gBACD,aAAa,YAAY,OAAO,MAAM,IAAI;YAC3C;YAED,OAAO;gBACL,aAAa,aAAa;oBAAC,UAAW;iBAAA,GAAG,CAAE,CAAA;gBAC3C,WAAW;oBACT,qBAAsB,YAAY,SAC9B;gBACL;YACF;QACF,OAAM;YACL,MAAM,OAAO,MAAM,IAAA,CAAK,mBAAA,CACtB;gBACE,WAAO,6MAAA,EAAgC;oBACrC;oBACA,YAAY,IAAA,CAAK,UAAA,IAAc;oBAC/B,OAAO,IAAA,CAAK,KAAA;gBACb,EAAC;gBACF,GAAG,gBAAA;gBACH,QAAQ;YACT,GACD;gBAAE,QAAQ,SAAS;gBAAQ,GAAG,SAAS,OAAA;YAAS,EACjD;YAED,OAAO;gBACL,aAAa;oBACX;wBACE,MAAM,KAAK,WAAA;wBACX,aAAS,gNAAA,EAAmC,KAAK;oBAClD,CACF;iBAAA;gBACD,WAAW;oBACT,IAAI,KAAK,EAAA;oBACT,qBAAqB,KAAK,KAAA,GACtB;wBACE,cAAc,KAAK,KAAA,CAAM,YAAA;wBACzB,kBAAkB,KAAK,KAAA,CAAM,aAAA;wBAC7B,aAAa,KAAK,KAAA,CAAM,YAAA;oBACzB,IACD,KAAA;gBACL;YACF;QACF;IACF;IAED,OAAO,sBACLH,QAAAA,EACAC,OAAAA,EACAC,UAAAA,EACqC;QACrC,MAAM,iBAAiB,MAAM,IAAA,CAAK,mBAAA,CAChC;YACE,GAAG,IAAA,CAAK,gBAAA,CAAiB,QAAQ;YACjC,WAAO,6MAAA,EAAgC;gBACrC;gBACA,YAAY,IAAA,CAAK,UAAA,IAAc;gBAC/B,OAAO,IAAA,CAAK,KAAA;YACb,EAAC;YACF,QAAQ;QACT,GACD,QACD;QAED,WAAW,MAAM,QAAQ,eAAgB;YACvC,MAAM,YAAQ,wNAAA,EAA2C,KAAK;YAC9D,IAAI,SAAS,KAAM,CAAA;YACnB,MAAM;YACN,MAAM,YAAY,kBAChB,MAAM,IAAA,IAAQ,IACd;gBACE,QAAQ,QAAQ,WAAA,IAAe;gBAC/B,YAAY;YACb,GACD,KAAA,GACA,KAAA,GACA,KAAA,GACA;gBAAE;YAAO,EACV;QACF;IACF;IAkBD,MAAM,oBACJE,OAAAA,EACAC,cAAAA,EAIA;QACA,OAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,YAAY;YAClC,MAAM,gBAAgB,IAAA,CAAK,iBAAA,CAAkB,eAAe;YAC5D,IAAI;gBAEF,IAAI,QAAQ,IAAA,EAAM,QAAQ,SAAS,iBAAiB,CAAC,QAAQ,MAAA,CAC3D,CAAA,OAAO,MAAM,IAAA,CAAK,MAAA,CAAO,SAAA,CAAU,KAAA,CAAM,SAAS,cAAc;gBAElE,OAAO,MAAM,IAAA,CAAK,MAAA,CAAO,SAAA,CAAU,MAAA,CAAO,SAAS,cAAc;YAClE,EAAA,OAAQ,GAAG;gBACV,MAAM,YAAQ,2LAAA,EAAsB,EAAE;gBACtC,MAAM;YACP;QACF,EAAC;IACH;qBAGS,uBACRC,KAAAA,EACAC,MAAAA,EACiB;QACjB,MAAMC,eAAgC,CAAE,CAAA;QACxC,KAAK,MAAM,QAAQ,MACjB,QAAI,kLAAA,EAAc,KAAK,EAAE;YACvB,IAAI,KAAK,IAAA,KAAS,sBAAsB,QAAQ,QAG9C,KAAK,cAAA,GAAiB;YAExB,aAAa,IAAA,CAAK,KAAK;QACxB,OAAA,QAAU,iLAAA,EAAa,KAAK,EAAE;YAC7B,MAAM,iBAAiB,KAAK,QAAA,CAAS,UAAA;YACrC,aAAa,IAAA,CAAK;gBAChB,MAAM;gBACN,MAAM,eAAe,IAAA;gBACrB,aAAa,eAAe,WAAA;gBAC5B,QAAQ,eAAe,MAAA;YACxB,EAAkB;QACpB,OAAA,QAAUC,wLAAAA,EAAqB,KAAK,EACnC,aAAa,IAAA,CAAK;YAChB,MAAM;YACN,MAAM,KAAK,QAAA,CAAS,IAAA;YACpB,YAAY,KAAK,QAAA,CAAS,UAAA;YAC1B,aAAa,KAAK,QAAA,CAAS,WAAA;YAC3B,QAAQ,QAAQ,UAAU;QAC3B,EAAC;qBACO,uLAAA,EAAmB,KAAK,EACjC,aAAa,IAAA,KAAK,iMAAA,EAA6B,KAAK,CAAC;QAGzD,OAAO;IACR;AACF"}},
    {"offset": {"line": 4230, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/azure/chat_models/responses.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/azure/chat_models/responses.ts"],"sourcesContent":["import { LangSmithParams } from \"@langchain/core/language_models/chat_models\";\nimport { Serialized } from \"@langchain/core/load/serializable\";\nimport {\n  ChatOpenAIResponsesCallOptions,\n  ChatOpenAIResponses,\n} from \"../../chat_models/responses.js\";\nimport { AzureOpenAIChatInput, OpenAICoreRequestOptions } from \"../../types.js\";\nimport {\n  _constructAzureFields,\n  _getAzureClientOptions,\n  _serializeAzureChat,\n  AZURE_ALIASES,\n  AZURE_SECRETS,\n  AZURE_SERIALIZABLE_KEYS,\n  AzureChatOpenAIFields,\n} from \"./common.js\";\n\nexport class AzureChatOpenAIResponses<\n    CallOptions extends ChatOpenAIResponsesCallOptions = ChatOpenAIResponsesCallOptions\n  >\n  extends ChatOpenAIResponses<CallOptions>\n  implements Partial<AzureOpenAIChatInput>\n{\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  _llmType(): string {\n    return \"azure_openai\";\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      ...AZURE_ALIASES,\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      ...AZURE_SECRETS,\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, ...AZURE_SERIALIZABLE_KEYS];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = super.getLsParams(options);\n    params.ls_provider = \"azure\";\n    return params;\n  }\n\n  constructor(fields?: AzureChatOpenAIFields) {\n    super(fields);\n    _constructAzureFields.call(this, fields);\n  }\n\n  override _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    return _getAzureClientOptions.call(this, options);\n  }\n\n  override toJSON(): Serialized {\n    return _serializeAzureChat.call(this, super.toJSON());\n  }\n}\n"],"names":["options: this[\"ParsedCallOptions\"]","fields?: AzureChatOpenAIFields","options: OpenAICoreRequestOptions | undefined"],"mappings":";;;;;;;;;AAiBA,IAAa,2BAAb,cAGU,kMAAA,CAEV;IACE,sBAAA;IAEA,kBAAA;IAEA,qBAAA;IAEA,2BAAA;IAEA,6BAAA;IAEA,oBAAA;IAEA,oBAAA;IAEA,WAAmB;QACjB,OAAO;IACR;IAED,IAAI,aAAqC;QACvC,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,GAAG,kMAAA;QACJ;IACF;IAED,IAAI,aAAoD;QACtD,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,GAAG,kMAAA;QACJ;IACF;IAED,IAAI,uBAAiC;QACnC,OAAO,CAAC;eAAG,KAAA,CAAM,sBAAsB;eAAG,4MAAwB;SAAA;IACnE;IAED,YAAYA,OAAAA,EAAqD;QAC/D,MAAM,SAAS,KAAA,CAAM,YAAY,QAAQ;QACzC,OAAO,WAAA,GAAc;QACrB,OAAO;IACR;IAED,YAAYC,MAAAA,CAAgC;QAC1C,KAAA,CAAM,OAAO;QACb,0MAAA,CAAsB,IAAA,CAAK,IAAA,EAAM,OAAO;IACzC;IAEQ,kBACPC,OAAAA,EAC0B;QAC1B,OAAO,2MAAA,CAAuB,IAAA,CAAK,IAAA,EAAM,QAAQ;IAClD;IAEQ,SAAqB;QAC5B,OAAO,wMAAA,CAAoB,IAAA,CAAK,IAAA,EAAM,KAAA,CAAM,QAAQ,CAAC;IACtD;AACF"}},
    {"offset": {"line": 4290, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/chat_models/index.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/chat_models/index.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatResult } from \"@langchain/core/outputs\";\nimport { type BaseLanguageModelInput } from \"@langchain/core/language_models/base\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { type OpenAICallOptions, type OpenAIChatInput } from \"../types.js\";\nimport {\n  _convertToOpenAITool,\n  isBuiltInTool,\n  isCustomTool,\n  isOpenAICustomTool,\n} from \"../utils/tools.js\";\nimport { _modelPrefersResponsesAPI } from \"../utils/misc.js\";\nimport { _convertOpenAIResponsesUsageToLangChainUsage } from \"../utils/output.js\";\nimport {\n  ChatOpenAICompletions,\n  ChatOpenAICompletionsCallOptions,\n} from \"./completions.js\";\nimport {\n  ChatOpenAIResponses,\n  ChatOpenAIResponsesCallOptions,\n} from \"./responses.js\";\nimport { BaseChatOpenAI, BaseChatOpenAIFields } from \"./base.js\";\n\nexport type { OpenAICallOptions, OpenAIChatInput };\n\nexport type ChatOpenAICallOptions = ChatOpenAICompletionsCallOptions &\n  ChatOpenAIResponsesCallOptions;\n\nexport interface ChatOpenAIFields extends BaseChatOpenAIFields {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi?: boolean;\n  /**\n   * The completions chat instance\n   * @internal\n   */\n  completions?: ChatOpenAICompletions;\n  /**\n   * The responses chat instance\n   * @internal\n   */\n  responses?: ChatOpenAIResponses;\n}\n\n/**\n * OpenAI chat model integration.\n *\n * To use with Azure, import the `AzureChatOpenAI` class.\n *\n * Setup:\n * Install `@langchain/openai` and set an environment variable named `OPENAI_API_KEY`.\n *\n * ```bash\n * npm install @langchain/openai\n * export OPENAI_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from '@langchain/openai';\n *\n * const llm = new ChatOpenAI({\n *   model: \"gpt-4o-mini\",\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // configuration: {\n *   //   baseURL: \"...\",\n *   // }\n *   // organization: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools(\n *   [GetWeather, GetPopulation],\n *   {\n *     // strict: true  // enforce tool args schema is respected\n *   }\n * );\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, {\n *   name: \"Joke\",\n *   strict: true, // Optionally enable OpenAI structured outputs\n * });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.withConfig({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ model: \"gpt-4o-mini\", logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Schema Structured Output</strong></summary>\n *\n * ```typescript\n * const llmForJsonSchema = new ChatOpenAI({\n *   model: \"gpt-4o-2024-08-06\",\n * }).withStructuredOutput(\n *   z.object({\n *     command: z.string().describe(\"The command to execute\"),\n *     expectedOutput: z.string().describe(\"The expected output of the command\"),\n *     options: z\n *       .array(z.string())\n *       .describe(\"The options you can pass to the command\"),\n *   }),\n *   {\n *     method: \"jsonSchema\",\n *     strict: true, // Optional when using the `jsonSchema` method\n *   }\n * );\n *\n * const jsonSchemaRes = await llmForJsonSchema.invoke(\n *   \"What is the command to list files in a directory?\"\n * );\n * console.log(jsonSchemaRes);\n * ```\n *\n * ```txt\n * {\n *   command: 'ls',\n *   expectedOutput: 'A list of files and subdirectories within the specified directory.',\n *   options: [\n *     '-a: include directory entries whose names begin with a dot (.).',\n *     '-l: use a long listing format.',\n *     '-h: with -l, print sizes in human readable format (e.g., 1K, 234M, 2G).',\n *     '-t: sort by time, newest first.',\n *     '-r: reverse order while sorting.',\n *     '-S: sort by file size, largest first.',\n *     '-R: list subdirectories recursively.'\n *   ]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castMessageContent = audioOutputResult.content[0] as Record<string, any>;\n *\n * console.log({\n *   ...castMessageContent,\n *   data: castMessageContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\n *\n * console.log({\n *   ...castAudioContent,\n *   data: castAudioContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatOpenAI<\n  CallOptions extends ChatOpenAICallOptions = ChatOpenAICallOptions\n> extends BaseChatOpenAI<CallOptions> {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi = false;\n\n  protected responses: ChatOpenAIResponses;\n\n  protected completions: ChatOpenAICompletions;\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, \"useResponsesApi\"];\n  }\n\n  get callKeys(): string[] {\n    return [...super.callKeys, \"useResponsesApi\"];\n  }\n\n  constructor(protected fields?: ChatOpenAIFields) {\n    super(fields);\n    this.useResponsesApi = fields?.useResponsesApi ?? false;\n    this.responses = fields?.responses ?? new ChatOpenAIResponses(fields);\n    this.completions = fields?.completions ?? new ChatOpenAICompletions(fields);\n  }\n\n  protected _useResponsesApi(options: this[\"ParsedCallOptions\"] | undefined) {\n    const usesBuiltInTools = options?.tools?.some(isBuiltInTool);\n    const hasResponsesOnlyKwargs =\n      options?.previous_response_id != null ||\n      options?.text != null ||\n      options?.truncation != null ||\n      options?.include != null ||\n      options?.reasoning?.summary != null ||\n      this.reasoning?.summary != null;\n    const hasCustomTools =\n      options?.tools?.some(isOpenAICustomTool) ||\n      options?.tools?.some(isCustomTool);\n\n    return (\n      this.useResponsesApi ||\n      usesBuiltInTools ||\n      hasResponsesOnlyKwargs ||\n      hasCustomTools ||\n      _modelPrefersResponsesAPI(this.model)\n    );\n  }\n\n  override getLsParams(options: this[\"ParsedCallOptions\"]) {\n    const optionsWithDefaults = this._combineCallOptions(options);\n    if (this._useResponsesApi(options)) {\n      return this.responses.getLsParams(optionsWithDefaults);\n    }\n    return this.completions.getLsParams(optionsWithDefaults);\n  }\n\n  override invocationParams(options?: this[\"ParsedCallOptions\"]) {\n    const optionsWithDefaults = this._combineCallOptions(options);\n    if (this._useResponsesApi(options)) {\n      return this.responses.invocationParams(optionsWithDefaults);\n    }\n    return this.completions.invocationParams(optionsWithDefaults);\n  }\n\n  /** @ignore */\n  override async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (this._useResponsesApi(options)) {\n      return this.responses._generate(messages, options);\n    }\n    return this.completions._generate(messages, options, runManager);\n  }\n\n  override async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    if (this._useResponsesApi(options)) {\n      yield* this.responses._streamResponseChunks(\n        messages,\n        this._combineCallOptions(options),\n        runManager\n      );\n      return;\n    }\n    yield* this.completions._streamResponseChunks(\n      messages,\n      this._combineCallOptions(options),\n      runManager\n    );\n  }\n\n  override withConfig(\n    config: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    const newModel = new ChatOpenAI<CallOptions>(this.fields);\n    newModel.defaultOptions = { ...this.defaultOptions, ...config };\n    return newModel;\n  }\n}\n"],"names":["fields?: ChatOpenAIFields","options: this[\"ParsedCallOptions\"] | undefined","options: this[\"ParsedCallOptions\"]","options?: this[\"ParsedCallOptions\"]","messages: BaseMessage[]","runManager?: CallbackManagerForLLMRun","config: Partial<CallOptions>"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA2kBA,IAAa,aAAb,MAAa,mBAEH,wLAAA,CAA4B;;;;IAKpC,kBAAkB,MAAA;IAER,UAAA;IAEA,YAAA;IAEV,IAAI,uBAAiC;QACnC,OAAO,CAAC;eAAG,KAAA,CAAM;YAAsB,iBAAkB;SAAA;IAC1D;IAED,IAAI,WAAqB;QACvB,OAAO,CAAC;eAAG,KAAA,CAAM;YAAU,iBAAkB;SAAA;IAC9C;IAED,YAAsBA,MAAAA,CAA2B;QAC/C,KAAA,CAAM,OAAO;QADO,IAAA,CAAA,MAAA,GAAA;QAEpB,IAAA,CAAK,eAAA,GAAkB,QAAQ,mBAAmB;QAClD,IAAA,CAAK,SAAA,GAAY,QAAQ,aAAa,IAAI,kMAAA,CAAoB;QAC9D,IAAA,CAAK,WAAA,GAAc,QAAQ,eAAe,IAAI,sMAAA,CAAsB;IACrE;IAES,iBAAiBC,OAAAA,EAAgD;QACzE,MAAM,mBAAmB,SAAS,OAAO,KAAK,kLAAA,CAAc;QAC5D,MAAM,yBACJ,SAAS,wBAAwB,QACjC,SAAS,QAAQ,QACjB,SAAS,cAAc,QACvB,SAAS,WAAW,QACpB,SAAS,WAAW,WAAW,QAC/B,IAAA,CAAK,SAAA,EAAW,WAAW;QAC7B,MAAM,iBACJ,SAAS,OAAO,KAAK,uLAAA,CAAmB,IACxC,SAAS,OAAO,KAAK,iLAAA,CAAa;QAEpC,OACE,IAAA,CAAK,eAAA,IACL,oBACA,0BACA,sBACA,6LAAA,EAA0B,IAAA,CAAK,KAAA,CAAM;IAExC;IAEQ,YAAYC,OAAAA,EAAoC;QACvD,MAAM,sBAAsB,IAAA,CAAK,mBAAA,CAAoB,QAAQ;QAC7D,IAAI,IAAA,CAAK,gBAAA,CAAiB,QAAQ,CAChC,CAAA,OAAO,IAAA,CAAK,SAAA,CAAU,WAAA,CAAY,oBAAoB;QAExD,OAAO,IAAA,CAAK,WAAA,CAAY,WAAA,CAAY,oBAAoB;IACzD;IAEQ,iBAAiBC,OAAAA,EAAqC;QAC7D,MAAM,sBAAsB,IAAA,CAAK,mBAAA,CAAoB,QAAQ;QAC7D,IAAI,IAAA,CAAK,gBAAA,CAAiB,QAAQ,CAChC,CAAA,OAAO,IAAA,CAAK,SAAA,CAAU,gBAAA,CAAiB,oBAAoB;QAE7D,OAAO,IAAA,CAAK,WAAA,CAAY,gBAAA,CAAiB,oBAAoB;IAC9D;mBAGD,MAAe,UACbC,QAAAA,EACAF,OAAAA,EACAG,UAAAA,EACqB;QACrB,IAAI,IAAA,CAAK,gBAAA,CAAiB,QAAQ,CAChC,CAAA,OAAO,IAAA,CAAK,SAAA,CAAU,SAAA,CAAU,UAAU,QAAQ;QAEpD,OAAO,IAAA,CAAK,WAAA,CAAY,SAAA,CAAU,UAAU,SAAS,WAAW;IACjE;IAED,OAAgB,sBACdD,QAAAA,EACAF,OAAAA,EACAG,UAAAA,EACqC;QACrC,IAAI,IAAA,CAAK,gBAAA,CAAiB,QAAQ,EAAE;YAClC,OAAO,IAAA,CAAK,SAAA,CAAU,qBAAA,CACpB,UACA,IAAA,CAAK,mBAAA,CAAoB,QAAQ,EACjC,WACD;YACD;QACD;QACD,OAAO,IAAA,CAAK,WAAA,CAAY,qBAAA,CACtB,UACA,IAAA,CAAK,mBAAA,CAAoB,QAAQ,EACjC,WACD;IACF;IAEQ,WACPC,MAAAA,EAC+D;QAC/D,MAAM,WAAW,IAAI,WAAwB,IAAA,CAAK,MAAA;QAClD,SAAS,cAAA,GAAiB;YAAE,GAAG,IAAA,CAAK,cAAA;YAAgB,GAAG,MAAA;QAAQ;QAC/D,OAAO;IACR;AACF"}},
    {"offset": {"line": 4912, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/azure/chat_models/index.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/azure/chat_models/index.ts"],"sourcesContent":["import { StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport type { Serialized } from \"@langchain/core/load/serializable\";\nimport { LangSmithParams } from \"@langchain/core/language_models/chat_models\";\nimport { ChatOpenAI, ChatOpenAICallOptions } from \"../../chat_models/index.js\";\nimport { AzureOpenAIChatInput } from \"../../types.js\";\nimport {\n  _constructAzureFields,\n  _serializeAzureChat,\n  AZURE_ALIASES,\n  AZURE_SECRETS,\n  AZURE_SERIALIZABLE_KEYS,\n  AzureChatOpenAIFields,\n} from \"./common.js\";\nimport { AzureChatOpenAICompletions } from \"./completions.js\";\nimport { AzureChatOpenAIResponses } from \"./responses.js\";\n\n/**\n * Azure OpenAI chat model integration.\n *\n * Setup:\n * Install `@langchain/openai` and set the following environment variables:\n *\n * ```bash\n * npm install @langchain/openai\n * export AZURE_OPENAI_API_KEY=\"your-api-key\"\n * export AZURE_OPENAI_API_DEPLOYMENT_NAME=\"your-deployment-name\"\n * export AZURE_OPENAI_API_VERSION=\"your-version\"\n * export AZURE_OPENAI_BASE_PATH=\"your-base-path\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { AzureChatOpenAI } from '@langchain/openai';\n *\n * const llm = new AzureChatOpenAI({\n *   azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n *   azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n *   azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n *   azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // baseUrl: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, { name: \"Joke\" });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.withConfig({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ model: \"gpt-4o-mini\", logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n */\nexport class AzureChatOpenAI<\n    CallOptions extends ChatOpenAICallOptions = ChatOpenAICallOptions\n  >\n  extends ChatOpenAI<CallOptions>\n  implements Partial<AzureOpenAIChatInput>\n{\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  _llmType(): string {\n    return \"azure_openai\";\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      ...AZURE_ALIASES,\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      ...AZURE_SECRETS,\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, ...AZURE_SERIALIZABLE_KEYS];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = super.getLsParams(options);\n    params.ls_provider = \"azure\";\n    return params;\n  }\n\n  constructor(fields?: AzureChatOpenAIFields) {\n    super({\n      ...fields,\n      completions: new AzureChatOpenAICompletions(fields),\n      responses: new AzureChatOpenAIResponses(fields),\n    });\n    _constructAzureFields.call(this, fields);\n  }\n\n  /** @internal */\n  override _getStructuredOutputMethod(\n    config: StructuredOutputMethodOptions<boolean>\n  ) {\n    const ensuredConfig = { ...config };\n    // Not all Azure gpt-4o deployments models support jsonSchema yet\n    if (this.model.startsWith(\"gpt-4o\")) {\n      if (ensuredConfig?.method === undefined) {\n        return \"functionCalling\";\n      }\n    }\n    return super._getStructuredOutputMethod(ensuredConfig);\n  }\n\n  override toJSON(): Serialized {\n    return _serializeAzureChat.call(this, super.toJSON());\n  }\n}\n"],"names":["options: this[\"ParsedCallOptions\"]","fields?: AzureChatOpenAIFields","config: StructuredOutputMethodOptions<boolean>"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA4aA,IAAa,kBAAb,cAGU,qLAAA,CAEV;IACE,sBAAA;IAEA,kBAAA;IAEA,qBAAA;IAEA,2BAAA;IAEA,6BAAA;IAEA,oBAAA;IAEA,oBAAA;IAEA,WAAmB;QACjB,OAAO;IACR;IAED,IAAI,aAAqC;QACvC,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,GAAG,kMAAA;QACJ;IACF;IAED,IAAI,aAAoD;QACtD,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,GAAG,kMAAA;QACJ;IACF;IAED,IAAI,uBAAiC;QACnC,OAAO,CAAC;eAAG,KAAA,CAAM,sBAAsB;eAAG,4MAAwB;SAAA;IACnE;IAED,YAAYA,OAAAA,EAAqD;QAC/D,MAAM,SAAS,KAAA,CAAM,YAAY,QAAQ;QACzC,OAAO,WAAA,GAAc;QACrB,OAAO;IACR;IAED,YAAYC,MAAAA,CAAgC;QAC1C,KAAA,CAAM;YACJ,GAAG,MAAA;YACH,aAAa,IAAI,oNAAA,CAA2B;YAC5C,WAAW,IAAI,gNAAA,CAAyB;QACzC,EAAC;QACF,0MAAA,CAAsB,IAAA,CAAK,IAAA,EAAM,OAAO;IACzC;qBAGQ,2BACPC,MAAAA,EACA;QACA,MAAM,gBAAgB;YAAE,GAAG,MAAA;QAAQ;QAEnC,IAAI,IAAA,CAAK,KAAA,CAAM,UAAA,CAAW,SAAS,EACjC;gBAAI,eAAe,WAAW,KAAA,EAC5B,CAAA,OAAO;QACR;QAEH,OAAO,KAAA,CAAM,2BAA2B,cAAc;IACvD;IAEQ,SAAqB;QAC5B,OAAO,wMAAA,CAAoB,IAAA,CAAK,IAAA,EAAM,KAAA,CAAM,QAAQ,CAAC;IACtD;AACF"}},
    {"offset": {"line": 5397, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/llms.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/llms.ts"],"sourcesContent":["import type { TiktokenModel } from \"js-tiktoken/lite\";\nimport { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { calculateMaxTokens } from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { GenerationChunk, type LLMResult } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  BaseLLM,\n  type BaseLLMParams,\n} from \"@langchain/core/language_models/llms\";\nimport { chunkArray } from \"@langchain/core/utils/chunk_array\";\nimport type {\n  OpenAIApiKey,\n  OpenAICallOptions,\n  OpenAICoreRequestOptions,\n  OpenAIInput,\n} from \"./types.js\";\nimport {\n  OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"./utils/azure.js\";\nimport { wrapOpenAIClientError } from \"./utils/client.js\";\n\nexport type { OpenAICallOptions, OpenAIInput };\n\n/**\n * Interface for tracking token usage in OpenAI calls.\n */\ninterface TokenUsage {\n  completionTokens?: number;\n  promptTokens?: number;\n  totalTokens?: number;\n}\n\n/**\n * Wrapper around OpenAI large language models.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure, import the `AzureOpenAI` class.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/completions/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n * @example\n * ```typescript\n * const model = new OpenAI({\n *   modelName: \"gpt-4\",\n *   temperature: 0.7,\n *   maxTokens: 1000,\n *   maxRetries: 5,\n * });\n *\n * const res = await model.invoke(\n *   \"Question: What would be a good company name for a company that makes colorful socks?\\nAnswer:\"\n * );\n * console.log({ res });\n * ```\n */\nexport class OpenAI<CallOptions extends OpenAICallOptions = OpenAICallOptions>\n  extends BaseLLM<CallOptions>\n  implements Partial<OpenAIInput>\n{\n  static lc_name() {\n    return \"OpenAI\";\n  }\n\n  get callKeys() {\n    return [...super.callKeys, \"options\"];\n  }\n\n  lc_serializable = true;\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      openAIApiKey: \"OPENAI_API_KEY\",\n      apiKey: \"OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\",\n    };\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      modelName: \"model\",\n      openAIApiKey: \"openai_api_key\",\n      apiKey: \"openai_api_key\",\n    };\n  }\n\n  temperature?: number;\n\n  maxTokens?: number;\n\n  topP?: number;\n\n  frequencyPenalty?: number;\n\n  presencePenalty?: number;\n\n  n = 1;\n\n  bestOf?: number;\n\n  logitBias?: Record<string, number>;\n\n  model = \"gpt-3.5-turbo-instruct\";\n\n  /** @deprecated Use \"model\" instead */\n  modelName: string;\n\n  modelKwargs?: OpenAIInput[\"modelKwargs\"];\n\n  batchSize = 20;\n\n  timeout?: number;\n\n  stop?: string[];\n\n  stopSequences?: string[];\n\n  user?: string;\n\n  streaming = false;\n\n  openAIApiKey?: OpenAIApiKey;\n\n  apiKey?: OpenAIApiKey;\n\n  organization?: string;\n\n  protected client: OpenAIClient;\n\n  protected clientConfig: ClientOptions;\n\n  constructor(\n    fields?: Partial<OpenAIInput> &\n      BaseLLMParams & {\n        configuration?: ClientOptions;\n      }\n  ) {\n    super(fields ?? {});\n\n    this.openAIApiKey =\n      fields?.apiKey ??\n      fields?.openAIApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.apiKey = this.openAIApiKey;\n\n    this.organization =\n      fields?.configuration?.organization ??\n      getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    if (\n      (this.model?.startsWith(\"gpt-3.5-turbo\") ||\n        this.model?.startsWith(\"gpt-4\") ||\n        this.model?.startsWith(\"o1\")) &&\n      !this.model?.includes(\"-instruct\")\n    ) {\n      throw new Error(\n        [\n          `Your chosen OpenAI model, \"${this.model}\", is a chat model and not a text-in/text-out LLM.`,\n          `Passing it into the \"OpenAI\" class is no longer supported.`,\n          `Please use the \"ChatOpenAI\" class instead.`,\n          \"\",\n          `See this page for more information:`,\n          \"|\",\n          `> https://js.langchain.com/docs/integrations/chat/openai`,\n        ].join(\"\\n\")\n      );\n    }\n    this.modelName = this.model;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.batchSize = fields?.batchSize ?? this.batchSize;\n    this.timeout = fields?.timeout;\n\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.n = fields?.n ?? this.n;\n    this.bestOf = fields?.bestOf ?? this.bestOf;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stopSequences ?? fields?.stop;\n    this.stopSequences = this.stop;\n    this.user = fields?.user;\n\n    this.streaming = fields?.streaming ?? false;\n\n    if (this.streaming && this.bestOf && this.bestOf > 1) {\n      throw new Error(\"Cannot stream results when bestOf > 1\");\n    }\n\n    this.clientConfig = {\n      apiKey: this.apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration,\n    };\n  }\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): Omit<OpenAIClient.CompletionCreateParams, \"prompt\"> {\n    return {\n      model: this.model,\n      temperature: this.temperature,\n      max_tokens: this.maxTokens,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      n: this.n,\n      best_of: this.bestOf,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stopSequences,\n      user: this.user,\n      stream: this.streaming,\n      ...this.modelKwargs,\n    };\n  }\n\n  /** @ignore */\n  _identifyingParams(): Omit<OpenAIClient.CompletionCreateParams, \"prompt\"> & {\n    model_name: string;\n  } & ClientOptions {\n    return {\n      model_name: this.model,\n      ...this.invocationParams(),\n      ...this.clientConfig,\n    };\n  }\n\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams(): Omit<OpenAIClient.CompletionCreateParams, \"prompt\"> & {\n    model_name: string;\n  } & ClientOptions {\n    return this._identifyingParams();\n  }\n\n  /**\n   * Call out to OpenAI's endpoint with k unique prompts\n   *\n   * @param [prompts] - The prompts to pass into the model.\n   * @param [options] - Optional list of stop words to use when generating.\n   * @param [runManager] - Optional callback manager to use when generating.\n   *\n   * @returns The full LLM output.\n   *\n   * @example\n   * ```ts\n   * import { OpenAI } from \"langchain/llms/openai\";\n   * const openai = new OpenAI();\n   * const response = await openai.generate([\"Tell me a joke.\"]);\n   * ```\n   */\n  async _generate(\n    prompts: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult> {\n    const subPrompts = chunkArray(prompts, this.batchSize);\n    const choices: OpenAIClient.CompletionChoice[] = [];\n    const tokenUsage: TokenUsage = {};\n\n    const params = this.invocationParams(options);\n\n    if (params.max_tokens === -1) {\n      if (prompts.length !== 1) {\n        throw new Error(\n          \"max_tokens set to -1 not supported for multiple inputs\"\n        );\n      }\n      params.max_tokens = await calculateMaxTokens({\n        prompt: prompts[0],\n        // Cast here to allow for other models that may not fit the union\n        modelName: this.model as TiktokenModel,\n      });\n    }\n\n    for (let i = 0; i < subPrompts.length; i += 1) {\n      const data = params.stream\n        ? await (async () => {\n            const choices: OpenAIClient.CompletionChoice[] = [];\n            let response: Omit<OpenAIClient.Completion, \"choices\"> | undefined;\n            const stream = await this.completionWithRetry(\n              {\n                ...params,\n                stream: true,\n                prompt: subPrompts[i],\n              },\n              options\n            );\n            for await (const message of stream) {\n              // on the first message set the response properties\n              if (!response) {\n                response = {\n                  id: message.id,\n                  object: message.object,\n                  created: message.created,\n                  model: message.model,\n                };\n              }\n\n              // on all messages, update choice\n              for (const part of message.choices) {\n                if (!choices[part.index]) {\n                  choices[part.index] = part;\n                } else {\n                  const choice = choices[part.index];\n                  choice.text += part.text;\n                  choice.finish_reason = part.finish_reason;\n                  choice.logprobs = part.logprobs;\n                }\n                // eslint-disable-next-line no-void\n                void runManager?.handleLLMNewToken(part.text, {\n                  prompt: Math.floor(part.index / this.n),\n                  completion: part.index % this.n,\n                });\n              }\n            }\n            if (options.signal?.aborted) {\n              throw new Error(\"AbortError\");\n            }\n            return { ...response, choices };\n          })()\n        : await this.completionWithRetry(\n            {\n              ...params,\n              stream: false,\n              prompt: subPrompts[i],\n            },\n            {\n              signal: options.signal,\n              ...options.options,\n            }\n          );\n\n      choices.push(...data.choices);\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens,\n      } = data.usage\n        ? data.usage\n        : {\n            completion_tokens: undefined,\n            prompt_tokens: undefined,\n            total_tokens: undefined,\n          };\n\n      if (completionTokens) {\n        tokenUsage.completionTokens =\n          (tokenUsage.completionTokens ?? 0) + completionTokens;\n      }\n\n      if (promptTokens) {\n        tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n      }\n\n      if (totalTokens) {\n        tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n      }\n    }\n\n    const generations = chunkArray(choices, this.n).map((promptChoices) =>\n      promptChoices.map((choice) => ({\n        text: choice.text ?? \"\",\n        generationInfo: {\n          finishReason: choice.finish_reason,\n          logprobs: choice.logprobs,\n        },\n      }))\n    );\n    return {\n      generations,\n      llmOutput: { tokenUsage },\n    };\n  }\n\n  // TODO(jacoblee): Refactor with _generate(..., {stream: true}) implementation?\n  async *_streamResponseChunks(\n    input: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    const params = {\n      ...this.invocationParams(options),\n      prompt: input,\n      stream: true as const,\n    };\n    const stream = await this.completionWithRetry(params, options);\n    for await (const data of stream) {\n      const choice = data?.choices[0];\n      if (!choice) {\n        continue;\n      }\n      const chunk = new GenerationChunk({\n        text: choice.text,\n        generationInfo: {\n          finishReason: choice.finish_reason,\n        },\n      });\n      yield chunk;\n      // eslint-disable-next-line no-void\n      void runManager?.handleLLMNewToken(chunk.text ?? \"\");\n    }\n    if (options.signal?.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n\n  /**\n   * Calls the OpenAI API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  async completionWithRetry(\n    request: OpenAIClient.CompletionCreateParamsStreaming,\n    options?: OpenAICoreRequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Completion>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.CompletionCreateParamsNonStreaming,\n    options?: OpenAICoreRequestOptions\n  ): Promise<OpenAIClient.Completions.Completion>;\n\n  async completionWithRetry(\n    request:\n      | OpenAIClient.CompletionCreateParamsStreaming\n      | OpenAIClient.CompletionCreateParamsNonStreaming,\n    options?: OpenAICoreRequestOptions\n  ): Promise<\n    AsyncIterable<OpenAIClient.Completion> | OpenAIClient.Completions.Completion\n  > {\n    const requestOptions = this._getClientOptions(options);\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.completions.create(\n          request,\n          requestOptions\n        );\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /**\n   * Calls the OpenAI API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  protected _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(params.defaultHeaders);\n\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options,\n    } as OpenAICoreRequestOptions;\n    return requestOptions;\n  }\n\n  _llmType() {\n    return \"openai\";\n  }\n}\n"],"names":["OpenAI","fields?: Partial<OpenAIInput> &\n      BaseLLMParams & {\n        configuration?: ClientOptions;\n      }","options?: this[\"ParsedCallOptions\"]","prompts: string[]","options: this[\"ParsedCallOptions\"]","runManager?: CallbackManagerForLLMRun","choices: OpenAIClient.CompletionChoice[]","tokenUsage: TokenUsage","response: Omit<OpenAIClient.Completion, \"choices\"> | undefined","choices","input: string","request:\n      | OpenAIClient.CompletionCreateParamsStreaming\n      | OpenAIClient.CompletionCreateParamsNonStreaming","options?: OpenAICoreRequestOptions","options: OpenAICoreRequestOptions | undefined","openAIEndpointConfig: OpenAIEndpointConfig","OpenAIClient"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA+DA,IAAaA,WAAb,cACU,mLAAA,CAEV;IACE,OAAO,UAAU;QACf,OAAO;IACR;IAED,IAAI,WAAW;QACb,OAAO,CAAC;eAAG,KAAA,CAAM;YAAU,SAAU;SAAA;IACtC;IAED,kBAAkB,KAAA;IAElB,IAAI,aAAoD;QACtD,OAAO;YACL,cAAc;YACd,QAAQ;YACR,cAAc;QACf;IACF;IAED,IAAI,aAAqC;QACvC,OAAO;YACL,WAAW;YACX,cAAc;YACd,QAAQ;QACT;IACF;IAED,YAAA;IAEA,UAAA;IAEA,KAAA;IAEA,iBAAA;IAEA,gBAAA;IAEA,IAAI,EAAA;IAEJ,OAAA;IAEA,UAAA;IAEA,QAAQ,yBAAA;2CAGR,UAAA;IAEA,YAAA;IAEA,YAAY,GAAA;IAEZ,QAAA;IAEA,KAAA;IAEA,cAAA;IAEA,KAAA;IAEA,YAAY,MAAA;IAEZ,aAAA;IAEA,OAAA;IAEA,aAAA;IAEU,OAAA;IAEA,aAAA;IAEV,YACEC,MAAAA,CAIA;QACA,KAAA,CAAM,UAAU,CAAE,EAAC;QAEnB,IAAA,CAAK,YAAA,GACH,QAAQ,UACR,QAAQ,oBACR,uLAAA,EAAuB,iBAAiB;QAC1C,IAAA,CAAK,MAAA,GAAS,IAAA,CAAK,YAAA;QAEnB,IAAA,CAAK,YAAA,GACH,QAAQ,eAAe,oBACvB,uLAAA,EAAuB,sBAAsB;QAE/C,IAAA,CAAK,KAAA,GAAQ,QAAQ,SAAS,QAAQ,aAAa,IAAA,CAAK,KAAA;QACxD,IAAA,CACG,IAAA,CAAK,KAAA,EAAO,WAAW,gBAAgB,IACtC,IAAA,CAAK,KAAA,EAAO,WAAW,QAAQ,IAC/B,IAAA,CAAK,KAAA,EAAO,WAAW,KAAK,KAC9B,CAAC,IAAA,CAAK,KAAA,EAAO,SAAS,YAAY,CAElC,CAAA,MAAM,IAAI,MACR;YACE,CAAC,2BAA2B,EAAE,IAAA,CAAK,KAAA,CAAM,kDAAkD,CAAC;YAC5F,CAAC,0DAA0D,CAAC;YAC5D,CAAC,0CAA0C,CAAC;YAC5C;YACA,CAAC,mCAAmC,CAAC;YACrC;YACA,CAAC,yDAAyD,CAAC;SAC5D,CAAC,IAAA,CAAK,KAAK;QAGhB,IAAA,CAAK,SAAA,GAAY,IAAA,CAAK,KAAA;QACtB,IAAA,CAAK,WAAA,GAAc,QAAQ,eAAe,CAAE;QAC5C,IAAA,CAAK,SAAA,GAAY,QAAQ,aAAa,IAAA,CAAK,SAAA;QAC3C,IAAA,CAAK,OAAA,GAAU,QAAQ;QAEvB,IAAA,CAAK,WAAA,GAAc,QAAQ,eAAe,IAAA,CAAK,WAAA;QAC/C,IAAA,CAAK,SAAA,GAAY,QAAQ,aAAa,IAAA,CAAK,SAAA;QAC3C,IAAA,CAAK,IAAA,GAAO,QAAQ,QAAQ,IAAA,CAAK,IAAA;QACjC,IAAA,CAAK,gBAAA,GAAmB,QAAQ,oBAAoB,IAAA,CAAK,gBAAA;QACzD,IAAA,CAAK,eAAA,GAAkB,QAAQ,mBAAmB,IAAA,CAAK,eAAA;QACvD,IAAA,CAAK,CAAA,GAAI,QAAQ,KAAK,IAAA,CAAK,CAAA;QAC3B,IAAA,CAAK,MAAA,GAAS,QAAQ,UAAU,IAAA,CAAK,MAAA;QACrC,IAAA,CAAK,SAAA,GAAY,QAAQ;QACzB,IAAA,CAAK,IAAA,GAAO,QAAQ,iBAAiB,QAAQ;QAC7C,IAAA,CAAK,aAAA,GAAgB,IAAA,CAAK,IAAA;QAC1B,IAAA,CAAK,IAAA,GAAO,QAAQ;QAEpB,IAAA,CAAK,SAAA,GAAY,QAAQ,aAAa;QAEtC,IAAI,IAAA,CAAK,SAAA,IAAa,IAAA,CAAK,MAAA,IAAU,IAAA,CAAK,MAAA,GAAS,EACjD,CAAA,MAAM,IAAI,MAAM;QAGlB,IAAA,CAAK,YAAA,GAAe;YAClB,QAAQ,IAAA,CAAK,MAAA;YACb,cAAc,IAAA,CAAK,YAAA;YACnB,yBAAyB;YACzB,GAAG,QAAQ,aAAA;QACZ;IACF;;;IAKD,iBACEC,OAAAA,EACqD;QACrD,OAAO;YACL,OAAO,IAAA,CAAK,KAAA;YACZ,aAAa,IAAA,CAAK,WAAA;YAClB,YAAY,IAAA,CAAK,SAAA;YACjB,OAAO,IAAA,CAAK,IAAA;YACZ,mBAAmB,IAAA,CAAK,gBAAA;YACxB,kBAAkB,IAAA,CAAK,eAAA;YACvB,GAAG,IAAA,CAAK,CAAA;YACR,SAAS,IAAA,CAAK,MAAA;YACd,YAAY,IAAA,CAAK,SAAA;YACjB,MAAM,SAAS,QAAQ,IAAA,CAAK,aAAA;YAC5B,MAAM,IAAA,CAAK,IAAA;YACX,QAAQ,IAAA,CAAK,SAAA;YACb,GAAG,IAAA,CAAK,WAAA;QACT;IACF;mBAGD,qBAEkB;QAChB,OAAO;YACL,YAAY,IAAA,CAAK,KAAA;YACjB,GAAG,IAAA,CAAK,gBAAA,EAAkB;YAC1B,GAAG,IAAA,CAAK,YAAA;QACT;IACF;;;IAKD,oBAEkB;QAChB,OAAO,IAAA,CAAK,kBAAA,EAAoB;IACjC;;;;;;;;;;;;;;;;IAkBD,MAAM,UACJC,OAAAA,EACAC,OAAAA,EACAC,UAAAA,EACoB;QACpB,MAAM,iBAAa,mLAAA,EAAW,SAAS,IAAA,CAAK,SAAA,CAAU;QACtD,MAAMC,UAA2C,CAAE,CAAA;QACnD,MAAMC,aAAyB,CAAE;QAEjC,MAAM,SAAS,IAAA,CAAK,gBAAA,CAAiB,QAAQ;QAE7C,IAAI,OAAO,UAAA,KAAe,CAAA,GAAI;YAC5B,IAAI,QAAQ,MAAA,KAAW,EACrB,CAAA,MAAM,IAAI,MACR;YAGJ,OAAO,UAAA,GAAa,UAAM,8LAAA,EAAmB;gBAC3C,QAAQ,OAAA,CAAQ,EAAA;gBAEhB,WAAW,IAAA,CAAK,KAAA;YACjB,EAAC;QACH;QAED,IAAK,IAAI,IAAI,GAAG,IAAI,WAAW,MAAA,EAAQ,KAAK,EAAG;YAC7C,MAAM,OAAO,OAAO,MAAA,GAChB,MAAA,CAAO,YAAY;gBACjB,MAAMD,YAA2C,CAAE,CAAA;gBACnD,IAAIE;gBACJ,MAAM,SAAS,MAAM,IAAA,CAAK,mBAAA,CACxB;oBACE,GAAG,MAAA;oBACH,QAAQ;oBACR,QAAQ,UAAA,CAAW,EAAA;gBACpB,GACD,QACD;gBACD,WAAW,MAAM,WAAW,OAAQ;oBAElC,IAAI,CAAC,UACH,WAAW;wBACT,IAAI,QAAQ,EAAA;wBACZ,QAAQ,QAAQ,MAAA;wBAChB,SAAS,QAAQ,OAAA;wBACjB,OAAO,QAAQ,KAAA;oBAChB;oBAIH,KAAK,MAAM,QAAQ,QAAQ,OAAA,CAAS;wBAClC,IAAI,CAACC,SAAAA,CAAQ,KAAK,KAAA,CAAA,EAChBA,SAAAA,CAAQ,KAAK,KAAA,CAAA,GAAS;6BACjB;4BACL,MAAM,SAASA,SAAAA,CAAQ,KAAK,KAAA,CAAA;4BAC5B,OAAO,IAAA,IAAQ,KAAK,IAAA;4BACpB,OAAO,aAAA,GAAgB,KAAK,aAAA;4BAC5B,OAAO,QAAA,GAAW,KAAK,QAAA;wBACxB;wBAEI,YAAY,kBAAkB,KAAK,IAAA,EAAM;4BAC5C,QAAQ,KAAK,KAAA,CAAM,KAAK,KAAA,GAAQ,IAAA,CAAK,CAAA,CAAE;4BACvC,YAAY,KAAK,KAAA,GAAQ,IAAA,CAAK,CAAA;wBAC/B,EAAC;oBACH;gBACF;gBACD,IAAI,QAAQ,MAAA,EAAQ,QAClB,CAAA,MAAM,IAAI,MAAM;gBAElB,OAAO;oBAAE,GAAG,QAAA;oBAAU,SAAA;gBAAS;YAChC,CAAA,GAAG,GACJ,MAAM,IAAA,CAAK,mBAAA,CACT;gBACE,GAAG,MAAA;gBACH,QAAQ;gBACR,QAAQ,UAAA,CAAW,EAAA;YACpB,GACD;gBACE,QAAQ,QAAQ,MAAA;gBAChB,GAAG,QAAQ,OAAA;YACZ,EACF;YAEL,QAAQ,IAAA,CAAK,GAAG,KAAK,OAAA,CAAQ;YAC7B,MAAM,EACJ,mBAAmB,gBAAA,EACnB,eAAe,YAAA,EACf,cAAc,WAAA,EACf,GAAG,KAAK,KAAA,GACL,KAAK,KAAA,GACL;gBACE,mBAAmB,KAAA;gBACnB,eAAe,KAAA;gBACf,cAAc,KAAA;YACf;YAEL,IAAI,kBACF,WAAW,gBAAA,GAAA,CACR,WAAW,gBAAA,IAAoB,CAAA,IAAK;YAGzC,IAAI,cACF,WAAW,YAAA,GAAA,CAAgB,WAAW,YAAA,IAAgB,CAAA,IAAK;YAG7D,IAAI,aACF,WAAW,WAAA,GAAA,CAAe,WAAW,WAAA,IAAe,CAAA,IAAK;QAE5D;QAED,MAAM,kBAAc,mLAAA,EAAW,SAAS,IAAA,CAAK,CAAA,CAAE,CAAC,GAAA,CAAI,CAAC,gBACnD,cAAc,GAAA,CAAI,CAAC,SAAA,CAAY;oBAC7B,MAAM,OAAO,IAAA,IAAQ;oBACrB,gBAAgB;wBACd,cAAc,OAAO,aAAA;wBACrB,UAAU,OAAO,QAAA;oBAClB;gBACF,CAAA,EAAE,CACJ;QACD,OAAO;YACL;YACA,WAAW;gBAAE;YAAY;QAC1B;IACF;IAGD,OAAO,sBACLC,KAAAA,EACAN,OAAAA,EACAC,UAAAA,EACiC;QACjC,MAAM,SAAS;YACb,GAAG,IAAA,CAAK,gBAAA,CAAiB,QAAQ;YACjC,QAAQ;YACR,QAAQ;QACT;QACD,MAAM,SAAS,MAAM,IAAA,CAAK,mBAAA,CAAoB,QAAQ,QAAQ;QAC9D,WAAW,MAAM,QAAQ,OAAQ;YAC/B,MAAM,SAAS,MAAM,OAAA,CAAQ,EAAA;YAC7B,IAAI,CAAC,OACH,CAAA;YAEF,MAAM,QAAQ,IAAI,2KAAA,CAAgB;gBAChC,MAAM,OAAO,IAAA;gBACb,gBAAgB;oBACd,cAAc,OAAO,aAAA;gBACtB;YACF;YACD,MAAM;YAED,YAAY,kBAAkB,MAAM,IAAA,IAAQ,GAAG;QACrD;QACD,IAAI,QAAQ,MAAA,EAAQ,QAClB,CAAA,MAAM,IAAI,MAAM;IAEnB;IAkBD,MAAM,oBACJM,OAAAA,EAGAC,OAAAA,EAGA;QACA,MAAM,iBAAiB,IAAA,CAAK,iBAAA,CAAkB,QAAQ;QACtD,OAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,YAAY;YAClC,IAAI;gBACF,MAAM,MAAM,MAAM,IAAA,CAAK,MAAA,CAAO,WAAA,CAAY,MAAA,CACxC,SACA,eACD;gBACD,OAAO;YACR,EAAA,OAAQ,GAAG;gBACV,MAAM,YAAQ,2LAAA,EAAsB,EAAE;gBACtC,MAAM;YACP;QACF,EAAC;IACH;;;;;;IAQS,kBACRC,OAAAA,EAC0B;QAC1B,IAAI,CAAC,IAAA,CAAK,MAAA,EAAQ;YAChB,MAAMC,uBAA6C;gBACjD,SAAS,IAAA,CAAK,YAAA,CAAa,OAAA;YAC5B;YAED,MAAM,eAAW,gLAAA,EAAY,qBAAqB;YAElD,MAAM,SAAS;gBACb,GAAG,IAAA,CAAK,YAAA;gBACR,SAAS;gBACT,SAAS,IAAA,CAAK,OAAA;gBACd,YAAY;YACb;YAED,IAAI,CAAC,OAAO,OAAA,EACV,OAAO,OAAO,OAAA;YAGhB,OAAO,cAAA,OAAiB,4LAAA,EAAwB,OAAO,cAAA,CAAe;YAEtE,IAAA,CAAK,MAAA,GAAS,IAAIC,6IAAAA,CAAa;QAChC;QACD,MAAM,iBAAiB;YACrB,GAAG,IAAA,CAAK,YAAA;YACR,GAAG,OAAA;QACJ;QACD,OAAO;IACR;IAED,WAAW;QACT,OAAO;IACR;AACF"}},
    {"offset": {"line": 5727, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/azure/llms.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/azure/llms.ts"],"sourcesContent":["import { type ClientOptions, AzureOpenAI as AzureOpenAIClient } from \"openai\";\nimport { type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { OpenAI } from \"../llms.js\";\nimport {\n  OpenAIEndpointConfig,\n  getEndpoint,\n  getHeadersWithUserAgent,\n} from \"../utils/azure.js\";\nimport type {\n  OpenAIInput,\n  AzureOpenAIInput,\n  OpenAICoreRequestOptions,\n} from \"../types.js\";\n\nexport class AzureOpenAI extends OpenAI {\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  azureOpenAIEndpoint?: string;\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      ...super.lc_aliases,\n      openAIApiKey: \"openai_api_key\",\n      openAIApiVersion: \"openai_api_version\",\n      openAIBasePath: \"openai_api_base\",\n      deploymentName: \"deployment_name\",\n      azureOpenAIEndpoint: \"azure_endpoint\",\n      azureOpenAIApiVersion: \"openai_api_version\",\n      azureOpenAIBasePath: \"openai_api_base\",\n      azureOpenAIApiDeploymentName: \"deployment_name\",\n    };\n  }\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      ...super.lc_secrets,\n      azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n    };\n  }\n\n  constructor(\n    fields?: Partial<OpenAIInput> & {\n      openAIApiKey?: string;\n      openAIApiVersion?: string;\n      openAIBasePath?: string;\n      deploymentName?: string;\n    } & Partial<AzureOpenAIInput> &\n      BaseLLMParams & {\n        configuration?: ClientOptions;\n      }\n  ) {\n    super(fields);\n\n    this.azureOpenAIApiDeploymentName =\n      (fields?.azureOpenAIApiCompletionsDeploymentName ||\n        fields?.azureOpenAIApiDeploymentName) ??\n      (getEnvironmentVariable(\"AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\") ||\n        getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n\n    this.azureOpenAIApiKey =\n      fields?.azureOpenAIApiKey ??\n      (typeof fields?.openAIApiKey === \"string\"\n        ? fields?.openAIApiKey\n        : undefined) ??\n      (typeof fields?.apiKey === \"string\" ? fields?.apiKey : undefined) ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n\n    this.azureOpenAIApiInstanceName =\n      fields?.azureOpenAIApiInstanceName ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n\n    this.azureOpenAIApiVersion =\n      fields?.azureOpenAIApiVersion ??\n      fields?.openAIApiVersion ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n\n    this.azureOpenAIBasePath =\n      fields?.azureOpenAIBasePath ??\n      getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n\n    this.azureOpenAIEndpoint =\n      fields?.azureOpenAIEndpoint ??\n      getEnvironmentVariable(\"AZURE_OPENAI_ENDPOINT\");\n\n    this.azureADTokenProvider = fields?.azureADTokenProvider;\n\n    if (!this.azureOpenAIApiKey && !this.apiKey && !this.azureADTokenProvider) {\n      throw new Error(\"Azure OpenAI API key or Token Provider not found\");\n    }\n  }\n\n  protected _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n        azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n        azureOpenAIApiKey: this.azureOpenAIApiKey,\n        azureOpenAIBasePath: this.azureOpenAIBasePath,\n        azureADTokenProvider: this.azureADTokenProvider,\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const { apiKey: existingApiKey, ...clientConfigRest } = this.clientConfig;\n      const params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string } = {\n        ...clientConfigRest,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!this.azureADTokenProvider) {\n        params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;\n      }\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(\n        params.defaultHeaders,\n        true,\n        \"2.0.0\"\n      );\n\n      this.client = new AzureOpenAIClient({\n        apiVersion: this.azureOpenAIApiVersion,\n        azureADTokenProvider: this.azureADTokenProvider,\n        ...params,\n      });\n    }\n\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options,\n    } as OpenAICoreRequestOptions;\n    if (this.azureOpenAIApiKey) {\n      requestOptions.headers = {\n        \"api-key\": this.azureOpenAIApiKey,\n        ...requestOptions.headers,\n      };\n      requestOptions.query = {\n        \"api-version\": this.azureOpenAIApiVersion,\n        ...requestOptions.query,\n      };\n    }\n    return requestOptions;\n  }\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  toJSON(): any {\n    const json = super.toJSON() as unknown;\n\n    function isRecord(obj: unknown): obj is Record<string, unknown> {\n      return typeof obj === \"object\" && obj != null;\n    }\n\n    if (isRecord(json) && isRecord(json.kwargs)) {\n      delete json.kwargs.azure_openai_base_path;\n      delete json.kwargs.azure_openai_api_deployment_name;\n      delete json.kwargs.azure_openai_api_key;\n      delete json.kwargs.azure_openai_api_version;\n      delete json.kwargs.azure_open_ai_base_path;\n    }\n\n    return json;\n  }\n}\n"],"names":["AzureOpenAI","fields?: Partial<OpenAIInput> & {\n      openAIApiKey?: string;\n      openAIApiVersion?: string;\n      openAIBasePath?: string;\n      deploymentName?: string;\n    } & Partial<AzureOpenAIInput> &\n      BaseLLMParams & {\n        configuration?: ClientOptions;\n      }","options: OpenAICoreRequestOptions | undefined","openAIEndpointConfig: OpenAIEndpointConfig","params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string }","AzureOpenAIClient","obj: unknown"],"mappings":";;;;;;;;;;;;;;AAeA,IAAaA,gBAAb,cAAiC,iKAAA,CAAO;IACtC,sBAAA;IAEA,kBAAA;IAEA,qBAAA;IAEA,2BAAA;IAEA,6BAAA;IAEA,oBAAA;IAEA,oBAAA;IAEA,IAAI,aAAqC;QACvC,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,cAAc;YACd,kBAAkB;YAClB,gBAAgB;YAChB,gBAAgB;YAChB,qBAAqB;YACrB,uBAAuB;YACvB,qBAAqB;YACrB,8BAA8B;QAC/B;IACF;IAED,IAAI,aAAoD;QACtD,OAAO;YACL,GAAG,KAAA,CAAM,UAAA;YACT,mBAAmB;QACpB;IACF;IAED,YACEC,MAAAA,CASA;QACA,KAAA,CAAM,OAAO;QAEb,IAAA,CAAK,4BAAA,GAAA,CACF,QAAQ,2CACP,QAAQ,4BAAA,KAAA,KACT,uLAAA,EAAuB,+CAA+C,QACrE,uLAAA,EAAuB,mCAAmC;QAE9D,IAAA,CAAK,iBAAA,GACH,QAAQ,qBAAA,CACP,OAAO,QAAQ,iBAAiB,WAC7B,QAAQ,eACR,KAAA,CAAA,KAAA,CACH,OAAO,QAAQ,WAAW,WAAW,QAAQ,SAAS,KAAA,CAAA,SACvD,uLAAA,EAAuB,uBAAuB;QAEhD,IAAA,CAAK,0BAAA,GACH,QAAQ,kCACR,uLAAA,EAAuB,iCAAiC;QAE1D,IAAA,CAAK,qBAAA,GACH,QAAQ,yBACR,QAAQ,wBACR,uLAAA,EAAuB,2BAA2B;QAEpD,IAAA,CAAK,mBAAA,GACH,QAAQ,2BACR,uLAAA,EAAuB,yBAAyB;QAElD,IAAA,CAAK,mBAAA,GACH,QAAQ,2BACR,uLAAA,EAAuB,wBAAwB;QAEjD,IAAA,CAAK,oBAAA,GAAuB,QAAQ;QAEpC,IAAI,CAAC,IAAA,CAAK,iBAAA,IAAqB,CAAC,IAAA,CAAK,MAAA,IAAU,CAAC,IAAA,CAAK,oBAAA,CACnD,CAAA,MAAM,IAAI,MAAM;IAEnB;IAES,kBACRC,OAAAA,EAC0B;QAC1B,IAAI,CAAC,IAAA,CAAK,MAAA,EAAQ;YAChB,MAAMC,uBAA6C;gBACjD,8BAA8B,IAAA,CAAK,4BAAA;gBACnC,4BAA4B,IAAA,CAAK,0BAAA;gBACjC,mBAAmB,IAAA,CAAK,iBAAA;gBACxB,qBAAqB,IAAA,CAAK,mBAAA;gBAC1B,sBAAsB,IAAA,CAAK,oBAAA;gBAC3B,SAAS,IAAA,CAAK,YAAA,CAAa,OAAA;YAC5B;YAED,MAAM,eAAW,gLAAA,EAAY,qBAAqB;YAElD,MAAM,EAAE,QAAQ,cAAA,EAAgB,GAAG,kBAAkB,GAAG,IAAA,CAAK,YAAA;YAC7D,MAAMC,SAA8D;gBAClE,GAAG,gBAAA;gBACH,SAAS;gBACT,SAAS,IAAA,CAAK,OAAA;gBACd,YAAY;YACb;YAED,IAAI,CAAC,IAAA,CAAK,oBAAA,EACR,OAAO,MAAA,GAAS,qBAAqB,iBAAA;YAGvC,IAAI,CAAC,OAAO,OAAA,EACV,OAAO,OAAO,OAAA;YAGhB,OAAO,cAAA,OAAiB,4LAAA,EACtB,OAAO,cAAA,EACP,MACA,QACD;YAED,IAAA,CAAK,MAAA,GAAS,IAAIC,iJAAAA,CAAkB;gBAClC,YAAY,IAAA,CAAK,qBAAA;gBACjB,sBAAsB,IAAA,CAAK,oBAAA;gBAC3B,GAAG,MAAA;YACJ;QACF;QAED,MAAM,iBAAiB;YACrB,GAAG,IAAA,CAAK,YAAA;YACR,GAAG,OAAA;QACJ;QACD,IAAI,IAAA,CAAK,iBAAA,EAAmB;YAC1B,eAAe,OAAA,GAAU;gBACvB,WAAW,IAAA,CAAK,iBAAA;gBAChB,GAAG,eAAe,OAAA;YACnB;YACD,eAAe,KAAA,GAAQ;gBACrB,eAAe,IAAA,CAAK,qBAAA;gBACpB,GAAG,eAAe,KAAA;YACnB;QACF;QACD,OAAO;IACR;IAGD,SAAc;QACZ,MAAM,OAAO,KAAA,CAAM,QAAQ;QAE3B,SAAS,SAASC,GAAAA,EAA8C;YAC9D,OAAO,OAAO,QAAQ,YAAY,OAAO;QAC1C;QAED,IAAI,SAAS,KAAK,IAAI,SAAS,KAAK,MAAA,CAAO,EAAE;YAC3C,OAAO,KAAK,MAAA,CAAO,sBAAA;YACnB,OAAO,KAAK,MAAA,CAAO,gCAAA;YACnB,OAAO,KAAK,MAAA,CAAO,oBAAA;YACnB,OAAO,KAAK,MAAA,CAAO,wBAAA;YACnB,OAAO,KAAK,MAAA,CAAO,uBAAA;QACpB;QAED,OAAO;IACR;AACF"}},
    {"offset": {"line": 5843, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/embeddings.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/embeddings.ts"],"sourcesContent":["import { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { Embeddings, type EmbeddingsParams } from \"@langchain/core/embeddings\";\nimport { chunkArray } from \"@langchain/core/utils/chunk_array\";\nimport type { OpenAIApiKey } from \"./types.js\";\nimport {\n  getEndpoint,\n  OpenAIEndpointConfig,\n  getHeadersWithUserAgent,\n} from \"./utils/azure.js\";\nimport { wrapOpenAIClientError } from \"./utils/client.js\";\n\n/**\n * @see https://platform.openai.com/docs/guides/embeddings#embedding-models\n */\nexport type OpenAIEmbeddingModelId =\n  | OpenAIClient.EmbeddingModel\n  | (string & NonNullable<unknown>);\n\n/**\n * Interface for OpenAIEmbeddings parameters. Extends EmbeddingsParams and\n * defines additional parameters specific to the OpenAIEmbeddings class.\n */\nexport interface OpenAIEmbeddingsParams extends EmbeddingsParams {\n  /**\n   * Model name to use\n   * Alias for `model`\n   * @deprecated Use \"model\" instead.\n   */\n  modelName: OpenAIEmbeddingModelId;\n  /** Model name to use */\n  model: OpenAIEmbeddingModelId;\n\n  /**\n   * The number of dimensions the resulting output embeddings should have.\n   * Only supported in `text-embedding-3` and later models.\n   */\n  dimensions?: number;\n\n  /**\n   * Timeout to use when making requests to OpenAI.\n   */\n  timeout?: number;\n\n  /**\n   * The maximum number of documents to embed in a single request. This is\n   * limited by the OpenAI API to a maximum of 2048.\n   */\n  batchSize?: number;\n\n  /**\n   * Whether to strip new lines from the input text. This is recommended by\n   * OpenAI for older models, but may not be suitable for all use cases.\n   * See: https://github.com/openai/openai-python/issues/418#issuecomment-1525939500\n   */\n  stripNewLines?: boolean;\n\n  /**\n   * The format to return the embeddings in. Can be either 'float' or 'base64'.\n   */\n  encodingFormat?: \"float\" | \"base64\";\n}\n\n/**\n * Class for generating embeddings using the OpenAI API.\n *\n * To use with Azure, import the `AzureOpenAIEmbeddings` class.\n *\n * @example\n * ```typescript\n * // Embed a query using OpenAIEmbeddings to generate embeddings for a given text\n * const model = new OpenAIEmbeddings();\n * const res = await model.embedQuery(\n *   \"What would be a good company name for a company that makes colorful socks?\",\n * );\n * console.log({ res });\n *\n * ```\n */\nexport class OpenAIEmbeddings<TOutput = number[]>\n  extends Embeddings<TOutput>\n  implements Partial<OpenAIEmbeddingsParams>\n{\n  model = \"text-embedding-ada-002\";\n\n  /** @deprecated Use \"model\" instead */\n  modelName: string;\n\n  batchSize = 512;\n\n  // TODO: Update to `false` on next minor release (see: https://github.com/langchain-ai/langchainjs/pull/3612)\n  stripNewLines = true;\n\n  /**\n   * The number of dimensions the resulting output embeddings should have.\n   * Only supported in `text-embedding-3` and later models.\n   */\n  dimensions?: number;\n\n  timeout?: number;\n\n  organization?: string;\n\n  encodingFormat?: \"float\" | \"base64\";\n\n  protected client: OpenAIClient;\n\n  protected clientConfig: ClientOptions;\n\n  protected apiKey?: OpenAIApiKey;\n\n  constructor(\n    fields?: Partial<OpenAIEmbeddingsParams> & {\n      verbose?: boolean;\n      /**\n       * The OpenAI API key to use.\n       * Alias for `apiKey`.\n       */\n      openAIApiKey?: OpenAIApiKey;\n      /** The OpenAI API key to use. */\n      apiKey?: OpenAIApiKey;\n      configuration?: ClientOptions;\n    }\n  ) {\n    const fieldsWithDefaults = { maxConcurrency: 2, ...fields };\n\n    super(fieldsWithDefaults);\n\n    const apiKey =\n      fieldsWithDefaults?.apiKey ??\n      fieldsWithDefaults?.openAIApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n\n    this.organization =\n      fieldsWithDefaults?.configuration?.organization ??\n      getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    this.model =\n      fieldsWithDefaults?.model ?? fieldsWithDefaults?.modelName ?? this.model;\n    this.modelName = this.model;\n    this.batchSize = fieldsWithDefaults?.batchSize ?? this.batchSize;\n    this.stripNewLines =\n      fieldsWithDefaults?.stripNewLines ?? this.stripNewLines;\n    this.timeout = fieldsWithDefaults?.timeout;\n    this.dimensions = fieldsWithDefaults?.dimensions;\n    this.encodingFormat = fieldsWithDefaults?.encodingFormat;\n\n    this.clientConfig = {\n      apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration,\n    };\n  }\n\n  /**\n   * Method to generate embeddings for an array of documents. Splits the\n   * documents into batches and makes requests to the OpenAI API to generate\n   * embeddings.\n   * @param texts Array of documents to generate embeddings for.\n   * @returns Promise that resolves to a 2D array of embeddings for each document.\n   */\n  async embedDocuments(texts: string[]): Promise<TOutput[]> {\n    const batches = chunkArray(\n      this.stripNewLines ? texts.map((t) => t.replace(/\\n/g, \" \")) : texts,\n      this.batchSize\n    );\n\n    const batchRequests = batches.map((batch) => {\n      const params: OpenAIClient.EmbeddingCreateParams = {\n        model: this.model,\n        input: batch,\n      };\n      if (this.dimensions) {\n        params.dimensions = this.dimensions;\n      }\n      if (this.encodingFormat) {\n        params.encoding_format = this.encodingFormat;\n      }\n      return this.embeddingWithRetry(params);\n    });\n    const batchResponses = await Promise.all(batchRequests);\n\n    const embeddings: TOutput[] = [];\n    for (let i = 0; i < batchResponses.length; i += 1) {\n      const batch = batches[i];\n      const { data: batchResponse } = batchResponses[i];\n      for (let j = 0; j < batch.length; j += 1) {\n        embeddings.push(batchResponse[j].embedding as TOutput);\n      }\n    }\n    return embeddings;\n  }\n\n  /**\n   * Method to generate an embedding for a single document. Calls the\n   * embeddingWithRetry method with the document as the input.\n   * @param text Document to generate an embedding for.\n   * @returns Promise that resolves to an embedding for the document.\n   */\n  async embedQuery(text: string): Promise<TOutput> {\n    const params: OpenAIClient.EmbeddingCreateParams = {\n      model: this.model,\n      input: this.stripNewLines ? text.replace(/\\n/g, \" \") : text,\n    };\n    if (this.dimensions) {\n      params.dimensions = this.dimensions;\n    }\n    if (this.encodingFormat) {\n      params.encoding_format = this.encodingFormat;\n    }\n    const { data } = await this.embeddingWithRetry(params);\n    return data[0].embedding as TOutput;\n  }\n\n  /**\n   * Private method to make a request to the OpenAI API to generate\n   * embeddings. Handles the retry logic and returns the response from the\n   * API.\n   * @param request Request to send to the OpenAI API.\n   * @returns Promise that resolves to the response from the API.\n   */\n  protected async embeddingWithRetry(\n    request: OpenAIClient.EmbeddingCreateParams\n  ) {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(params.defaultHeaders);\n\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {};\n\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.embeddings.create(\n          request,\n          requestOptions\n        );\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n}\n"],"names":["fields?: Partial<OpenAIEmbeddingsParams> & {\n      verbose?: boolean;\n      /**\n       * The OpenAI API key to use.\n       * Alias for `apiKey`.\n       */\n      openAIApiKey?: OpenAIApiKey;\n      /** The OpenAI API key to use. */\n      apiKey?: OpenAIApiKey;\n      configuration?: ClientOptions;\n    }","texts: string[]","params: OpenAIClient.EmbeddingCreateParams","embeddings: TOutput[]","text: string","request: OpenAIClient.EmbeddingCreateParams","openAIEndpointConfig: OpenAIEndpointConfig","OpenAIClient"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA+EA,IAAa,mBAAb,cACU,yKAAA,CAEV;IACE,QAAQ,yBAAA;2CAGR,UAAA;IAEA,YAAY,IAAA;IAGZ,gBAAgB,KAAA;;;;IAMhB,WAAA;IAEA,QAAA;IAEA,aAAA;IAEA,eAAA;IAEU,OAAA;IAEA,aAAA;IAEA,OAAA;IAEV,YACEA,MAAAA,CAWA;QACA,MAAM,qBAAqB;YAAE,gBAAgB;YAAG,GAAG,MAAA;QAAQ;QAE3D,KAAA,CAAM,mBAAmB;QAEzB,MAAM,SACJ,oBAAoB,UACpB,oBAAoB,oBACpB,uLAAA,EAAuB,iBAAiB;QAE1C,IAAA,CAAK,YAAA,GACH,oBAAoB,eAAe,oBACnC,uLAAA,EAAuB,sBAAsB;QAE/C,IAAA,CAAK,KAAA,GACH,oBAAoB,SAAS,oBAAoB,aAAa,IAAA,CAAK,KAAA;QACrE,IAAA,CAAK,SAAA,GAAY,IAAA,CAAK,KAAA;QACtB,IAAA,CAAK,SAAA,GAAY,oBAAoB,aAAa,IAAA,CAAK,SAAA;QACvD,IAAA,CAAK,aAAA,GACH,oBAAoB,iBAAiB,IAAA,CAAK,aAAA;QAC5C,IAAA,CAAK,OAAA,GAAU,oBAAoB;QACnC,IAAA,CAAK,UAAA,GAAa,oBAAoB;QACtC,IAAA,CAAK,cAAA,GAAiB,oBAAoB;QAE1C,IAAA,CAAK,YAAA,GAAe;YAClB;YACA,cAAc,IAAA,CAAK,YAAA;YACnB,yBAAyB;YACzB,GAAG,QAAQ,aAAA;QACZ;IACF;;;;;;;IASD,MAAM,eAAeC,KAAAA,EAAqC;QACxD,MAAM,cAAU,mLAAA,EACd,IAAA,CAAK,aAAA,GAAgB,MAAM,GAAA,CAAI,CAAC,IAAM,EAAE,OAAA,CAAQ,OAAO,IAAI,CAAC,GAAG,OAC/D,IAAA,CAAK,SAAA,CACN;QAED,MAAM,gBAAgB,QAAQ,GAAA,CAAI,CAAC,UAAU;YAC3C,MAAMC,SAA6C;gBACjD,OAAO,IAAA,CAAK,KAAA;gBACZ,OAAO;YACR;YACD,IAAI,IAAA,CAAK,UAAA,EACP,OAAO,UAAA,GAAa,IAAA,CAAK,UAAA;YAE3B,IAAI,IAAA,CAAK,cAAA,EACP,OAAO,eAAA,GAAkB,IAAA,CAAK,cAAA;YAEhC,OAAO,IAAA,CAAK,kBAAA,CAAmB,OAAO;QACvC,EAAC;QACF,MAAM,iBAAiB,MAAM,QAAQ,GAAA,CAAI,cAAc;QAEvD,MAAMC,aAAwB,CAAE,CAAA;QAChC,IAAK,IAAI,IAAI,GAAG,IAAI,eAAe,MAAA,EAAQ,KAAK,EAAG;YACjD,MAAM,QAAQ,OAAA,CAAQ,EAAA;YACtB,MAAM,EAAE,MAAM,aAAA,EAAe,GAAG,cAAA,CAAe,EAAA;YAC/C,IAAK,IAAI,IAAI,GAAG,IAAI,MAAM,MAAA,EAAQ,KAAK,EACrC,WAAW,IAAA,CAAK,aAAA,CAAc,EAAA,CAAG,SAAA,CAAqB;QAEzD;QACD,OAAO;IACR;;;;;;IAQD,MAAM,WAAWC,IAAAA,EAAgC;QAC/C,MAAMF,SAA6C;YACjD,OAAO,IAAA,CAAK,KAAA;YACZ,OAAO,IAAA,CAAK,aAAA,GAAgB,KAAK,OAAA,CAAQ,OAAO,IAAI,GAAG;QACxD;QACD,IAAI,IAAA,CAAK,UAAA,EACP,OAAO,UAAA,GAAa,IAAA,CAAK,UAAA;QAE3B,IAAI,IAAA,CAAK,cAAA,EACP,OAAO,eAAA,GAAkB,IAAA,CAAK,cAAA;QAEhC,MAAM,EAAE,IAAA,EAAM,GAAG,MAAM,IAAA,CAAK,kBAAA,CAAmB,OAAO;QACtD,OAAO,IAAA,CAAK,EAAA,CAAG,SAAA;IAChB;;;;;;;IASD,MAAgB,mBACdG,OAAAA,EACA;QACA,IAAI,CAAC,IAAA,CAAK,MAAA,EAAQ;YAChB,MAAMC,uBAA6C;gBACjD,SAAS,IAAA,CAAK,YAAA,CAAa,OAAA;YAC5B;YAED,MAAM,eAAW,gLAAA,EAAY,qBAAqB;YAElD,MAAM,SAAS;gBACb,GAAG,IAAA,CAAK,YAAA;gBACR,SAAS;gBACT,SAAS,IAAA,CAAK,OAAA;gBACd,YAAY;YACb;YAED,IAAI,CAAC,OAAO,OAAA,EACV,OAAO,OAAO,OAAA;YAGhB,OAAO,cAAA,OAAiB,4LAAA,EAAwB,OAAO,cAAA,CAAe;YAEtE,IAAA,CAAK,MAAA,GAAS,IAAIC,6IAAAA,CAAa;QAChC;QACD,MAAM,iBAAiB,CAAE;QAEzB,OAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,YAAY;YAClC,IAAI;gBACF,MAAM,MAAM,MAAM,IAAA,CAAK,MAAA,CAAO,UAAA,CAAW,MAAA,CACvC,SACA,eACD;gBACD,OAAO;YACR,EAAA,OAAQ,GAAG;gBACV,MAAM,YAAQ,2LAAA,EAAsB,EAAE;gBACtC,MAAM;YACP;QACF,EAAC;IACH;AACF"}},
    {"offset": {"line": 5994, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/azure/embeddings.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/azure/embeddings.ts"],"sourcesContent":["import {\n  type ClientOptions,\n  AzureOpenAI as AzureOpenAIClient,\n  OpenAI as OpenAIClient,\n} from \"openai\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { OpenAIEmbeddings, OpenAIEmbeddingsParams } from \"../embeddings.js\";\nimport { AzureOpenAIInput, OpenAICoreRequestOptions } from \"../types.js\";\nimport {\n  getEndpoint,\n  OpenAIEndpointConfig,\n  getHeadersWithUserAgent,\n} from \"../utils/azure.js\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\n\nexport class AzureOpenAIEmbeddings extends OpenAIEmbeddings {\n  azureOpenAIApiVersion?: string;\n\n  azureOpenAIApiKey?: string;\n\n  azureADTokenProvider?: () => Promise<string>;\n\n  azureOpenAIApiInstanceName?: string;\n\n  azureOpenAIApiDeploymentName?: string;\n\n  azureOpenAIBasePath?: string;\n\n  constructor(\n    fields?: Partial<OpenAIEmbeddingsParams> &\n      Partial<AzureOpenAIInput> & {\n        verbose?: boolean;\n        /** The OpenAI API key to use. */\n        apiKey?: string;\n        configuration?: ClientOptions;\n        deploymentName?: string;\n        openAIApiVersion?: string;\n      }\n  ) {\n    super(fields);\n    this.batchSize = fields?.batchSize ?? 1;\n    this.azureOpenAIApiKey =\n      fields?.azureOpenAIApiKey ??\n      (typeof fields?.apiKey === \"string\" ? fields?.apiKey : undefined) ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_KEY\");\n\n    this.azureOpenAIApiVersion =\n      fields?.azureOpenAIApiVersion ??\n      fields?.openAIApiVersion ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_VERSION\");\n\n    this.azureOpenAIBasePath =\n      fields?.azureOpenAIBasePath ??\n      getEnvironmentVariable(\"AZURE_OPENAI_BASE_PATH\");\n\n    this.azureOpenAIApiInstanceName =\n      fields?.azureOpenAIApiInstanceName ??\n      getEnvironmentVariable(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n\n    this.azureOpenAIApiDeploymentName =\n      (fields?.azureOpenAIApiEmbeddingsDeploymentName ||\n        fields?.azureOpenAIApiDeploymentName) ??\n      (getEnvironmentVariable(\"AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\") ||\n        getEnvironmentVariable(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n\n    this.azureADTokenProvider = fields?.azureADTokenProvider;\n  }\n\n  protected async embeddingWithRetry(\n    request: OpenAIClient.EmbeddingCreateParams\n  ) {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n        azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n        azureOpenAIApiKey: this.azureOpenAIApiKey,\n        azureOpenAIBasePath: this.azureOpenAIBasePath,\n        azureADTokenProvider: this.azureADTokenProvider,\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n\n      const { apiKey: existingApiKey, ...clientConfigRest } = this.clientConfig;\n      const params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string } = {\n        ...clientConfigRest,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n\n      if (!this.azureADTokenProvider) {\n        params.apiKey = openAIEndpointConfig.azureOpenAIApiKey;\n      }\n\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      params.defaultHeaders = getHeadersWithUserAgent(\n        params.defaultHeaders,\n        true,\n        \"2.0.0\"\n      );\n\n      this.client = new AzureOpenAIClient({\n        apiVersion: this.azureOpenAIApiVersion,\n        azureADTokenProvider: this.azureADTokenProvider,\n        deployment: this.azureOpenAIApiDeploymentName,\n        ...params,\n      });\n    }\n    const requestOptions: OpenAICoreRequestOptions = {};\n    if (this.azureOpenAIApiKey) {\n      requestOptions.headers = {\n        \"api-key\": this.azureOpenAIApiKey,\n        ...requestOptions.headers,\n      };\n      requestOptions.query = {\n        \"api-version\": this.azureOpenAIApiVersion,\n        ...requestOptions.query,\n      };\n    }\n    return this.caller.call(async () => {\n      try {\n        const res = await this.client.embeddings.create(\n          request,\n          requestOptions\n        );\n        return res;\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n}\n"],"names":["fields?: Partial<OpenAIEmbeddingsParams> &\n      Partial<AzureOpenAIInput> & {\n        verbose?: boolean;\n        /** The OpenAI API key to use. */\n        apiKey?: string;\n        configuration?: ClientOptions;\n        deploymentName?: string;\n        openAIApiVersion?: string;\n      }","request: OpenAIClient.EmbeddingCreateParams","openAIEndpointConfig: OpenAIEndpointConfig","params: Omit<ClientOptions, \"apiKey\"> & { apiKey?: string }","AzureOpenAIClient","requestOptions: OpenAICoreRequestOptions"],"mappings":";;;;;;;;;;;;;;;;AAeA,IAAa,wBAAb,cAA2C,iLAAA,CAAiB;IAC1D,sBAAA;IAEA,kBAAA;IAEA,qBAAA;IAEA,2BAAA;IAEA,6BAAA;IAEA,oBAAA;IAEA,YACEA,MAAAA,CASA;QACA,KAAA,CAAM,OAAO;QACb,IAAA,CAAK,SAAA,GAAY,QAAQ,aAAa;QACtC,IAAA,CAAK,iBAAA,GACH,QAAQ,qBAAA,CACP,OAAO,QAAQ,WAAW,WAAW,QAAQ,SAAS,KAAA,CAAA,SACvD,uLAAA,EAAuB,uBAAuB;QAEhD,IAAA,CAAK,qBAAA,GACH,QAAQ,yBACR,QAAQ,wBACR,uLAAA,EAAuB,2BAA2B;QAEpD,IAAA,CAAK,mBAAA,GACH,QAAQ,2BACR,uLAAA,EAAuB,yBAAyB;QAElD,IAAA,CAAK,0BAAA,GACH,QAAQ,kCACR,uLAAA,EAAuB,iCAAiC;QAE1D,IAAA,CAAK,4BAAA,GAAA,CACF,QAAQ,0CACP,QAAQ,4BAAA,KAAA,KACT,uLAAA,EAAuB,8CAA8C,QACpE,uLAAA,EAAuB,mCAAmC;QAE9D,IAAA,CAAK,oBAAA,GAAuB,QAAQ;IACrC;IAED,MAAgB,mBACdC,OAAAA,EACA;QACA,IAAI,CAAC,IAAA,CAAK,MAAA,EAAQ;YAChB,MAAMC,uBAA6C;gBACjD,8BAA8B,IAAA,CAAK,4BAAA;gBACnC,4BAA4B,IAAA,CAAK,0BAAA;gBACjC,mBAAmB,IAAA,CAAK,iBAAA;gBACxB,qBAAqB,IAAA,CAAK,mBAAA;gBAC1B,sBAAsB,IAAA,CAAK,oBAAA;gBAC3B,SAAS,IAAA,CAAK,YAAA,CAAa,OAAA;YAC5B;YAED,MAAM,eAAW,gLAAA,EAAY,qBAAqB;YAElD,MAAM,EAAE,QAAQ,cAAA,EAAgB,GAAG,kBAAkB,GAAG,IAAA,CAAK,YAAA;YAC7D,MAAMC,SAA8D;gBAClE,GAAG,gBAAA;gBACH,SAAS;gBACT,SAAS,IAAA,CAAK,OAAA;gBACd,YAAY;YACb;YAED,IAAI,CAAC,IAAA,CAAK,oBAAA,EACR,OAAO,MAAA,GAAS,qBAAqB,iBAAA;YAGvC,IAAI,CAAC,OAAO,OAAA,EACV,OAAO,OAAO,OAAA;YAGhB,OAAO,cAAA,OAAiB,4LAAA,EACtB,OAAO,cAAA,EACP,MACA,QACD;YAED,IAAA,CAAK,MAAA,GAAS,IAAIC,iJAAAA,CAAkB;gBAClC,YAAY,IAAA,CAAK,qBAAA;gBACjB,sBAAsB,IAAA,CAAK,oBAAA;gBAC3B,YAAY,IAAA,CAAK,4BAAA;gBACjB,GAAG,MAAA;YACJ;QACF;QACD,MAAMC,iBAA2C,CAAE;QACnD,IAAI,IAAA,CAAK,iBAAA,EAAmB;YAC1B,eAAe,OAAA,GAAU;gBACvB,WAAW,IAAA,CAAK,iBAAA;gBAChB,GAAG,eAAe,OAAA;YACnB;YACD,eAAe,KAAA,GAAQ;gBACrB,eAAe,IAAA,CAAK,qBAAA;gBACpB,GAAG,eAAe,KAAA;YACnB;QACF;QACD,OAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,YAAY;YAClC,IAAI;gBACF,MAAM,MAAM,MAAM,IAAA,CAAK,MAAA,CAAO,UAAA,CAAW,MAAA,CACvC,SACA,eACD;gBACD,OAAO;YACR,EAAA,OAAQ,GAAG;gBACV,MAAM,YAAQ,2LAAA,EAAsB,EAAE;gBACtC,MAAM;YACP;QACF,EAAC;IACH;AACF"}},
    {"offset": {"line": 6083, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/dalle.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/dalle.ts"],"sourcesContent":["import { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { Tool, ToolParams } from \"@langchain/core/tools\";\nimport {\n  MessageContentComplex,\n  MessageContentImageUrl,\n} from \"@langchain/core/messages\";\n\n/**\n * @see https://platform.openai.com/docs/api-reference/images/create\n */\nexport type OpenAIImageModelId =\n  | OpenAIClient.ImageModel\n  | (string & NonNullable<unknown>);\n\n/**\n * An interface for the Dall-E API Wrapper.\n */\nexport interface DallEAPIWrapperParams extends ToolParams {\n  /**\n   * The OpenAI API key\n   * Alias for `apiKey`\n   */\n  openAIApiKey?: string;\n  /**\n   * The OpenAI API key\n   */\n  apiKey?: string;\n  /**\n   * The model to use.\n   * Alias for `model`\n   * @params \"dall-e-2\" | \"dall-e-3\"\n   * @default \"dall-e-3\"\n   * @deprecated Use `model` instead.\n   */\n  modelName?: OpenAIImageModelId;\n  /**\n   * The model to use.\n   * @params \"dall-e-2\" | \"dall-e-3\"\n   * @default \"dall-e-3\"\n   */\n  model?: OpenAIImageModelId;\n  /**\n   * The style of the generated images. Must be one of vivid or natural.\n   * Vivid causes the model to lean towards generating hyper-real and dramatic images.\n   * Natural causes the model to produce more natural, less hyper-real looking images.\n   * @default \"vivid\"\n   */\n  style?: \"natural\" | \"vivid\";\n  /**\n   * The quality of the image that will be generated. hd creates images with finer\n   * details and greater consistency across the image.\n   * @default \"standard\"\n   */\n  quality?: \"standard\" | \"hd\";\n  /**\n   * The number of images to generate.\n   * Must be between 1 and 10.\n   * For dall-e-3, only `n: 1` is supported.\n   * @default 1\n   */\n  n?: number;\n  /**\n   * The size of the generated images.\n   * Must be one of 256x256, 512x512, or 1024x1024 for DALLE-2 models.\n   * Must be one of 1024x1024, 1792x1024, or 1024x1792 for DALLE-3 models.\n   * @default \"1024x1024\"\n   */\n  size?: \"256x256\" | \"512x512\" | \"1024x1024\" | \"1792x1024\" | \"1024x1792\";\n  /**\n   * The format in which the generated images are returned.\n   * Must be one of \"url\" or \"b64_json\".\n   * @default \"url\"\n   */\n  dallEResponseFormat?: \"url\" | \"b64_json\";\n  /**\n   * @deprecated Use dallEResponseFormat instead for the Dall-E response type.\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  responseFormat?: any;\n  /**\n   * A unique identifier representing your end-user, which will help\n   * OpenAI to monitor and detect abuse.\n   */\n  user?: string;\n  /**\n   * The organization to use\n   */\n  organization?: string;\n  /**\n   * The base URL of the OpenAI API.\n   */\n  baseUrl?: string;\n}\n\n/**\n * A tool for generating images with Open AIs Dall-E 2 or 3 API.\n */\nexport class DallEAPIWrapper extends Tool {\n  static lc_name() {\n    return \"DallEAPIWrapper\";\n  }\n\n  name = \"dalle_api_wrapper\";\n\n  description =\n    \"A wrapper around OpenAI DALL-E API. Useful for when you need to generate images from a text description. Input should be an image description.\";\n\n  protected client: OpenAIClient;\n\n  static readonly toolName = \"dalle_api_wrapper\";\n\n  private model = \"dall-e-3\";\n\n  private style: \"natural\" | \"vivid\" = \"vivid\";\n\n  private quality: \"standard\" | \"hd\" = \"standard\";\n\n  private n = 1;\n\n  private size:\n    | \"256x256\"\n    | \"512x512\"\n    | \"1024x1024\"\n    | \"1792x1024\"\n    | \"1024x1792\" = \"1024x1024\";\n\n  private dallEResponseFormat: \"url\" | \"b64_json\" = \"url\";\n\n  private user?: string;\n\n  constructor(fields?: DallEAPIWrapperParams) {\n    // Shim for new base tool param name\n    if (\n      fields?.responseFormat !== undefined &&\n      [\"url\", \"b64_json\"].includes(fields.responseFormat)\n    ) {\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      fields.dallEResponseFormat = fields.responseFormat as any;\n      fields.responseFormat = \"content\";\n    }\n    super(fields);\n    const openAIApiKey =\n      fields?.apiKey ??\n      fields?.openAIApiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n\n    const organization =\n      fields?.organization ?? getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    const clientConfig = {\n      apiKey: openAIApiKey,\n      organization,\n      dangerouslyAllowBrowser: true,\n      baseURL: fields?.baseUrl,\n    };\n    this.client = new OpenAIClient(clientConfig);\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    this.style = fields?.style ?? this.style;\n    this.quality = fields?.quality ?? this.quality;\n    this.n = fields?.n ?? this.n;\n    this.size = fields?.size ?? this.size;\n    this.dallEResponseFormat =\n      fields?.dallEResponseFormat ?? this.dallEResponseFormat;\n    this.user = fields?.user;\n  }\n\n  /**\n   * Processes the API response if multiple images are generated.\n   * Returns a list of MessageContentImageUrl objects. If the response\n   * format is `url`, then the `image_url` field will contain the URL.\n   * If it is `b64_json`, then the `image_url` field will contain an object\n   * with a `url` field with the base64 encoded image.\n   *\n   * @param {OpenAIClient.Images.ImagesResponse[]} response The API response\n   * @returns {MessageContentImageUrl[]}\n   */\n  private processMultipleGeneratedUrls(\n    response: OpenAIClient.Images.ImagesResponse[]\n  ): MessageContentImageUrl[] {\n    if (this.dallEResponseFormat === \"url\") {\n      return response.flatMap((res) => {\n        const imageUrlContent =\n          res.data\n            ?.flatMap((item) => {\n              if (!item.url) return [];\n              return {\n                type: \"image_url\" as const,\n                image_url: item.url,\n              };\n            })\n            .filter(\n              (item) =>\n                item !== undefined &&\n                item.type === \"image_url\" &&\n                typeof item.image_url === \"string\" &&\n                item.image_url !== undefined\n            ) ?? [];\n        return imageUrlContent;\n      });\n    } else {\n      return response.flatMap((res) => {\n        const b64Content =\n          res.data\n            ?.flatMap((item) => {\n              if (!item.b64_json) return [];\n              return {\n                type: \"image_url\" as const,\n                image_url: {\n                  url: item.b64_json,\n                },\n              };\n            })\n            .filter(\n              (item) =>\n                item !== undefined &&\n                item.type === \"image_url\" &&\n                typeof item.image_url === \"object\" &&\n                \"url\" in item.image_url &&\n                typeof item.image_url.url === \"string\" &&\n                item.image_url.url !== undefined\n            ) ?? [];\n        return b64Content;\n      });\n    }\n  }\n\n  /** @ignore */\n  async _call(input: string): Promise<string | MessageContentComplex[]> {\n    const generateImageFields = {\n      model: this.model,\n      prompt: input,\n      n: 1,\n      size: this.size,\n      response_format: this.dallEResponseFormat,\n      style: this.style,\n      quality: this.quality,\n      user: this.user,\n    };\n\n    if (this.n > 1) {\n      const results = await Promise.all(\n        Array.from({ length: this.n }).map(() =>\n          this.client.images.generate(generateImageFields)\n        )\n      );\n\n      return this.processMultipleGeneratedUrls(results);\n    }\n\n    const response = await this.client.images.generate(generateImageFields);\n\n    let data = \"\";\n    if (this.dallEResponseFormat === \"url\") {\n      [data] =\n        response.data\n          ?.map((item) => item.url)\n          .filter((url): url is string => url !== \"undefined\") ?? [];\n    } else {\n      [data] =\n        response.data\n          ?.map((item) => item.b64_json)\n          .filter((b64_json): b64_json is string => b64_json !== \"undefined\") ??\n        [];\n    }\n    return data;\n  }\n}\n"],"names":["fields?: DallEAPIWrapperParams","OpenAIClient","response: OpenAIClient.Images.ImagesResponse[]","input: string"],"mappings":";;;;;;;;;;;;;;GAkGA,IAAa,kBAAb,cAAqC,uLAAA,CAAK;IACxC,OAAO,UAAU;QACf,OAAO;IACR;IAED,OAAO,oBAAA;IAEP,cACE,iJAAA;IAEQ,OAAA;IAEV,OAAgB,WAAW,oBAAA;IAEnB,QAAQ,WAAA;IAER,QAA6B,QAAA;IAE7B,UAA6B,WAAA;IAE7B,IAAI,EAAA;IAEJ,OAKU,YAAA;IAEV,sBAA0C,MAAA;IAE1C,KAAA;IAER,YAAYA,MAAAA,CAAgC;QAE1C,IACE,QAAQ,mBAAmB,KAAA,KAC3B;YAAC;YAAO,UAAW;SAAA,CAAC,QAAA,CAAS,OAAO,cAAA,CAAe,EACnD;YAEA,OAAO,mBAAA,GAAsB,OAAO,cAAA;YACpC,OAAO,cAAA,GAAiB;QACzB;QACD,KAAA,CAAM,OAAO;QACb,MAAM,eACJ,QAAQ,UACR,QAAQ,oBACR,uLAAA,EAAuB,iBAAiB;QAE1C,MAAM,eACJ,QAAQ,oBAAgB,uLAAA,EAAuB,sBAAsB;QAEvE,MAAM,eAAe;YACnB,QAAQ;YACR;YACA,yBAAyB;YACzB,SAAS,QAAQ;QAClB;QACD,IAAA,CAAK,MAAA,GAAS,IAAIC,6IAAAA,CAAa;QAC/B,IAAA,CAAK,KAAA,GAAQ,QAAQ,SAAS,QAAQ,aAAa,IAAA,CAAK,KAAA;QACxD,IAAA,CAAK,KAAA,GAAQ,QAAQ,SAAS,IAAA,CAAK,KAAA;QACnC,IAAA,CAAK,OAAA,GAAU,QAAQ,WAAW,IAAA,CAAK,OAAA;QACvC,IAAA,CAAK,CAAA,GAAI,QAAQ,KAAK,IAAA,CAAK,CAAA;QAC3B,IAAA,CAAK,IAAA,GAAO,QAAQ,QAAQ,IAAA,CAAK,IAAA;QACjC,IAAA,CAAK,mBAAA,GACH,QAAQ,uBAAuB,IAAA,CAAK,mBAAA;QACtC,IAAA,CAAK,IAAA,GAAO,QAAQ;IACrB;;;;;;;;;;IAYO,6BACNC,QAAAA,EAC0B;QAC1B,IAAI,IAAA,CAAK,mBAAA,KAAwB,MAC/B,CAAA,OAAO,SAAS,OAAA,CAAQ,CAAC,QAAQ;YAC/B,MAAM,kBACJ,IAAI,IAAA,EACA,QAAQ,CAAC,SAAS;gBAClB,IAAI,CAAC,KAAK,GAAA,CAAK,CAAA,OAAO,CAAE,CAAA;gBACxB,OAAO;oBACL,MAAM;oBACN,WAAW,KAAK,GAAA;gBACjB;YACF,EAAC,CACD,OACC,CAAC,OACC,SAAS,KAAA,KACT,KAAK,IAAA,KAAS,eACd,OAAO,KAAK,SAAA,KAAc,YAC1B,KAAK,SAAA,KAAc,KAAA,EACtB,IAAI,CAAE,CAAA;YACX,OAAO;QACR,EAAC;aAEF,OAAO,SAAS,OAAA,CAAQ,CAAC,QAAQ;YAC/B,MAAM,aACJ,IAAI,IAAA,EACA,QAAQ,CAAC,SAAS;gBAClB,IAAI,CAAC,KAAK,QAAA,CAAU,CAAA,OAAO,CAAE,CAAA;gBAC7B,OAAO;oBACL,MAAM;oBACN,WAAW;wBACT,KAAK,KAAK,QAAA;oBACX;gBACF;YACF,EAAC,CACD,OACC,CAAC,OACC,SAAS,KAAA,KACT,KAAK,IAAA,KAAS,eACd,OAAO,KAAK,SAAA,KAAc,YAC1B,SAAS,KAAK,SAAA,IACd,OAAO,KAAK,SAAA,CAAU,GAAA,KAAQ,YAC9B,KAAK,SAAA,CAAU,GAAA,KAAQ,KAAA,EAC1B,IAAI,CAAE,CAAA;YACX,OAAO;QACR,EAAC;IAEL;mBAGD,MAAM,MAAMC,KAAAA,EAA0D;QACpE,MAAM,sBAAsB;YAC1B,OAAO,IAAA,CAAK,KAAA;YACZ,QAAQ;YACR,GAAG;YACH,MAAM,IAAA,CAAK,IAAA;YACX,iBAAiB,IAAA,CAAK,mBAAA;YACtB,OAAO,IAAA,CAAK,KAAA;YACZ,SAAS,IAAA,CAAK,OAAA;YACd,MAAM,IAAA,CAAK,IAAA;QACZ;QAED,IAAI,IAAA,CAAK,CAAA,GAAI,GAAG;YACd,MAAM,UAAU,MAAM,QAAQ,GAAA,CAC5B,MAAM,IAAA,CAAK;gBAAE,QAAQ,IAAA,CAAK,CAAA;YAAG,EAAC,CAAC,GAAA,CAAI,IACjC,IAAA,CAAK,MAAA,CAAO,MAAA,CAAO,QAAA,CAAS,oBAAoB,CACjD,CACF;YAED,OAAO,IAAA,CAAK,4BAAA,CAA6B,QAAQ;QAClD;QAED,MAAM,WAAW,MAAM,IAAA,CAAK,MAAA,CAAO,MAAA,CAAO,QAAA,CAAS,oBAAoB;QAEvE,IAAI,OAAO;QACX,IAAI,IAAA,CAAK,mBAAA,KAAwB,OAC/B,CAAC,KAAK,GACJ,SAAS,IAAA,EACL,IAAI,CAAC,OAAS,KAAK,GAAA,CAAI,CACxB,OAAO,CAAC,MAAuB,QAAQ,YAAY,IAAI,CAAE,CAAA;aAE9D,CAAC,KAAK,GACJ,SAAS,IAAA,EACL,IAAI,CAAC,OAAS,KAAK,QAAA,CAAS,CAC7B,OAAO,CAAC,WAAiC,aAAa,YAAY,IACrE,CAAE,CAAA;QAEN,OAAO;IACR;AACF"}},
    {"offset": {"line": 6201, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/webSearch.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/webSearch.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * User location configuration for geographic search refinement.\n */\nexport interface WebSearchUserLocation {\n  /**\n   * The type of location. Currently only \"approximate\" is supported.\n   */\n  type: \"approximate\";\n  /**\n   * Two-letter ISO country code (e.g., \"US\", \"GB\").\n   * @see https://en.wikipedia.org/wiki/ISO_3166-1\n   */\n  country?: string;\n  /**\n   * City name (e.g., \"San Francisco\", \"London\").\n   */\n  city?: string;\n  /**\n   * Region or state name (e.g., \"California\", \"London\").\n   */\n  region?: string;\n  /**\n   * IANA timezone (e.g., \"America/Los_Angeles\", \"Europe/London\").\n   * @see https://timeapi.io/documentation/iana-timezones\n   */\n  timezone?: string;\n}\n\n/**\n * Domain filtering configuration for web search.\n */\nexport interface WebSearchFilters {\n  /**\n   * Allow-list of up to 100 domains to limit search results.\n   * Omit the HTTP/HTTPS prefix (e.g., \"openai.com\" instead of \"https://openai.com/\").\n   * Includes subdomains automatically.\n   */\n  allowedDomains?: string[];\n}\n\n/**\n * Options for the OpenAI web search tool.\n */\nexport interface WebSearchOptions {\n  /**\n   * Domain filtering configuration.\n   * Limit results to a specific set of up to 100 domains.\n   */\n  filters?: WebSearchFilters;\n  /**\n   * Approximate user location for geographic search refinement.\n   * Not supported for deep research models.\n   */\n  userLocation?: WebSearchUserLocation;\n  /**\n   * High level guidance for the amount of context window space to use for the\n   * search. One of `low`, `medium`, or `high`. `medium` is the default.\n   */\n  search_context_size?: \"low\" | \"medium\" | \"high\";\n}\n\n/**\n * OpenAI web search tool type for the Responses API.\n */\nexport type WebSearchTool = OpenAIClient.Responses.WebSearchTool;\n\n/**\n * Creates a web search tool that allows OpenAI models to search the web\n * for up-to-date information before generating a response.\n *\n * Web search supports three main types:\n * 1. **Non-reasoning web search**: Quick lookups where the model passes queries\n *    directly to the search tool.\n * 2. **Agentic search with reasoning models**: The model actively manages the\n *    search process, analyzing results and deciding whether to keep searching.\n * 3. **Deep research**: Extended investigations using models like `o3-deep-research`\n *    or `gpt-5` with high reasoning effort.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-web-search | OpenAI Web Search Documentation}\n * @param options - Configuration options for the web search tool\n * @returns A web search tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({\n *   model: \"gpt-4o\",\n * });\n *\n * // Basic usage\n * const response = await model.invoke(\"What was a positive news story from today?\", {\n *   tools: [tools.webSearch()],\n * });\n *\n * // With domain filtering\n * const filteredResponse = await model.invoke(\"Latest AI research news\", {\n *   tools: [tools.webSearch({\n *     filters: {\n *       allowedDomains: [\"arxiv.org\", \"nature.com\", \"science.org\"],\n *     },\n *   })],\n * });\n *\n * // With user location for geographic relevance\n * const localResponse = await model.invoke(\"What are the best restaurants near me?\", {\n *   tools: [tools.webSearch({\n *     userLocation: {\n *       type: \"approximate\",\n *       country: \"US\",\n *       city: \"San Francisco\",\n *       region: \"California\",\n *       timezone: \"America/Los_Angeles\",\n *     },\n *   })],\n * });\n *\n * // Cache-only mode (no live internet access)\n * const cachedResponse = await model.invoke(\"Find information about OpenAI\", {\n *   tools: [tools.webSearch({\n *     externalWebAccess: false,\n *   })],\n * });\n * ```\n */\nexport function webSearch(options?: WebSearchOptions): ServerTool {\n  return {\n    type: \"web_search\",\n    filters: options?.filters?.allowedDomains\n      ? { allowed_domains: options.filters.allowedDomains }\n      : undefined,\n    user_location: options?.userLocation,\n    search_context_size: options?.search_context_size,\n  } satisfies WebSearchTool;\n}\n"],"names":["options?: WebSearchOptions"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAgIA,SAAgB,UAAUA,OAAAA,EAAwC;IAChE,OAAO;QACL,MAAM;QACN,SAAS,SAAS,SAAS,iBACvB;YAAE,iBAAiB,QAAQ,OAAA,CAAQ,cAAA;QAAgB,IACnD,KAAA;QACJ,eAAe,SAAS;QACxB,qBAAqB,SAAS;IAC/B;AACF"}},
    {"offset": {"line": 6280, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/mcp.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/mcp.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Available connector IDs for OpenAI's built-in service connectors.\n * These are OpenAI-maintained MCP wrappers for popular services.\n */\nexport type McpConnectorId =\n  | \"connector_dropbox\"\n  | \"connector_gmail\"\n  | \"connector_googlecalendar\"\n  | \"connector_googledrive\"\n  | \"connector_microsoftteams\"\n  | \"connector_outlookcalendar\"\n  | \"connector_outlookemail\"\n  | \"connector_sharepoint\";\n\n/**\n * Filter object to specify which tools are allowed.\n */\nexport interface McpToolFilter {\n  /**\n   * List of allowed tool names.\n   */\n  toolNames?: string[];\n  /**\n   * Indicates whether or not a tool modifies data or is read-only.\n   * If an MCP server is annotated with `readOnlyHint`, it will match this filter.\n   */\n  readOnly?: boolean;\n}\n\n/**\n * Filter object for approval requirements.\n */\nexport interface McpApprovalFilter {\n  /**\n   * Tools that always require approval before execution.\n   */\n  always?: McpToolFilter;\n  /**\n   * Tools that never require approval.\n   */\n  never?: McpToolFilter;\n}\n\n/**\n * Base options shared between remote MCP servers and connectors.\n */\ninterface McpBaseOptions {\n  /**\n   * A label for this MCP server, used to identify it in tool calls.\n   */\n  serverLabel: string;\n  /**\n   * List of allowed tool names or a filter object.\n   * Use this to limit which tools from the MCP server are available to the model.\n   */\n  allowedTools?: string[] | McpToolFilter;\n  /**\n   * An OAuth access token for authentication with the MCP server.\n   * Your application must handle the OAuth authorization flow and provide the token here.\n   */\n  authorization?: string;\n  /**\n   * Optional HTTP headers to send to the MCP server.\n   * Use for authentication or other purposes.\n   */\n  headers?: Record<string, string>;\n  /**\n   * Specify which of the MCP server's tools require approval before execution.\n   * - `\"always\"`: All tools require approval\n   * - `\"never\"`: No tools require approval\n   * - `McpApprovalFilter`: Fine-grained control over which tools require approval\n   *\n   * @default \"always\" (approval required for all tools)\n   */\n  requireApproval?: \"always\" | \"never\" | McpApprovalFilter;\n  /**\n   * Optional description of the MCP server, used to provide more context to the model.\n   */\n  serverDescription?: string;\n}\n\n/**\n * Options for connecting to a remote MCP server via URL.\n */\nexport interface McpRemoteServerOptions extends McpBaseOptions {\n  /**\n   * The URL for the MCP server.\n   * The server must implement the Streamable HTTP or HTTP/SSE transport protocol.\n   */\n  serverUrl: string;\n}\n\n/**\n * Options for connecting to an OpenAI-maintained service connector.\n */\nexport interface McpConnectorOptions extends McpBaseOptions {\n  /**\n   * Identifier for the service connector.\n   * These are OpenAI-maintained MCP wrappers for popular services.\n   *\n   * Available connectors:\n   * - `connector_dropbox`: Dropbox file access\n   * - `connector_gmail`: Gmail email access\n   * - `connector_googlecalendar`: Google Calendar access\n   * - `connector_googledrive`: Google Drive file access\n   * - `connector_microsoftteams`: Microsoft Teams access\n   * - `connector_outlookcalendar`: Outlook Calendar access\n   * - `connector_outlookemail`: Outlook Email access\n   * - `connector_sharepoint`: SharePoint file access\n   */\n  connectorId: McpConnectorId;\n}\n\n/**\n * OpenAI MCP tool type for the Responses API.\n */\nexport type McpTool = OpenAIClient.Responses.Tool.Mcp;\n\n/**\n * Converts a McpToolFilter to the API format.\n */\nfunction convertToolFilter(\n  filter: McpToolFilter\n): OpenAIClient.Responses.Tool.Mcp.McpToolFilter {\n  return {\n    tool_names: filter.toolNames,\n    read_only: filter.readOnly,\n  };\n}\n\n/**\n * Converts allowed_tools option to API format.\n */\nfunction convertAllowedTools(\n  allowedTools: string[] | McpToolFilter | undefined\n): Array<string> | OpenAIClient.Responses.Tool.Mcp.McpToolFilter | undefined {\n  if (!allowedTools) return undefined;\n  if (Array.isArray(allowedTools)) return allowedTools;\n  return convertToolFilter(allowedTools);\n}\n\n/**\n * Converts require_approval option to API format.\n */\nfunction convertRequireApproval(\n  requireApproval: \"always\" | \"never\" | McpApprovalFilter | undefined\n):\n  | OpenAIClient.Responses.Tool.Mcp.McpToolApprovalFilter\n  | \"always\"\n  | \"never\"\n  | undefined {\n  if (!requireApproval) return undefined;\n  if (typeof requireApproval === \"string\") return requireApproval;\n  return {\n    always: requireApproval.always\n      ? convertToolFilter(requireApproval.always)\n      : undefined,\n    never: requireApproval.never\n      ? convertToolFilter(requireApproval.never)\n      : undefined,\n  };\n}\n\n/**\n * Creates an MCP tool that connects to a remote MCP server or OpenAI service connector.\n * This allows OpenAI models to access external tools and services via the Model Context Protocol.\n *\n * There are two ways to use MCP tools:\n * 1. **Remote MCP servers**: Connect to any server on the public Internet that implements\n *    the MCP protocol using `serverUrl`.\n * 2. **Connectors**: Use OpenAI-maintained MCP wrappers for popular services like\n *    Google Workspace or Dropbox using `connectorId`.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-remote-mcp | OpenAI MCP Documentation}\n *\n * @param options - Configuration options for the MCP tool\n * @returns An MCP tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n *\n * // Using a remote MCP server\n * const response = await model.invoke(\"Roll 2d4+1\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"dmcp\",\n *     serverDescription: \"A D&D MCP server for dice rolling\",\n *     serverUrl: \"https://dmcp-server.deno.dev/sse\",\n *     requireApproval: \"never\",\n *   })],\n * });\n *\n * // Using a connector (e.g., Google Calendar)\n * const calendarResponse = await model.invoke(\"What's on my calendar today?\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"google_calendar\",\n *     connectorId: \"connector_googlecalendar\",\n *     authorization: \"<oauth-access-token>\",\n *     requireApproval: \"never\",\n *   })],\n * });\n *\n * // With tool filtering - only allow specific tools\n * const filteredResponse = await model.invoke(\"Roll some dice\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"dmcp\",\n *     serverUrl: \"https://dmcp-server.deno.dev/sse\",\n *     allowedTools: [\"roll\"],  // Only allow the \"roll\" tool\n *     requireApproval: \"never\",\n *   })],\n * });\n *\n * // With fine-grained approval control\n * const controlledResponse = await model.invoke(\"Search and modify files\", {\n *   tools: [tools.mcp({\n *     serverLabel: \"deepwiki\",\n *     serverUrl: \"https://mcp.deepwiki.com/mcp\",\n *     requireApproval: {\n *       never: { toolNames: [\"ask_question\", \"read_wiki_structure\"] },\n *       // All other tools will require approval\n *     },\n *   })],\n * });\n * ```\n */\nexport function mcp(options: McpRemoteServerOptions): ServerTool;\nexport function mcp(options: McpConnectorOptions): ServerTool;\nexport function mcp(\n  options: McpRemoteServerOptions | McpConnectorOptions\n): ServerTool {\n  const baseConfig: McpTool = {\n    type: \"mcp\",\n    server_label: options.serverLabel,\n    allowed_tools: convertAllowedTools(options.allowedTools),\n    authorization: options.authorization,\n    headers: options.headers,\n    require_approval: convertRequireApproval(options.requireApproval),\n    server_description: options.serverDescription,\n  };\n\n  if (\"serverUrl\" in options) {\n    return {\n      ...baseConfig,\n      server_url: options.serverUrl,\n    } satisfies McpTool;\n  }\n\n  return {\n    ...baseConfig,\n    connector_id: options.connectorId,\n  } satisfies McpTool;\n}\n"],"names":["filter: McpToolFilter","allowedTools: string[] | McpToolFilter | undefined","requireApproval: \"always\" | \"never\" | McpApprovalFilter | undefined","options: McpRemoteServerOptions | McpConnectorOptions","baseConfig: McpTool"],"mappings":";;;;;;;GA4HA,SAAS,kBACPA,MAAAA,EAC+C;IAC/C,OAAO;QACL,YAAY,OAAO,SAAA;QACnB,WAAW,OAAO,QAAA;IACnB;AACF;;;GAKD,SAAS,oBACPC,YAAAA,EAC2E;IAC3E,IAAI,CAAC,aAAc,CAAA,OAAO,KAAA;IAC1B,IAAI,MAAM,OAAA,CAAQ,aAAa,CAAE,CAAA,OAAO;IACxC,OAAO,kBAAkB,aAAa;AACvC;;;GAKD,SAAS,uBACPC,eAAAA,EAKY;IACZ,IAAI,CAAC,gBAAiB,CAAA,OAAO,KAAA;IAC7B,IAAI,OAAO,oBAAoB,SAAU,CAAA,OAAO;IAChD,OAAO;QACL,QAAQ,gBAAgB,MAAA,GACpB,kBAAkB,gBAAgB,MAAA,CAAO,GACzC,KAAA;QACJ,OAAO,gBAAgB,KAAA,GACnB,kBAAkB,gBAAgB,KAAA,CAAM,GACxC,KAAA;IACL;AACF;AAoED,SAAgB,IACdC,OAAAA,EACY;IACZ,MAAMC,aAAsB;QAC1B,MAAM;QACN,cAAc,QAAQ,WAAA;QACtB,eAAe,oBAAoB,QAAQ,YAAA,CAAa;QACxD,eAAe,QAAQ,aAAA;QACvB,SAAS,QAAQ,OAAA;QACjB,kBAAkB,uBAAuB,QAAQ,eAAA,CAAgB;QACjE,oBAAoB,QAAQ,iBAAA;IAC7B;IAED,IAAI,eAAe,QACjB,CAAA,OAAO;QACL,GAAG,UAAA;QACH,YAAY,QAAQ,SAAA;IACrB;IAGH,OAAO;QACL,GAAG,UAAA;QACH,cAAc,QAAQ,WAAA;IACvB;AACF"}},
    {"offset": {"line": 6335, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/codeInterpreter.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/codeInterpreter.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Memory limit options for the Code Interpreter container.\n * Higher tiers offer more RAM and are billed at different rates.\n */\nexport type CodeInterpreterMemoryLimit = \"1g\" | \"4g\" | \"16g\" | \"64g\";\n\n/**\n * Auto container configuration for Code Interpreter.\n * Creates a new container automatically or reuses an active one.\n */\nexport interface CodeInterpreterAutoContainer {\n  /**\n   * Memory limit for the container.\n   * - `\"1g\"` (default): 1 GB RAM\n   * - `\"4g\"`: 4 GB RAM\n   * - `\"16g\"`: 16 GB RAM\n   * - `\"64g\"`: 64 GB RAM\n   */\n  memoryLimit?: CodeInterpreterMemoryLimit;\n  /**\n   * Optional list of uploaded file IDs to make available to the code.\n   * Files in the model input are automatically uploaded, so this is only\n   * needed for additional files.\n   */\n  fileIds?: string[];\n}\n\n/**\n * Options for the Code Interpreter tool.\n */\nexport interface CodeInterpreterOptions {\n  /**\n   * The container configuration for the Code Interpreter.\n   *\n   * Can be either:\n   * - A string container ID for explicit mode (created via `/v1/containers` endpoint)\n   * - An auto configuration object that creates/reuses containers automatically\n   *\n   * If not provided, defaults to auto mode with default settings.\n   */\n  container?: string | CodeInterpreterAutoContainer;\n}\n\n/**\n * OpenAI Code Interpreter tool type for the Responses API.\n */\nexport type CodeInterpreterTool = OpenAIClient.Responses.Tool.CodeInterpreter;\n\n/**\n * Converts container options to the API format.\n */\nfunction convertContainer(\n  container: string | CodeInterpreterAutoContainer | undefined\n):\n  | string\n  | OpenAIClient.Responses.Tool.CodeInterpreter.CodeInterpreterToolAuto {\n  // If a string container ID is provided, use it directly\n  if (typeof container === \"string\") {\n    return container;\n  }\n\n  // Auto mode configuration\n  return {\n    type: \"auto\",\n    file_ids: container?.fileIds,\n    memory_limit: container?.memoryLimit,\n  };\n}\n\n/**\n * Creates a Code Interpreter tool that allows models to write and run Python code\n * in a sandboxed environment to solve complex problems.\n *\n * Use Code Interpreter for:\n * - **Data analysis**: Processing files with diverse data and formatting\n * - **File generation**: Creating files with data and images of graphs\n * - **Iterative coding**: Writing and running code iteratively to solve problems\n * - **Visual intelligence**: Cropping, zooming, rotating, and transforming images\n *\n * The tool runs in a container, which is a fully sandboxed virtual machine.\n * Containers can be created automatically (auto mode) or explicitly via the\n * `/v1/containers` endpoint.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-code-interpreter | OpenAI Code Interpreter Documentation}\n *\n * @param options - Configuration options for the Code Interpreter tool\n * @returns A Code Interpreter tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4.1\" });\n *\n * // Basic usage with auto container (default 1GB memory)\n * const response = await model.invoke(\n *   \"Solve the equation 3x + 11 = 14\",\n *   { tools: [tools.codeInterpreter()] }\n * );\n *\n * // With increased memory limit for larger computations\n * const response = await model.invoke(\n *   \"Analyze this large dataset and create visualizations\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: { memoryLimit: \"4g\" }\n *     })]\n *   }\n * );\n *\n * // With specific files available to the code\n * const response = await model.invoke(\n *   \"Process the uploaded CSV file\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: {\n *         memoryLimit: \"4g\",\n *         fileIds: [\"file-abc123\", \"file-def456\"]\n *       }\n *     })]\n *   }\n * );\n *\n * // Using an explicit container ID (created via /v1/containers)\n * const response = await model.invoke(\n *   \"Continue working with the data\",\n *   {\n *     tools: [tools.codeInterpreter({\n *       container: \"cntr_abc123\"\n *     })]\n *   }\n * );\n * ```\n *\n * @remarks\n * - Containers expire after 20 minutes of inactivity\n * - While called \"Code Interpreter\", the model knows it as the \"python tool\"\n * - For explicit prompting, ask for \"the python tool\" in your prompts\n * - Files in model input are automatically uploaded to the container\n * - Generated files are returned as `container_file_citation` annotations\n */\nexport function codeInterpreter(options?: CodeInterpreterOptions): ServerTool {\n  return {\n    type: \"code_interpreter\",\n    container: convertContainer(options?.container),\n  } satisfies CodeInterpreterTool;\n}\n"],"names":["container: string | CodeInterpreterAutoContainer | undefined","options?: CodeInterpreterOptions"],"mappings":";;;;;;;GAsDA,SAAS,iBACPA,SAAAA,EAGsE;IAEtE,IAAI,OAAO,cAAc,SACvB,CAAA,OAAO;IAIT,OAAO;QACL,MAAM;QACN,UAAU,WAAW;QACrB,cAAc,WAAW;IAC1B;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA0ED,SAAgB,gBAAgBC,OAAAA,EAA8C;IAC5E,OAAO;QACL,MAAM;QACN,WAAW,iBAAiB,SAAS,UAAU;IAChD;AACF"}},
    {"offset": {"line": 6433, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/fileSearch.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/fileSearch.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Comparison operators for file attribute filtering.\n */\nexport type FileSearchComparisonType =\n  | \"eq\"\n  | \"ne\"\n  | \"gt\"\n  | \"gte\"\n  | \"lt\"\n  | \"lte\";\n\n/**\n * A filter used to compare a specified attribute key to a given value.\n */\nexport interface FileSearchComparisonFilter {\n  /**\n   * The comparison operator to use.\n   * - `eq`: equals\n   * - `ne`: not equal\n   * - `gt`: greater than\n   * - `gte`: greater than or equal\n   * - `lt`: less than\n   * - `lte`: less than or equal\n   */\n  type: FileSearchComparisonType;\n  /**\n   * The attribute key to compare against.\n   */\n  key: string;\n  /**\n   * The value to compare against the attribute key.\n   * Supports string, number, boolean, or array of strings/numbers.\n   */\n  value: string | number | boolean | Array<string | number>;\n}\n\n/**\n * Combine multiple filters using `and` or `or`.\n */\nexport interface FileSearchCompoundFilter {\n  /**\n   * Type of operation: `and` or `or`.\n   */\n  type: \"and\" | \"or\";\n  /**\n   * Array of filters to combine.\n   */\n  filters: Array<FileSearchComparisonFilter | FileSearchCompoundFilter>;\n}\n\n/**\n * Filter type for file search - can be a comparison or compound filter.\n */\nexport type FileSearchFilter =\n  | FileSearchComparisonFilter\n  | FileSearchCompoundFilter;\n\n/**\n * Weights for hybrid search balancing semantic and keyword matches.\n */\nexport interface FileSearchHybridSearchWeights {\n  /**\n   * The weight of semantic embedding matches (0-1).\n   */\n  embeddingWeight: number;\n  /**\n   * The weight of keyword/text matches (0-1).\n   */\n  textWeight: number;\n}\n\n/**\n * Ranking options for file search results.\n */\nexport interface FileSearchRankingOptions {\n  /**\n   * The ranker to use for the file search.\n   * - `auto`: Automatically select the best ranker\n   * - `default-2024-11-15`: Default ranker as of November 2024\n   */\n  ranker?: \"auto\" | \"default-2024-11-15\";\n  /**\n   * The score threshold for results (0-1).\n   * Numbers closer to 1 return only the most relevant results but may return fewer.\n   */\n  scoreThreshold?: number;\n  /**\n   * Weights that control how reciprocal rank fusion balances semantic\n   * embedding matches versus sparse keyword matches when hybrid search is enabled.\n   */\n  hybridSearch?: FileSearchHybridSearchWeights;\n}\n\n/**\n * Options for the File Search tool.\n */\nexport interface FileSearchOptions {\n  /**\n   * The IDs of the vector stores to search.\n   * You must have previously created vector stores and uploaded files to them.\n   */\n  vectorStoreIds: string[];\n  /**\n   * The maximum number of results to return.\n   * Must be between 1 and 50 inclusive.\n   */\n  maxNumResults?: number;\n  /**\n   * A filter to apply based on file attributes/metadata.\n   * Use this to narrow down search results to specific categories or file types.\n   */\n  filters?: FileSearchFilter;\n  /**\n   * Ranking options to customize how results are scored and ordered.\n   */\n  rankingOptions?: FileSearchRankingOptions;\n}\n\n/**\n * OpenAI File Search tool type for the Responses API.\n */\nexport type FileSearchTool = OpenAIClient.Responses.FileSearchTool;\n\n/**\n * Converts ranking options to the API format.\n */\nfunction convertRankingOptions(\n  options: FileSearchRankingOptions | undefined\n): OpenAIClient.Responses.FileSearchTool.RankingOptions | undefined {\n  if (!options) return undefined;\n  return {\n    ranker: options.ranker,\n    score_threshold: options.scoreThreshold,\n    hybrid_search: options.hybridSearch\n      ? {\n          embedding_weight: options.hybridSearch.embeddingWeight,\n          text_weight: options.hybridSearch.textWeight,\n        }\n      : undefined,\n  };\n}\n\n/**\n * Creates a File Search tool that allows models to search your files\n * for relevant information using semantic and keyword search.\n *\n * File Search enables models to retrieve information from a knowledge base\n * of previously uploaded files stored in vector stores. This is a hosted tool\n * managed by OpenAI - you don't need to implement the search execution yourself.\n *\n * **Prerequisites**: Before using File Search, you must:\n * 1. Upload files to the File API with `purpose: \"assistants\"`\n * 2. Create a vector store\n * 3. Add files to the vector store\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-file-search | OpenAI File Search Documentation}\n *\n * @param options - Configuration options for the File Search tool\n * @returns A File Search tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4.1\" });\n *\n * // Basic usage with a vector store\n * const response = await model.invoke(\n *   \"What is deep research by OpenAI?\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"]\n *     })]\n *   }\n * );\n *\n * // Limit the number of results for lower latency\n * const response = await model.invoke(\n *   \"Find information about pricing\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       maxNumResults: 5\n *     })]\n *   }\n * );\n *\n * // With metadata filtering\n * const response = await model.invoke(\n *   \"Find recent blog posts about AI\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       filters: {\n *         type: \"eq\",\n *         key: \"category\",\n *         value: \"blog\"\n *       }\n *     })]\n *   }\n * );\n *\n * // With compound filters (AND/OR)\n * const response = await model.invoke(\n *   \"Find technical docs from 2024\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       filters: {\n *         type: \"and\",\n *         filters: [\n *           { type: \"eq\", key: \"category\", value: \"technical\" },\n *           { type: \"gte\", key: \"year\", value: 2024 }\n *         ]\n *       }\n *     })]\n *   }\n * );\n *\n * // With ranking options for more relevant results\n * const response = await model.invoke(\n *   \"Find the most relevant information\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\"],\n *       rankingOptions: {\n *         scoreThreshold: 0.8,\n *         ranker: \"auto\"\n *       }\n *     })]\n *   }\n * );\n *\n * // Search multiple vector stores\n * const response = await model.invoke(\n *   \"Search across all knowledge bases\",\n *   {\n *     tools: [tools.fileSearch({\n *       vectorStoreIds: [\"vs_abc123\", \"vs_def456\"]\n *     })]\n *   }\n * );\n * ```\n *\n * @remarks\n * - Vector stores must be created and populated before using this tool\n * - The tool returns file citations in the response annotations\n * - Use `include: [\"file_search_call.results\"]` in the API call to get search results\n * - Supported file types include PDF, DOCX, TXT, MD, and many code file formats\n */\nexport function fileSearch(options: FileSearchOptions): ServerTool {\n  return {\n    type: \"file_search\",\n    vector_store_ids: options.vectorStoreIds,\n    max_num_results: options.maxNumResults,\n    filters: options.filters as FileSearchTool[\"filters\"],\n    ranking_options: convertRankingOptions(options.rankingOptions),\n  } satisfies FileSearchTool;\n}\n"],"names":["options: FileSearchRankingOptions | undefined","options: FileSearchOptions"],"mappings":";;;;;;;GAiIA,SAAS,sBACPA,OAAAA,EACkE;IAClE,IAAI,CAAC,QAAS,CAAA,OAAO,KAAA;IACrB,OAAO;QACL,QAAQ,QAAQ,MAAA;QAChB,iBAAiB,QAAQ,cAAA;QACzB,eAAe,QAAQ,YAAA,GACnB;YACE,kBAAkB,QAAQ,YAAA,CAAa,eAAA;YACvC,aAAa,QAAQ,YAAA,CAAa,UAAA;QACnC,IACD,KAAA;IACL;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA8GD,SAAgB,WAAWC,OAAAA,EAAwC;IACjE,OAAO;QACL,MAAM;QACN,kBAAkB,QAAQ,cAAA;QAC1B,iBAAiB,QAAQ,aAAA;QACzB,SAAS,QAAQ,OAAA;QACjB,iBAAiB,sBAAsB,QAAQ,cAAA,CAAe;IAC/D;AACF"}},
    {"offset": {"line": 6573, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/imageGeneration.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/imageGeneration.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport type { ServerTool } from \"@langchain/core/tools\";\n\n/**\n * Optional mask for inpainting. Allows you to specify areas of the image\n * that should be regenerated.\n */\nexport interface ImageGenerationInputMask {\n  /**\n   * Base64-encoded mask image URL.\n   */\n  imageUrl?: string;\n  /**\n   * File ID for the mask image (uploaded via OpenAI File API).\n   */\n  fileId?: string;\n}\n\n/**\n * Options for the Image Generation tool.\n */\nexport interface ImageGenerationOptions {\n  /**\n   * Background type for the generated image.\n   * - `transparent`: Generate image with transparent background\n   * - `opaque`: Generate image with opaque background\n   * - `auto`: Let the model decide based on the prompt\n   * @default \"auto\"\n   */\n  background?: \"transparent\" | \"opaque\" | \"auto\";\n\n  /**\n   * Control how much effort the model will exert to match the style and features,\n   * especially facial features, of input images. This parameter is only supported\n   * for `gpt-image-1`. Unsupported for `gpt-image-1-mini`.\n   * - `high`: Higher fidelity to input images\n   * - `low`: Lower fidelity to input images\n   * @default \"low\"\n   */\n  inputFidelity?: \"high\" | \"low\";\n\n  /**\n   * Optional mask for inpainting. Use this to specify areas of an image\n   * that should be regenerated.\n   */\n  inputImageMask?: ImageGenerationInputMask;\n\n  /**\n   * The image generation model to use.\n   * @default \"gpt-image-1\"\n   */\n  model?: \"gpt-image-1\" | \"gpt-image-1-mini\";\n\n  /**\n   * Moderation level for the generated image.\n   * - `auto`: Standard moderation\n   * - `low`: Less restrictive moderation\n   * @default \"auto\"\n   */\n  moderation?: \"auto\" | \"low\";\n\n  /**\n   * Compression level for the output image (0-100).\n   * Only applies to JPEG and WebP formats.\n   * @default 100\n   */\n  outputCompression?: number;\n\n  /**\n   * The output format of the generated image.\n   * @default \"png\"\n   */\n  outputFormat?: \"png\" | \"webp\" | \"jpeg\";\n\n  /**\n   * Number of partial images to generate in streaming mode (0-3).\n   * When set, the model will return partial images as they are generated,\n   * providing faster visual feedback.\n   * @default 0\n   */\n  partialImages?: number;\n\n  /**\n   * The quality of the generated image.\n   * - `low`: Faster generation, lower quality\n   * - `medium`: Balanced generation time and quality\n   * - `high`: Slower generation, higher quality\n   * - `auto`: Let the model decide based on the prompt\n   * @default \"auto\"\n   */\n  quality?: \"low\" | \"medium\" | \"high\" | \"auto\";\n\n  /**\n   * The size of the generated image.\n   * - `1024x1024`: Square format\n   * - `1024x1536`: Portrait format\n   * - `1536x1024`: Landscape format\n   * - `auto`: Let the model decide based on the prompt\n   * @default \"auto\"\n   */\n  size?: \"1024x1024\" | \"1024x1536\" | \"1536x1024\" | \"auto\";\n}\n\n/**\n * OpenAI Image Generation tool type for the Responses API.\n */\nexport type ImageGenerationTool = OpenAIClient.Responses.Tool.ImageGeneration;\n\n/**\n * Converts input mask options to the API format.\n */\nfunction convertInputImageMask(\n  mask: ImageGenerationInputMask | undefined\n): ImageGenerationTool[\"input_image_mask\"] {\n  if (!mask) return undefined;\n  return {\n    image_url: mask.imageUrl,\n    file_id: mask.fileId,\n  };\n}\n\n/**\n * Creates an Image Generation tool that allows models to generate or edit images\n * using text prompts and optional image inputs.\n *\n * The image generation tool leverages the GPT Image model and automatically\n * optimizes text inputs for improved performance. When included in a request,\n * the model can decide when and how to generate images as part of the conversation.\n *\n * **Key Features**:\n * - Generate images from text descriptions\n * - Edit existing images with text instructions\n * - Multi-turn image editing by referencing previous responses\n * - Configurable output options (size, quality, format)\n * - Streaming support for partial image generation\n *\n * **Prompting Tips**:\n * - Use terms like \"draw\" or \"edit\" in your prompt for best results\n * - For combining images, say \"edit the first image by adding this element\" instead of \"combine\"\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-image-generation | OpenAI Image Generation Documentation}\n *\n * @param options - Configuration options for the Image Generation tool\n * @returns An Image Generation tool definition to be passed to the OpenAI Responses API\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-4o\" });\n *\n * // Basic usage - generate an image\n * const response = await model.invoke(\n *   \"Generate an image of a gray tabby cat hugging an otter with an orange scarf\",\n *   { tools: [tools.imageGeneration()] }\n * );\n *\n * // Access the generated image\n * const imageData = response.additional_kwargs.tool_outputs?.find(\n *   (output) => output.type === \"image_generation_call\"\n * );\n * if (imageData?.result) {\n *   // imageData.result contains the base64-encoded image\n *   const fs = await import(\"fs\");\n *   fs.writeFileSync(\"output.png\", Buffer.from(imageData.result, \"base64\"));\n * }\n *\n * // With custom options\n * const response = await model.invoke(\n *   \"Draw a beautiful sunset over mountains\",\n *   {\n *     tools: [tools.imageGeneration({\n *       size: \"1536x1024\",      // Landscape format\n *       quality: \"high\",        // Higher quality output\n *       outputFormat: \"jpeg\",   // JPEG format\n *       outputCompression: 90,  // 90% compression\n *     })]\n *   }\n * );\n *\n * // With transparent background\n * const response = await model.invoke(\n *   \"Create a logo with a transparent background\",\n *   {\n *     tools: [tools.imageGeneration({\n *       background: \"transparent\",\n *       outputFormat: \"png\",\n *     })]\n *   }\n * );\n *\n * // Force the model to use image generation\n * const response = await model.invoke(\n *   \"A serene lake at dawn\",\n *   {\n *     tools: [tools.imageGeneration()],\n *     tool_choice: { type: \"image_generation\" },\n *   }\n * );\n *\n * // Enable streaming with partial images\n * const response = await model.invoke(\n *   \"Draw a detailed fantasy castle\",\n *   {\n *     tools: [tools.imageGeneration({\n *       partialImages: 2,  // Get 2 partial images during generation\n *     })]\n *   }\n * );\n * ```\n *\n * @remarks\n * - Supported models: gpt-4o, gpt-4o-mini, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, o3\n * - The image generation process always uses `gpt-image-1` model internally\n * - The model will automatically revise prompts for improved performance\n * - Access the revised prompt via `revised_prompt` field in the output\n * - Multi-turn editing is supported by passing previous response messages\n */\nexport function imageGeneration(options?: ImageGenerationOptions): ServerTool {\n  return {\n    type: \"image_generation\",\n    background: options?.background,\n    input_fidelity: options?.inputFidelity,\n    input_image_mask: convertInputImageMask(options?.inputImageMask),\n    model: options?.model,\n    moderation: options?.moderation,\n    output_compression: options?.outputCompression,\n    output_format: options?.outputFormat,\n    partial_images: options?.partialImages,\n    quality: options?.quality,\n    size: options?.size,\n  } satisfies ImageGenerationTool;\n}\n"],"names":["mask: ImageGenerationInputMask | undefined","options?: ImageGenerationOptions"],"mappings":";;;;;;;GA+GA,SAAS,sBACPA,IAAAA,EACyC;IACzC,IAAI,CAAC,KAAM,CAAA,OAAO,KAAA;IAClB,OAAO;QACL,WAAW,KAAK,QAAA;QAChB,SAAS,KAAK,MAAA;IACf;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAmGD,SAAgB,gBAAgBC,OAAAA,EAA8C;IAC5E,OAAO;QACL,MAAM;QACN,YAAY,SAAS;QACrB,gBAAgB,SAAS;QACzB,kBAAkB,sBAAsB,SAAS,eAAe;QAChE,OAAO,SAAS;QAChB,YAAY,SAAS;QACrB,oBAAoB,SAAS;QAC7B,eAAe,SAAS;QACxB,gBAAgB,SAAS;QACzB,SAAS,SAAS;QAClB,MAAM,SAAS;IAChB;AACF"}},
    {"offset": {"line": 6704, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/computerUse.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/computerUse.ts"],"sourcesContent":["/* eslint-disable @typescript-eslint/no-explicit-any */\nimport { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\nimport { type ToolRuntime } from \"@langchain/core/tools\";\nimport {\n  ToolMessage,\n  type AIMessage,\n  type BaseMessage,\n} from \"@langchain/core/messages\";\n\n/**\n * The type of computer environment to control.\n */\nexport type ComputerUseEnvironment =\n  | \"browser\"\n  | \"mac\"\n  | \"windows\"\n  | \"linux\"\n  | \"ubuntu\";\n\n/**\n * Re-export action types from OpenAI SDK for convenience.\n */\nexport type ComputerUseClickAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Click;\nexport type ComputerUseDoubleClickAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.DoubleClick;\nexport type ComputerUseDragAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Drag;\nexport type ComputerUseKeypressAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Keypress;\nexport type ComputerUseMoveAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Move;\nexport type ComputerUseScreenshotAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Screenshot;\nexport type ComputerUseScrollAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Scroll;\nexport type ComputerUseTypeAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Type;\nexport type ComputerUseWaitAction =\n  OpenAIClient.Responses.ResponseComputerToolCall.Wait;\n\n/**\n * Union type of all computer use actions from OpenAI SDK.\n */\nexport type ComputerUseAction =\n  OpenAIClient.Responses.ResponseComputerToolCall[\"action\"];\n\n// Zod schemas for computer use actions\nconst ComputerUseScreenshotActionSchema = z.object({\n  type: z.literal(\"screenshot\"),\n});\n\nconst ComputerUseClickActionSchema = z.object({\n  type: z.literal(\"click\"),\n  x: z.number(),\n  y: z.number(),\n  button: z.enum([\"left\", \"right\", \"wheel\", \"back\", \"forward\"]).default(\"left\"),\n});\n\nconst ComputerUseDoubleClickActionSchema = z.object({\n  type: z.literal(\"double_click\"),\n  x: z.number(),\n  y: z.number(),\n  button: z.enum([\"left\", \"right\", \"wheel\", \"back\", \"forward\"]).default(\"left\"),\n});\n\nconst ComputerUseDragActionSchema = z.object({\n  type: z.literal(\"drag\"),\n  path: z.array(z.object({ x: z.number(), y: z.number() })),\n});\n\nconst ComputerUseKeypressActionSchema = z.object({\n  type: z.literal(\"keypress\"),\n  keys: z.array(z.string()),\n});\n\nconst ComputerUseMoveActionSchema = z.object({\n  type: z.literal(\"move\"),\n  x: z.number(),\n  y: z.number(),\n});\n\nconst ComputerUseScrollActionSchema = z.object({\n  type: z.literal(\"scroll\"),\n  x: z.number(),\n  y: z.number(),\n  scroll_x: z.number(),\n  scroll_y: z.number(),\n});\n\nconst ComputerUseTypeActionSchema = z.object({\n  type: z.literal(\"type\"),\n  text: z.string(),\n});\n\nconst ComputerUseWaitActionSchema = z.object({\n  type: z.literal(\"wait\"),\n  duration: z.number().optional(),\n});\n\n// Discriminated union schema for individual action types\nconst ComputerUseActionUnionSchema = z.discriminatedUnion(\"type\", [\n  ComputerUseScreenshotActionSchema,\n  ComputerUseClickActionSchema,\n  ComputerUseDoubleClickActionSchema,\n  ComputerUseDragActionSchema,\n  ComputerUseKeypressActionSchema,\n  ComputerUseMoveActionSchema,\n  ComputerUseScrollActionSchema,\n  ComputerUseTypeActionSchema,\n  ComputerUseWaitActionSchema,\n]);\n\n// Schema for the input structure received from parseComputerCall\n// The action is wrapped in an `action` property: { action: { type: 'screenshot' } }\nexport const ComputerUseActionSchema = z.object({\n  action: ComputerUseActionUnionSchema,\n});\n\n// TypeScript types derived from Zod schemas\nexport type ComputerUseScreenshotActionType = z.infer<\n  typeof ComputerUseScreenshotActionSchema\n>;\nexport type ComputerUseClickActionType = z.infer<\n  typeof ComputerUseClickActionSchema\n>;\nexport type ComputerUseDoubleClickActionType = z.infer<\n  typeof ComputerUseDoubleClickActionSchema\n>;\nexport type ComputerUseDragActionType = z.infer<\n  typeof ComputerUseDragActionSchema\n>;\nexport type ComputerUseKeypressActionType = z.infer<\n  typeof ComputerUseKeypressActionSchema\n>;\nexport type ComputerUseMoveActionType = z.infer<\n  typeof ComputerUseMoveActionSchema\n>;\nexport type ComputerUseScrollActionType = z.infer<\n  typeof ComputerUseScrollActionSchema\n>;\nexport type ComputerUseTypeActionType = z.infer<\n  typeof ComputerUseTypeActionSchema\n>;\nexport type ComputerUseWaitActionType = z.infer<\n  typeof ComputerUseWaitActionSchema\n>;\n\n/**\n * Input structure for the Computer Use tool.\n * The action is wrapped in an `action` property.\n */\nexport interface ComputerUseInput {\n  action: ComputerUseAction;\n}\n\nexport type ComputerUseReturnType =\n  | string\n  | Promise<string>\n  | ToolMessage<any>\n  | Promise<ToolMessage<any>>;\n\n/**\n * Options for the Computer Use tool.\n */\nexport interface ComputerUseOptions {\n  /**\n   * The width of the computer display in pixels.\n   */\n  displayWidth: number;\n\n  /**\n   * The height of the computer display in pixels.\n   */\n  displayHeight: number;\n\n  /**\n   * The type of computer environment to control.\n   * - `browser`: Browser automation (recommended for most use cases)\n   * - `mac`: macOS environment\n   * - `windows`: Windows environment\n   * - `linux`: Linux environment\n   * - `ubuntu`: Ubuntu environment\n   */\n  environment: ComputerUseEnvironment;\n\n  /**\n   * Execute function that handles computer action execution.\n   * This function receives the action input and should return a base64-encoded\n   * screenshot of the result.\n   */\n  execute: (\n    action: ComputerUseAction,\n    runtime: ToolRuntime<any, any>\n  ) => ComputerUseReturnType;\n}\n\n/**\n * OpenAI Computer Use tool type for the Responses API.\n */\nexport type ComputerUseTool = OpenAIClient.Responses.ComputerTool;\n\nconst TOOL_NAME = \"computer_use\";\n\n/**\n * Creates a Computer Use tool that allows models to control computer interfaces\n * and perform tasks by simulating mouse clicks, keyboard input, scrolling, and more.\n *\n * **Computer Use** is a practical application of OpenAI's Computer-Using Agent (CUA)\n * model (`computer-use-preview`), which combines vision capabilities with advanced\n * reasoning to simulate controlling computer interfaces.\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends computer actions (click, type, scroll, etc.)\n * 2. Your code executes these actions in a controlled environment\n * 3. You capture a screenshot of the result\n * 4. Send the screenshot back to the model\n * 5. Repeat until the task is complete\n *\n * **Important**: Computer use is in beta and requires careful consideration:\n * - Use in sandboxed environments only\n * - Do not use for high-stakes or authenticated tasks\n * - Always implement human-in-the-loop for important decisions\n * - Handle safety checks appropriately\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-computer-use | OpenAI Computer Use Documentation}\n *\n * @param options - Configuration options for the Computer Use tool\n * @returns A Computer Use tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n *\n * const model = new ChatOpenAI({ model: \"computer-use-preview\" });\n *\n * // With execute callback for automatic action handling\n * const computer = tools.computerUse({\n *   displayWidth: 1024,\n *   displayHeight: 768,\n *   environment: \"browser\",\n *   execute: async (action) => {\n *     if (action.type === \"screenshot\") {\n *       return captureScreenshot();\n *     }\n *     if (action.type === \"click\") {\n *       await page.mouse.click(action.x, action.y, { button: action.button });\n *       return captureScreenshot();\n *     }\n *     if (action.type === \"type\") {\n *       await page.keyboard.type(action.text);\n *       return captureScreenshot();\n *     }\n *     // Handle other actions...\n *     return captureScreenshot();\n *   },\n * });\n *\n * const llmWithComputer = model.bindTools([computer]);\n * const response = await llmWithComputer.invoke(\n *   \"Check the latest news on bing.com\"\n * );\n * ```\n *\n * @example\n * ```typescript\n * // Without execute callback (manual action handling)\n * const computer = tools.computerUse({\n *   displayWidth: 1024,\n *   displayHeight: 768,\n *   environment: \"browser\",\n * });\n *\n * const response = await model.invoke(\"Check the news\", {\n *   tools: [computer],\n * });\n *\n * // Access the computer call from the response\n * const computerCall = response.additional_kwargs.tool_outputs?.find(\n *   (output) => output.type === \"computer_call\"\n * );\n * if (computerCall) {\n *   console.log(\"Action to execute:\", computerCall.action);\n *   // Execute the action manually, then send back a screenshot\n * }\n * ```\n *\n * @example\n * ```typescript\n * // For macOS desktop automation with Docker\n * const computer = tools.computerUse({\n *   displayWidth: 1920,\n *   displayHeight: 1080,\n *   environment: \"mac\",\n *   execute: async (action) => {\n *     if (action.type === \"click\") {\n *       await dockerExec(\n *         `DISPLAY=:99 xdotool mousemove ${action.x} ${action.y} click 1`,\n *         containerName\n *       );\n *     }\n *     // Capture screenshot from container\n *     return await getDockerScreenshot(containerName);\n *   },\n * });\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Requires `computer-use-preview` model\n * - Actions include: click, double_click, drag, keypress, move, screenshot, scroll, type, wait\n * - Safety checks may be returned that require acknowledgment before proceeding\n * - Use `truncation: \"auto\"` parameter when making requests\n * - Recommended to use with `reasoning.summary` for debugging\n */\nexport function computerUse(options: ComputerUseOptions) {\n  const computerTool = tool(\n    async (\n      input: ComputerUseInput,\n      runtime: ToolRuntime<{ messages: BaseMessage[] }>\n    ) => {\n      /**\n       * get computer_use call id from runtime\n       */\n      const aiMessage = runtime.state?.messages.at(-1) as AIMessage | undefined;\n      const computerToolCall = aiMessage?.tool_calls?.find(\n        (tc) => tc.name === \"computer_use\"\n      );\n      const computerToolCallId = computerToolCall?.id;\n      if (!computerToolCallId) {\n        throw new Error(\"Computer use call id not found\");\n      }\n\n      const result = await options.execute(input.action, runtime);\n\n      /**\n       * make sure {@link ToolMessage} is returned with the correct additional kwargs\n       */\n      if (typeof result === \"string\") {\n        return new ToolMessage({\n          content: result,\n          tool_call_id: computerToolCallId,\n          additional_kwargs: {\n            type: \"computer_call_output\",\n          },\n        });\n      }\n\n      /**\n       * make sure {@link ToolMessage} is returned with the correct additional kwargs\n       */\n      return new ToolMessage({\n        ...result,\n        tool_call_id: computerToolCallId,\n        additional_kwargs: {\n          type: \"computer_call_output\",\n          ...result.additional_kwargs,\n        },\n      });\n    },\n    {\n      name: TOOL_NAME,\n      description:\n        \"Control a computer interface by executing mouse clicks, keyboard input, scrolling, and other actions.\",\n      schema: ComputerUseActionSchema,\n    }\n  );\n\n  computerTool.extras = {\n    ...(computerTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"computer_use_preview\",\n      display_width: options.displayWidth,\n      display_height: options.displayHeight,\n      environment: options.environment,\n    } satisfies ComputerUseTool,\n  };\n\n  /**\n   * return as typed {@link DynamicStructuredTool} so we don't get any type\n   * errors like \"can't export tool without reference\"\n   */\n  return computerTool as DynamicStructuredTool<\n    typeof ComputerUseActionSchema,\n    ComputerUseInput,\n    unknown,\n    ComputerUseReturnType\n  >;\n}\n"],"names":["options: ComputerUseOptions","input: ComputerUseInput","runtime: ToolRuntime<{ messages: BaseMessage[] }>"],"mappings":";;;;;;;;;;;;;AAkDA,MAAM,oCAAoC,oJAAA,CAAE,MAAA,CAAO;IACjD,MAAM,oJAAA,CAAE,OAAA,CAAQ,aAAa;AAC9B,EAAC;AAEF,MAAM,+BAA+B,oJAAA,CAAE,MAAA,CAAO;IAC5C,MAAM,oJAAA,CAAE,OAAA,CAAQ,QAAQ;IACxB,GAAG,oJAAA,CAAE,MAAA,EAAQ;IACb,GAAG,oJAAA,CAAE,MAAA,EAAQ;IACb,QAAQ,oJAAA,CAAE,IAAA,CAAK;QAAC;QAAQ;QAAS;QAAS;QAAQ;KAAU,CAAC,CAAC,OAAA,CAAQ,OAAO;AAC9E,EAAC;AAEF,MAAM,qCAAqC,oJAAA,CAAE,MAAA,CAAO;IAClD,MAAM,oJAAA,CAAE,OAAA,CAAQ,eAAe;IAC/B,GAAG,oJAAA,CAAE,MAAA,EAAQ;IACb,GAAG,oJAAA,CAAE,MAAA,EAAQ;IACb,QAAQ,oJAAA,CAAE,IAAA,CAAK;QAAC;QAAQ;QAAS;QAAS;QAAQ;KAAU,CAAC,CAAC,OAAA,CAAQ,OAAO;AAC9E,EAAC;AAEF,MAAM,8BAA8B,oJAAA,CAAE,MAAA,CAAO;IAC3C,MAAM,oJAAA,CAAE,OAAA,CAAQ,OAAO;IACvB,MAAM,oJAAA,CAAE,KAAA,CAAM,oJAAA,CAAE,MAAA,CAAO;QAAE,GAAG,oJAAA,CAAE,MAAA,EAAQ;QAAE,GAAG,oJAAA,CAAE,MAAA,EAAQ;IAAE,EAAC,CAAC;AAC1D,EAAC;AAEF,MAAM,kCAAkC,oJAAA,CAAE,MAAA,CAAO;IAC/C,MAAM,oJAAA,CAAE,OAAA,CAAQ,WAAW;IAC3B,MAAM,oJAAA,CAAE,KAAA,CAAM,oJAAA,CAAE,MAAA,EAAQ,CAAC;AAC1B,EAAC;AAEF,MAAM,8BAA8B,oJAAA,CAAE,MAAA,CAAO;IAC3C,MAAM,oJAAA,CAAE,OAAA,CAAQ,OAAO;IACvB,GAAG,oJAAA,CAAE,MAAA,EAAQ;IACb,GAAG,oJAAA,CAAE,MAAA,EAAQ;AACd,EAAC;AAEF,MAAM,gCAAgC,oJAAA,CAAE,MAAA,CAAO;IAC7C,MAAM,oJAAA,CAAE,OAAA,CAAQ,SAAS;IACzB,GAAG,oJAAA,CAAE,MAAA,EAAQ;IACb,GAAG,oJAAA,CAAE,MAAA,EAAQ;IACb,UAAU,oJAAA,CAAE,MAAA,EAAQ;IACpB,UAAU,oJAAA,CAAE,MAAA,EAAQ;AACrB,EAAC;AAEF,MAAM,8BAA8B,oJAAA,CAAE,MAAA,CAAO;IAC3C,MAAM,oJAAA,CAAE,OAAA,CAAQ,OAAO;IACvB,MAAM,oJAAA,CAAE,MAAA,EAAQ;AACjB,EAAC;AAEF,MAAM,8BAA8B,oJAAA,CAAE,MAAA,CAAO;IAC3C,MAAM,oJAAA,CAAE,OAAA,CAAQ,OAAO;IACvB,UAAU,oJAAA,CAAE,MAAA,EAAQ,CAAC,QAAA,EAAU;AAChC,EAAC;AAGF,MAAM,+BAA+B,oJAAA,CAAE,kBAAA,CAAmB,QAAQ;IAChE;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;CACD,CAAC;AAIF,MAAa,0BAA0B,oJAAA,CAAE,MAAA,CAAO;IAC9C,QAAQ;AACT,EAAC;AAqFF,MAAM,YAAY;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAkHlB,SAAgB,YAAYA,OAAAA,EAA6B;IACvD,MAAM,mBAAe,uLAAA,EACnB,OACEC,OACAC,YACG;;;KAIH,MAAM,YAAY,QAAQ,KAAA,EAAO,SAAS,GAAG,CAAA,EAAG;QAChD,MAAM,mBAAmB,WAAW,YAAY,KAC9C,CAAC,KAAO,GAAG,IAAA,KAAS,eACrB;QACD,MAAM,qBAAqB,kBAAkB;QAC7C,IAAI,CAAC,mBACH,CAAA,MAAM,IAAI,MAAM;QAGlB,MAAM,SAAS,MAAM,QAAQ,OAAA,CAAQ,MAAM,MAAA,EAAQ,QAAQ;;;KAK3D,IAAI,OAAO,WAAW,SACpB,CAAA,OAAO,IAAI,gLAAA,CAAY;YACrB,SAAS;YACT,cAAc;YACd,mBAAmB;gBACjB,MAAM;YACP;QACF;;;KAMH,OAAO,IAAI,gLAAA,CAAY;YACrB,GAAG,MAAA;YACH,cAAc;YACd,mBAAmB;gBACjB,MAAM;gBACN,GAAG,OAAO,iBAAA;YACX;QACF;IACF,GACD;QACE,MAAM;QACN,aACE;QACF,QAAQ;IACT,EACF;IAED,aAAa,MAAA,GAAS;QACpB,GAAI,aAAa,MAAA,IAAU,CAAE,CAAA;QAC7B,wBAAwB;YACtB,MAAM;YACN,eAAe,QAAQ,YAAA;YACvB,gBAAgB,QAAQ,aAAA;YACxB,aAAa,QAAQ,WAAA;QACtB;IACF;;;;IAMD,OAAO;AAMR"}},
    {"offset": {"line": 6954, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/localShell.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/localShell.ts"],"sourcesContent":["import { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\n\n/**\n * Re-export action type from OpenAI SDK for convenience.\n * The action contains command details like argv tokens, environment variables,\n * working directory, timeout, and user.\n */\nexport type LocalShellAction =\n  OpenAIClient.Responses.ResponseOutputItem.LocalShellCall.Action;\n\n// Zod schema for local shell exec action\nexport const LocalShellExecActionSchema = z.object({\n  type: z.literal(\"exec\"),\n  command: z.array(z.string()),\n  env: z.record(z.string(), z.string()).optional(),\n  working_directory: z.string().optional(),\n  timeout_ms: z.number().optional(),\n  user: z.string().optional(),\n});\n\n// Schema for all local shell actions (currently only exec)\nexport const LocalShellActionSchema = z.discriminatedUnion(\"type\", [\n  LocalShellExecActionSchema,\n]);\n\n/**\n * Options for the Local Shell tool.\n */\nexport interface LocalShellOptions {\n  /**\n   * Optional execute function that handles shell command execution.\n   * This function receives the action input and should return the command output\n   * (stdout + stderr combined).\n   *\n   * If not provided, you'll need to handle action execution manually by\n   * checking `local_shell_call` outputs in the response.\n   *\n   * @example\n   * ```typescript\n   * execute: async (action) => {\n   *   const result = await exec(action.command.join(' '), {\n   *     cwd: action.working_directory,\n   *     env: { ...process.env, ...action.env },\n   *     timeout: action.timeout_ms,\n   *   });\n   *   return result.stdout + result.stderr;\n   * }\n   * ```\n   */\n  execute: (action: LocalShellAction) => string | Promise<string>;\n}\n\n/**\n * OpenAI Local Shell tool type for the Responses API.\n */\nexport type LocalShellTool = OpenAIClient.Responses.Tool.LocalShell;\n\nconst TOOL_NAME = \"local_shell\";\n\n/**\n * Creates a Local Shell tool that allows models to run shell commands locally\n * on a machine you provide. Commands are executed inside your own runtime\n * the API only returns the instructions, but does not execute them on OpenAI infrastructure.\n *\n * **Important**: The local shell tool is designed to work with\n * [Codex CLI](https://github.com/openai/codex) and the `codex-mini-latest` model.\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends shell commands (`local_shell_call` with `exec` action)\n * 2. Your code executes the command locally\n * 3. You return the output back to the model\n * 4. Repeat until the task is complete\n *\n * **Security Warning**: Running arbitrary shell commands can be dangerous.\n * Always sandbox execution or add strict allow/deny-lists before forwarding\n * a command to the system shell.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-local-shell | OpenAI Local Shell Documentation}\n *\n * @param options - Optional configuration for the Local Shell tool\n * @returns A Local Shell tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n * import { exec } from \"child_process\";\n * import { promisify } from \"util\";\n *\n * const execAsync = promisify(exec);\n * const model = new ChatOpenAI({ model: \"codex-mini-latest\" });\n *\n * // With execute callback for automatic command handling\n * const shell = tools.localShell({\n *   execute: async (action) => {\n *     const { command, env, working_directory, timeout_ms } = action;\n *     const result = await execAsync(command.join(' '), {\n *       cwd: working_directory ?? process.cwd(),\n *       env: { ...process.env, ...env },\n *       timeout: timeout_ms ?? undefined,\n *     });\n *     return result.stdout + result.stderr;\n *   },\n * });\n *\n * const llmWithShell = model.bindTools([shell]);\n * const response = await llmWithShell.invoke(\n *   \"List files in the current directory\"\n * );\n * ```\n *\n * @example\n * ```typescript\n * // Without execute callback (manual handling)\n * const shell = tools.localShell();\n *\n * const response = await model.invoke(\"List files\", {\n *   tools: [shell],\n * });\n *\n * // Access the shell call from the response\n * const shellCall = response.additional_kwargs.tool_outputs?.find(\n *   (output) => output.type === \"local_shell_call\"\n * );\n * if (shellCall) {\n *   console.log(\"Command to execute:\", shellCall.action.command);\n *   // Execute the command manually, then send back the output\n * }\n * ```\n *\n * @example\n * ```typescript\n * // Full shell loop example\n * async function shellLoop(model, task) {\n *   let response = await model.invoke(task, {\n *     tools: [tools.localShell()],\n *   });\n *\n *   while (true) {\n *     const shellCall = response.additional_kwargs.tool_outputs?.find(\n *       (output) => output.type === \"local_shell_call\"\n *     );\n *\n *     if (!shellCall) break;\n *\n *     // Execute command (with proper sandboxing!)\n *     const output = await executeCommand(shellCall.action);\n *\n *     // Send output back to model\n *     response = await model.invoke([\n *       response,\n *       {\n *         type: \"local_shell_call_output\",\n *         id: shellCall.call_id,\n *         output: output,\n *       },\n *     ], {\n *       tools: [tools.localShell()],\n *     });\n *   }\n *\n *   return response;\n * }\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Designed for use with `codex-mini-latest` model\n * - Commands are provided as argv tokens in `action.command`\n * - Action includes: `command`, `env`, `working_directory`, `timeout_ms`, `user`\n * - Always sandbox or validate commands before execution\n * - The `timeout_ms` from the model is only a hintenforce your own limits\n */\nexport function localShell(options: LocalShellOptions) {\n  const shellTool = tool(options.execute, {\n    name: TOOL_NAME,\n    description:\n      \"Execute shell commands locally on the machine. Commands are provided as argv tokens.\",\n    schema: LocalShellActionSchema,\n  });\n\n  shellTool.extras = {\n    ...(shellTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"local_shell\",\n    } satisfies LocalShellTool,\n  };\n\n  return shellTool as DynamicStructuredTool<\n    typeof LocalShellActionSchema,\n    LocalShellAction,\n    unknown,\n    string\n  >;\n}\n"],"names":["options: LocalShellOptions"],"mappings":";;;;;;;;;;AAaA,MAAa,6BAA6B,oJAAA,CAAE,MAAA,CAAO;IACjD,MAAM,oJAAA,CAAE,OAAA,CAAQ,OAAO;IACvB,SAAS,oJAAA,CAAE,KAAA,CAAM,oJAAA,CAAE,MAAA,EAAQ,CAAC;IAC5B,KAAK,oJAAA,CAAE,MAAA,CAAO,oJAAA,CAAE,MAAA,EAAQ,EAAE,oJAAA,CAAE,MAAA,EAAQ,CAAC,CAAC,QAAA,EAAU;IAChD,mBAAmB,oJAAA,CAAE,MAAA,EAAQ,CAAC,QAAA,EAAU;IACxC,YAAY,oJAAA,CAAE,MAAA,EAAQ,CAAC,QAAA,EAAU;IACjC,MAAM,oJAAA,CAAE,MAAA,EAAQ,CAAC,QAAA,EAAU;AAC5B,EAAC;AAGF,MAAa,yBAAyB,oJAAA,CAAE,kBAAA,CAAmB,QAAQ;IACjE,0BACD;CAAA,CAAC;AAkCF,MAAM,YAAY;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAoHlB,SAAgB,WAAWA,OAAAA,EAA4B;IACrD,MAAM,gBAAY,uLAAA,EAAK,QAAQ,OAAA,EAAS;QACtC,MAAM;QACN,aACE;QACF,QAAQ;IACT,EAAC;IAEF,UAAU,MAAA,GAAS;QACjB,GAAI,UAAU,MAAA,IAAU,CAAE,CAAA;QAC1B,wBAAwB;YACtB,MAAM;QACP;IACF;IAED,OAAO;AAMR"}},
    {"offset": {"line": 7109, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/shell.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/shell.ts"],"sourcesContent":["import { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\n\n/**\n * Re-export action type from OpenAI SDK for convenience.\n * The action contains command details like commands array, timeout, and max output length.\n */\nexport type ShellAction =\n  OpenAIClient.Responses.ResponseFunctionShellToolCall.Action;\n\n// Zod schema for shell action\nexport const ShellActionSchema = z.object({\n  commands: z.array(z.string()).describe(\"Array of shell commands to execute\"),\n  timeout_ms: z\n    .number()\n    .optional()\n    .describe(\"Optional timeout in milliseconds for the commands\"),\n  max_output_length: z\n    .number()\n    .optional()\n    .describe(\n      \"Optional maximum number of characters to return from each command\"\n    ),\n});\n\n/**\n * Result of a single shell command execution.\n * Contains stdout, stderr, and the outcome (exit code or timeout).\n */\nexport type ShellCommandOutput =\n  OpenAIClient.Responses.ResponseFunctionShellCallOutputContent;\n\n/**\n * Outcome type for shell command execution - either exit with code or timeout.\n */\nexport type ShellCallOutcome = ShellCommandOutput[\"outcome\"];\n\n/**\n * Result of executing shell commands.\n * Contains an array of outputs (one per command) and the max_output_length parameter.\n */\nexport interface ShellResult {\n  /**\n   * Array of command outputs. Each entry corresponds to a command from the action.\n   * The order should match the order of commands in the action.\n   */\n  output: ShellCommandOutput[];\n  /**\n   * The max_output_length from the action, which must be passed back to the API.\n   * If not provided in the action, can be omitted.\n   */\n  maxOutputLength?: number | null;\n}\n\n/**\n * Options for the Shell tool.\n */\nexport interface ShellOptions {\n  /**\n   * Execute function that handles shell command execution.\n   * This function receives the action input containing the commands and limits,\n   * and should return a ShellResult with stdout, stderr, and outcome for each command.\n   *\n   * @example\n   * ```typescript\n   * execute: async (action) => {\n   *   const outputs = await Promise.all(\n   *     action.commands.map(async (cmd) => {\n   *       try {\n   *         const { stdout, stderr } = await exec(cmd, {\n   *           timeout: action.timeout_ms ?? undefined,\n   *         });\n   *         return {\n   *           stdout,\n   *           stderr,\n   *           outcome: { type: \"exit\" as const, exit_code: 0 },\n   *         };\n   *       } catch (error) {\n   *         const timedOut = error.killed && error.signal === \"SIGTERM\";\n   *         return {\n   *           stdout: error.stdout ?? \"\",\n   *           stderr: error.stderr ?? String(error),\n   *           outcome: timedOut\n   *             ? { type: \"timeout\" as const }\n   *             : { type: \"exit\" as const, exit_code: error.code ?? 1 },\n   *         };\n   *       }\n   *     })\n   *   );\n   *   return {\n   *     output: outputs,\n   *     maxOutputLength: action.max_output_length,\n   *   };\n   * }\n   * ```\n   */\n  execute: (action: ShellAction) => ShellResult | Promise<ShellResult>;\n}\n\n/**\n * OpenAI Shell tool type for the Responses API.\n */\nexport type ShellTool = OpenAIClient.Responses.FunctionShellTool;\n\nconst TOOL_NAME = \"shell\";\n\n/**\n * Creates a Shell tool that allows models to run shell commands through your integration.\n *\n * The shell tool allows the model to interact with your local computer through a controlled\n * command-line interface. The model proposes shell commands; your integration executes them\n * and returns the outputs. This creates a simple plan-execute loop that lets models inspect\n * the system, run utilities, and gather data until they can finish the task.\n *\n * **Important**: The shell tool is available through the Responses API for use with `GPT-5.1`.\n * It is not available on other models, or via the Chat Completions API.\n *\n * **When to use**:\n * - **Automating filesystem or process diagnostics**  For example, \"find the largest PDF\n *   under ~/Documents\" or \"show running gunicorn processes.\"\n * - **Extending the model's capabilities**  Using built-in UNIX utilities, python runtime\n *   and other CLIs in your environment.\n * - **Running multi-step build and test flows**  Chaining commands like `pip install` and `pytest`.\n * - **Complex agentic coding workflows**  Using other tools like `apply_patch` to complete\n *   workflows that involve complex file operations.\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends shell commands (`shell_call` with `commands` array)\n * 2. Your code executes the commands (can be concurrent)\n * 3. You return stdout, stderr, and outcome for each command\n * 4. Repeat until the task is complete\n *\n * **Security Warning**: Running arbitrary shell commands can be dangerous.\n * Always sandbox execution or add strict allow/deny-lists before forwarding\n * a command to the system shell.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-shell | OpenAI Shell Documentation}\n * @see {@link https://github.com/openai/codex | Codex CLI} for reference implementation.\n *\n * @param options - Configuration for the Shell tool\n * @returns A Shell tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n * import { exec } from \"child_process/promises\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-5.1\" });\n *\n * // With execute callback for automatic command handling\n * const shellTool = tools.shell({\n *   execute: async (action) => {\n *     const outputs = await Promise.all(\n *       action.commands.map(async (cmd) => {\n *         try {\n *           const { stdout, stderr } = await exec(cmd, {\n *             timeout: action.timeout_ms ?? undefined,\n *           });\n *           return {\n *             stdout,\n *             stderr,\n *             outcome: { type: \"exit\" as const, exit_code: 0 },\n *           };\n *         } catch (error) {\n *           const timedOut = error.killed && error.signal === \"SIGTERM\";\n *           return {\n *             stdout: error.stdout ?? \"\",\n *             stderr: error.stderr ?? String(error),\n *             outcome: timedOut\n *               ? { type: \"timeout\" as const }\n *               : { type: \"exit\" as const, exit_code: error.code ?? 1 },\n *           };\n *         }\n *       })\n *     );\n *     return {\n *       output: outputs,\n *       maxOutputLength: action.max_output_length,\n *     };\n *   },\n * });\n *\n * const llmWithShell = model.bindTools([shellTool]);\n * const response = await llmWithShell.invoke(\n *   \"Find the largest PDF file in ~/Documents\"\n * );\n * ```\n *\n * @example\n * ```typescript\n * // Full shell loop example\n * async function shellLoop(model, task) {\n *   let response = await model.invoke(task, {\n *     tools: [tools.shell({ execute: myExecutor })],\n *   });\n *\n *   while (true) {\n *     const shellCall = response.additional_kwargs.tool_outputs?.find(\n *       (output) => output.type === \"shell_call\"\n *     );\n *\n *     if (!shellCall) break;\n *\n *     // Execute commands (with proper sandboxing!)\n *     const result = await executeCommands(shellCall.action);\n *\n *     // Send output back to model\n *     response = await model.invoke([\n *       response,\n *       {\n *         type: \"shell_call_output\",\n *         call_id: shellCall.call_id,\n *         output: result.output,\n *         max_output_length: result.maxOutputLength,\n *       },\n *     ], {\n *       tools: [tools.shell({ execute: myExecutor })],\n *     });\n *   }\n *\n *   return response;\n * }\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Designed for use with `gpt-5.1` model\n * - Commands are provided as an array of strings that can be executed concurrently\n * - Action includes: `commands`, `timeout_ms`, `max_output_length`\n * - Always sandbox or validate commands before execution\n * - The `timeout_ms` from the model is only a hintenforce your own limits\n * - If `max_output_length` exists in the action, always pass it back in the output\n * - Many CLI tools return non-zero exit codes for warnings; still capture stdout/stderr\n */\nexport function shell(options: ShellOptions) {\n  // Wrapper that converts ShellResult to string for LangChain tool compatibility\n  const executeWrapper = async (action: ShellAction): Promise<string> => {\n    const result = await options.execute(action);\n    // Return a JSON string representation for the tool result\n    return JSON.stringify({\n      output: result.output,\n      max_output_length: result.maxOutputLength,\n    });\n  };\n\n  const shellTool = tool(executeWrapper, {\n    name: TOOL_NAME,\n    description:\n      \"Execute shell commands in a managed environment. Commands can be run concurrently.\",\n    schema: ShellActionSchema,\n  });\n\n  shellTool.extras = {\n    ...(shellTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"shell\",\n    } satisfies ShellTool,\n  };\n\n  return shellTool as DynamicStructuredTool<\n    typeof ShellActionSchema,\n    ShellAction,\n    unknown,\n    string\n  >;\n}\n"],"names":["options: ShellOptions","action: ShellAction"],"mappings":";;;;;;;;;;AAYA,MAAa,oBAAoB,oJAAA,CAAE,MAAA,CAAO;IACxC,UAAU,oJAAA,CAAE,KAAA,CAAM,oJAAA,CAAE,MAAA,EAAQ,CAAC,CAAC,QAAA,CAAS,qCAAqC;IAC5E,YAAY,oJAAA,CACT,MAAA,EAAQ,CACR,QAAA,EAAU,CACV,QAAA,CAAS,oDAAoD;IAChE,mBAAmB,oJAAA,CAChB,MAAA,EAAQ,CACR,QAAA,EAAU,CACV,QAAA,CACC,oEACD;AACJ,EAAC;AAiFF,MAAM,YAAY;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAmIlB,SAAgB,MAAMA,OAAAA,EAAuB;IAE3C,MAAM,iBAAiB,OAAOC,WAAyC;QACrE,MAAM,SAAS,MAAM,QAAQ,OAAA,CAAQ,OAAO;QAE5C,OAAO,KAAK,SAAA,CAAU;YACpB,QAAQ,OAAO,MAAA;YACf,mBAAmB,OAAO,eAAA;QAC3B,EAAC;IACH;IAED,MAAM,gBAAY,uLAAA,EAAK,gBAAgB;QACrC,MAAM;QACN,aACE;QACF,QAAQ;IACT,EAAC;IAEF,UAAU,MAAA,GAAS;QACjB,GAAI,UAAU,MAAA,IAAU,CAAE,CAAA;QAC1B,wBAAwB;YACtB,MAAM;QACP;IACF;IAED,OAAO;AAMR"}},
    {"offset": {"line": 7280, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/applyPatch.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/applyPatch.ts"],"sourcesContent":["import { z } from \"zod/v4\";\nimport { OpenAI as OpenAIClient } from \"openai\";\nimport { tool, type DynamicStructuredTool } from \"@langchain/core/tools\";\n\n/**\n * Re-export operation types from OpenAI SDK for convenience.\n */\nexport type ApplyPatchCreateFileOperation =\n  OpenAIClient.Responses.ResponseApplyPatchToolCall.CreateFile;\nexport type ApplyPatchUpdateFileOperation =\n  OpenAIClient.Responses.ResponseApplyPatchToolCall.UpdateFile;\nexport type ApplyPatchDeleteFileOperation =\n  OpenAIClient.Responses.ResponseApplyPatchToolCall.DeleteFile;\n\n/**\n * Union type of all apply patch operations from OpenAI SDK.\n */\nexport type ApplyPatchOperation = NonNullable<\n  OpenAIClient.Responses.ResponseApplyPatchToolCall[\"operation\"]\n>;\n\n// Zod schemas for apply patch operations\nexport const ApplyPatchCreateFileOperationSchema = z.object({\n  type: z.literal(\"create_file\"),\n  path: z.string(),\n  diff: z.string(),\n});\n\nexport const ApplyPatchUpdateFileOperationSchema = z.object({\n  type: z.literal(\"update_file\"),\n  path: z.string(),\n  diff: z.string(),\n});\n\nexport const ApplyPatchDeleteFileOperationSchema = z.object({\n  type: z.literal(\"delete_file\"),\n  path: z.string(),\n});\n\n// Discriminated union schema for all apply patch operations\nexport const ApplyPatchOperationSchema = z.discriminatedUnion(\"type\", [\n  ApplyPatchCreateFileOperationSchema,\n  ApplyPatchUpdateFileOperationSchema,\n  ApplyPatchDeleteFileOperationSchema,\n]);\n\n/**\n * Options for the Apply Patch tool.\n */\nexport interface ApplyPatchOptions {\n  /**\n   * Execute function that handles patch operations.\n   * This function receives the operation input and should return a string\n   * describing the result (success or failure message).\n   *\n   * The operation types are:\n   * - `create_file`: Create a new file at the specified path with the diff content\n   * - `update_file`: Modify an existing file using V4A diff format\n   * - `delete_file`: Remove a file at the specified path\n   *\n   * @example\n   * ```typescript\n   * execute: async (operation) => {\n   *   if (operation.type === \"create_file\") {\n   *     const content = applyDiff(\"\", operation.diff, \"create\");\n   *     await fs.writeFile(operation.path, content);\n   *     return `Created ${operation.path}`;\n   *   }\n   *   if (operation.type === \"update_file\") {\n   *     const current = await fs.readFile(operation.path, \"utf-8\");\n   *     const newContent = applyDiff(current, operation.diff);\n   *     await fs.writeFile(operation.path, newContent);\n   *     return `Updated ${operation.path}`;\n   *   }\n   *   if (operation.type === \"delete_file\") {\n   *     await fs.unlink(operation.path);\n   *     return `Deleted ${operation.path}`;\n   *   }\n   *   return \"Unknown operation type\";\n   * }\n   * ```\n   */\n  execute: (operation: ApplyPatchOperation) => string | Promise<string>;\n}\n\n/**\n * OpenAI Apply Patch tool type for the Responses API.\n */\nexport type ApplyPatchTool = OpenAIClient.Responses.ApplyPatchTool;\n\nconst TOOL_NAME = \"apply_patch\";\n\n/**\n * Creates an Apply Patch tool that allows models to propose structured diffs\n * that your integration applies. This enables iterative, multi-step code\n * editing workflows.\n *\n * **Apply Patch** lets GPT-5.1 create, update, and delete files in your codebase\n * using structured diffs. Instead of just suggesting edits, the model emits\n * patch operations that your application applies and then reports back on.\n *\n * **When to use**:\n * - **Multi-file refactors**  Rename symbols, extract helpers, or reorganize modules\n * - **Bug fixes**  Have the model both diagnose issues and emit precise patches\n * - **Tests & docs generation**  Create new test files, fixtures, and documentation\n * - **Migrations & mechanical edits**  Apply repetitive, structured updates\n *\n * **How it works**:\n * The tool operates in a continuous loop:\n * 1. Model sends patch operations (`apply_patch_call` with operation type)\n * 2. Your code applies the patch to your working directory or repo\n * 3. You return success/failure status and optional output\n * 4. Repeat until the task is complete\n *\n * **Security Warning**: Applying patches can modify files in your codebase.\n * Always validate paths, implement backups, and consider sandboxing.\n *\n * @see {@link https://platform.openai.com/docs/guides/tools-apply-patch | OpenAI Apply Patch Documentation}\n *\n * @param options - Configuration options for the Apply Patch tool\n * @returns An Apply Patch tool that can be passed to `bindTools`\n *\n * @example\n * ```typescript\n * import { ChatOpenAI, tools } from \"@langchain/openai\";\n * import { applyDiff } from \"@openai/agents\";\n * import * as fs from \"fs/promises\";\n *\n * const model = new ChatOpenAI({ model: \"gpt-5.1\" });\n *\n * // With execute callback for automatic patch handling\n * const patchTool = tools.applyPatch({\n *   execute: async (operation) => {\n *     if (operation.type === \"create_file\") {\n *       const content = applyDiff(\"\", operation.diff, \"create\");\n *       await fs.writeFile(operation.path, content);\n *       return `Created ${operation.path}`;\n *     }\n *     if (operation.type === \"update_file\") {\n *       const current = await fs.readFile(operation.path, \"utf-8\");\n *       const newContent = applyDiff(current, operation.diff);\n *       await fs.writeFile(operation.path, newContent);\n *       return `Updated ${operation.path}`;\n *     }\n *     if (operation.type === \"delete_file\") {\n *       await fs.unlink(operation.path);\n *       return `Deleted ${operation.path}`;\n *     }\n *     return \"Unknown operation type\";\n *   },\n * });\n *\n * const llmWithPatch = model.bindTools([patchTool]);\n * const response = await llmWithPatch.invoke(\n *   \"Rename the fib() function to fibonacci() in lib/fib.py\"\n * );\n * ```\n *\n * @remarks\n * - Only available through the Responses API (not Chat Completions)\n * - Designed for use with `gpt-5.1` model\n * - Operations include: `create_file`, `update_file`, `delete_file`\n * - Patches use V4A diff format for updates\n * - Always validate paths to prevent directory traversal attacks\n * - Consider backing up files before applying patches\n * - Implement \"all-or-nothing\" semantics if atomicity is required\n */\nexport function applyPatch(options: ApplyPatchOptions) {\n  const patchTool = tool(options.execute, {\n    name: TOOL_NAME,\n    description:\n      \"Apply structured diffs to create, update, or delete files in the codebase.\",\n    schema: ApplyPatchOperationSchema,\n  });\n\n  patchTool.extras = {\n    ...(patchTool.extras ?? {}),\n    providerToolDefinition: {\n      type: \"apply_patch\",\n    } satisfies ApplyPatchTool,\n  };\n\n  return patchTool as DynamicStructuredTool<\n    typeof ApplyPatchOperationSchema,\n    ApplyPatchOperation,\n    unknown,\n    string\n  >;\n}\n"],"names":["options: ApplyPatchOptions"],"mappings":";;;;;;;;;;AAsBA,MAAa,sCAAsC,oJAAA,CAAE,MAAA,CAAO;IAC1D,MAAM,oJAAA,CAAE,OAAA,CAAQ,cAAc;IAC9B,MAAM,oJAAA,CAAE,MAAA,EAAQ;IAChB,MAAM,oJAAA,CAAE,MAAA,EAAQ;AACjB,EAAC;AAEF,MAAa,sCAAsC,oJAAA,CAAE,MAAA,CAAO;IAC1D,MAAM,oJAAA,CAAE,OAAA,CAAQ,cAAc;IAC9B,MAAM,oJAAA,CAAE,MAAA,EAAQ;IAChB,MAAM,oJAAA,CAAE,MAAA,EAAQ;AACjB,EAAC;AAEF,MAAa,sCAAsC,oJAAA,CAAE,MAAA,CAAO;IAC1D,MAAM,oJAAA,CAAE,OAAA,CAAQ,cAAc;IAC9B,MAAM,oJAAA,CAAE,MAAA,EAAQ;AACjB,EAAC;AAGF,MAAa,4BAA4B,oJAAA,CAAE,kBAAA,CAAmB,QAAQ;IACpE;IACA;IACA;CACD,CAAC;AA8CF,MAAM,YAAY;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GA6ElB,SAAgB,WAAWA,OAAAA,EAA4B;IACrD,MAAM,gBAAY,uLAAA,EAAK,QAAQ,OAAA,EAAS;QACtC,MAAM;QACN,aACE;QACF,QAAQ;IACT,EAAC;IAEF,UAAU,MAAA,GAAS;QACjB,GAAI,UAAU,MAAA,IAAU,CAAE,CAAA;QAC1B,wBAAwB;YACtB,MAAM;QACP;IACF;IAED,OAAO;AAMR"}},
    {"offset": {"line": 7404, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/index.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/index.ts"],"sourcesContent":["export * from \"./dalle.js\";\n\nimport { webSearch } from \"./webSearch.js\";\nexport type {\n  WebSearchTool,\n  WebSearchFilters,\n  WebSearchOptions,\n} from \"./webSearch.js\";\n\nimport { mcp } from \"./mcp.js\";\nexport type {\n  McpTool,\n  McpConnectorId,\n  McpToolFilter,\n  McpApprovalFilter,\n  McpRemoteServerOptions,\n  McpConnectorOptions,\n} from \"./mcp.js\";\n\nimport { codeInterpreter } from \"./codeInterpreter.js\";\nexport type {\n  CodeInterpreterTool,\n  CodeInterpreterOptions,\n  CodeInterpreterAutoContainer,\n  CodeInterpreterMemoryLimit,\n} from \"./codeInterpreter.js\";\n\nimport { fileSearch } from \"./fileSearch.js\";\nexport type {\n  FileSearchTool,\n  FileSearchOptions,\n  FileSearchFilter,\n  FileSearchComparisonFilter,\n  FileSearchCompoundFilter,\n  FileSearchComparisonType,\n  FileSearchRankingOptions,\n  FileSearchHybridSearchWeights,\n} from \"./fileSearch.js\";\n\nimport { imageGeneration } from \"./imageGeneration.js\";\nexport type {\n  ImageGenerationTool,\n  ImageGenerationOptions,\n  ImageGenerationInputMask,\n} from \"./imageGeneration.js\";\n\nimport { computerUse } from \"./computerUse.js\";\nexport type {\n  ComputerUseTool,\n  ComputerUseInput,\n  ComputerUseOptions,\n  ComputerUseEnvironment,\n  ComputerUseAction,\n  ComputerUseClickAction,\n  ComputerUseDoubleClickAction,\n  ComputerUseDragAction,\n  ComputerUseKeypressAction,\n  ComputerUseMoveAction,\n  ComputerUseScreenshotAction,\n  ComputerUseScrollAction,\n  ComputerUseTypeAction,\n  ComputerUseWaitAction,\n} from \"./computerUse.js\";\n\nimport { localShell } from \"./localShell.js\";\nexport type {\n  LocalShellTool,\n  LocalShellOptions,\n  LocalShellAction,\n} from \"./localShell.js\";\n\nimport { shell } from \"./shell.js\";\nexport type {\n  ShellTool,\n  ShellOptions,\n  ShellAction,\n  ShellResult,\n  ShellCommandOutput,\n  ShellCallOutcome,\n} from \"./shell.js\";\n\nimport { applyPatch } from \"./applyPatch.js\";\nexport type {\n  ApplyPatchTool,\n  ApplyPatchOptions,\n  ApplyPatchOperation,\n  ApplyPatchCreateFileOperation,\n  ApplyPatchUpdateFileOperation,\n  ApplyPatchDeleteFileOperation,\n} from \"./applyPatch.js\";\n\nexport const tools = {\n  webSearch,\n  mcp,\n  codeInterpreter,\n  fileSearch,\n  imageGeneration,\n  computerUse,\n  localShell,\n  shell,\n  applyPatch,\n};\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;AA2FA,MAAa,QAAQ;eACnB,kLAAA;SACA,sKAAA;qBACA,8LAAA;gBACA,oLAAA;qBACA,8LAAA;iBACA,sLAAA;gBACA,oLAAA;WACA,0KAAA;gBACA,oLAAA;AACD"}},
    {"offset": {"line": 7446, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/tools/custom.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/tools/custom.ts"],"sourcesContent":["import {\n  patchConfig,\n  pickRunnableConfigKeys,\n  RunnableFunc,\n} from \"@langchain/core/runnables\";\nimport { AsyncLocalStorageProviderSingleton } from \"@langchain/core/singletons\";\nimport { DynamicTool, ToolRunnableConfig } from \"@langchain/core/tools\";\nimport OpenAI from \"openai\";\n\nexport type CustomToolFields = Omit<OpenAI.Responses.CustomTool, \"type\">;\n\nexport function customTool(\n  func: RunnableFunc<string, string, ToolRunnableConfig>,\n  fields: CustomToolFields\n): DynamicTool<string> {\n  return new DynamicTool({\n    ...fields,\n    description: \"\",\n    metadata: {\n      customTool: fields,\n    },\n    func: async (input, runManager, config) =>\n      new Promise<string>((resolve, reject) => {\n        const childConfig = patchConfig(config, {\n          callbacks: runManager?.getChild(),\n        });\n        // eslint-disable-next-line no-void\n        void AsyncLocalStorageProviderSingleton.runWithConfig(\n          pickRunnableConfigKeys(childConfig),\n          async () => {\n            try {\n              resolve(func(input, childConfig));\n            } catch (e) {\n              reject(e);\n            }\n          }\n        );\n      }),\n  });\n}\n"],"names":["func: RunnableFunc<string, string, ToolRunnableConfig>","fields: CustomToolFields"],"mappings":";;;;;;;;;;;;;AAWA,SAAgB,WACdA,IAAAA,EACAC,MAAAA,EACqB;IACrB,OAAO,IAAI,8LAAA,CAAY;QACrB,GAAG,MAAA;QACH,aAAa;QACb,UAAU;YACR,YAAY;QACb;QACD,MAAM,OAAO,OAAO,YAAY,SAC9B,IAAI,QAAgB,CAAC,SAAS,WAAW;gBACvC,MAAM,kBAAc,mLAAA,EAAY,QAAQ;oBACtC,WAAW,YAAY,UAAU;gBAClC,EAAC;gBAEG,iOAAA,CAAmC,aAAA,KACtC,8LAAA,EAAuB,YAAY,EACnC,YAAY;oBACV,IAAI;wBACF,QAAQ,KAAK,OAAO,YAAY,CAAC;oBAClC,EAAA,OAAQ,GAAG;wBACV,OAAO,EAAE;oBACV;gBACF,EACF;YACF;IACJ;AACF"}},
    {"offset": {"line": 7486, "column": 0}, "map": {"version":3,"file":"turbopack:///[project]/node_modules/@langchain/openai/dist/utils/prompts.js","sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/src/utils/prompts.ts"],"sourcesContent":["import type { BasePromptValue } from \"@langchain/core/prompt_values\";\nimport type { OpenAI } from \"openai\";\nimport { convertMessagesToCompletionsMessageParams } from \"../converters/completions.js\";\n\n/**\n * Convert a formatted LangChain prompt (e.g. pulled from the hub) into\n * a format expected by OpenAI's JS SDK.\n *\n * Requires the \"@langchain/openai\" package to be installed in addition\n * to the OpenAI SDK.\n *\n * @example\n * ```ts\n * import { convertPromptToOpenAI } from \"langsmith/utils/hub/openai\";\n * import { pull } from \"langchain/hub\";\n *\n * import OpenAI from 'openai';\n *\n * const prompt = await pull(\"jacob/joke-generator\");\n * const formattedPrompt = await prompt.invoke({\n *   topic: \"cats\",\n * });\n *\n * const { messages } = convertPromptToOpenAI(formattedPrompt);\n *\n * const openAIClient = new OpenAI();\n *\n * const openaiResponse = await openAIClient.chat.completions.create({\n *   model: \"gpt-4o-mini\",\n *   messages,\n * });\n * ```\n * @param formattedPrompt\n * @returns A partial OpenAI payload.\n */\n// TODO: make this a converter\nexport function convertPromptToOpenAI(formattedPrompt: BasePromptValue): {\n  messages: OpenAI.Chat.ChatCompletionMessageParam[];\n} {\n  const messages = formattedPrompt.toChatMessages();\n  return {\n    messages: convertMessagesToCompletionsMessageParams({\n      messages,\n    }) as OpenAI.Chat.ChatCompletionMessageParam[],\n  };\n}\n"],"names":["formattedPrompt: BasePromptValue"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;GAoCA,SAAgB,sBAAsBA,eAAAA,EAEpC;IACA,MAAM,WAAW,gBAAgB,cAAA,EAAgB;IACjD,OAAO;QACL,cAAU,yNAAA,EAA0C;YAClD;QACD,EAAC;IACH;AACF"}},
    {"offset": {"line": 7537, "column": 0}, "map": {"version":3,"sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/dist/converters/index.js"],"sourcesContent":["import { completionsApiContentBlockConverter, convertCompletionsDeltaToBaseMessageChunk, convertCompletionsMessageToBaseMessage, convertMessagesToCompletionsMessageParams, convertStandardContentBlockToCompletionsContentPart, convertStandardContentMessageToCompletionsMessage } from \"./completions.js\";\nimport { convertMessagesToResponsesInput, convertReasoningSummaryToResponsesReasoningItem, convertResponsesDeltaToChatGenerationChunk, convertResponsesMessageToAIMessage, convertResponsesUsageToUsageMetadata, convertStandardContentMessageToResponsesInput } from \"./responses.js\";\n"],"names":[],"mappings":";AAAA;AACA","ignoreList":[0]}},
    {"offset": {"line": 7546, "column": 0}, "map": {"version":3,"sources":["file:///Users/christofferwallman/Desktop/Utveckling/NORAV1.2/node_modules/%40langchain/openai/dist/index.js"],"sourcesContent":["import { wrapOpenAIClientError } from \"./utils/client.js\";\nimport { messageToOpenAIRole } from \"./utils/misc.js\";\nimport { getEndpoint, getFormattedEnv, getHeadersWithUserAgent, isHeaders, normalizeHeaders } from \"./utils/azure.js\";\nimport { BaseChatOpenAI } from \"./chat_models/base.js\";\nimport { completionsApiContentBlockConverter, convertCompletionsDeltaToBaseMessageChunk, convertCompletionsMessageToBaseMessage, convertMessagesToCompletionsMessageParams, convertStandardContentBlockToCompletionsContentPart, convertStandardContentMessageToCompletionsMessage } from \"./converters/completions.js\";\nimport { ChatOpenAICompletions } from \"./chat_models/completions.js\";\nimport { AzureChatOpenAICompletions } from \"./azure/chat_models/completions.js\";\nimport { convertMessagesToResponsesInput, convertReasoningSummaryToResponsesReasoningItem, convertResponsesDeltaToChatGenerationChunk, convertResponsesMessageToAIMessage, convertResponsesUsageToUsageMetadata, convertStandardContentMessageToResponsesInput } from \"./converters/responses.js\";\nimport { ChatOpenAIResponses } from \"./chat_models/responses.js\";\nimport { AzureChatOpenAIResponses } from \"./azure/chat_models/responses.js\";\nimport { ChatOpenAI } from \"./chat_models/index.js\";\nimport { AzureChatOpenAI } from \"./azure/chat_models/index.js\";\nimport { OpenAI } from \"./llms.js\";\nimport { AzureOpenAI } from \"./azure/llms.js\";\nimport { OpenAIEmbeddings } from \"./embeddings.js\";\nimport { AzureOpenAIEmbeddings } from \"./azure/embeddings.js\";\nimport { DallEAPIWrapper } from \"./tools/dalle.js\";\nimport { tools } from \"./tools/index.js\";\nimport { customTool } from \"./tools/custom.js\";\nimport { convertPromptToOpenAI } from \"./utils/prompts.js\";\nimport \"./converters/index.js\";\nimport { OpenAI as OpenAIClient, toFile } from \"openai\";\n\nexport { AzureChatOpenAI, AzureChatOpenAICompletions, AzureChatOpenAIResponses, AzureOpenAI, AzureOpenAIEmbeddings, BaseChatOpenAI, ChatOpenAI, ChatOpenAICompletions, ChatOpenAIResponses, DallEAPIWrapper, OpenAI, OpenAIClient, OpenAIEmbeddings, completionsApiContentBlockConverter, convertCompletionsDeltaToBaseMessageChunk, convertCompletionsMessageToBaseMessage, convertMessagesToCompletionsMessageParams, convertMessagesToResponsesInput, convertPromptToOpenAI, convertReasoningSummaryToResponsesReasoningItem, convertResponsesDeltaToChatGenerationChunk, convertResponsesMessageToAIMessage, convertResponsesUsageToUsageMetadata, convertStandardContentBlockToCompletionsContentPart, convertStandardContentMessageToCompletionsMessage, convertStandardContentMessageToResponsesInput, customTool, getEndpoint, getFormattedEnv, getHeadersWithUserAgent, isHeaders, messageToOpenAIRole, normalizeHeaders, toFile, tools, wrapOpenAIClientError };"],"names":[],"mappings":";AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAAA;AAAA","ignoreList":[0]}}]
}